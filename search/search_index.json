{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Best Python Practices","text":"<p>A collection of python tutorials and patterns to use in data science projects.</p> <p>This guide is meant to address common pitfalls and difficult areas of python which are important in data science code and not easily solved with a simple stack overflow search. The tutorials explore topics such as organization  and performance optimization.</p> <p>The Zen of Python is one of the guiding principles for this documentation:</p> <p><pre><code>import this\n</code></pre> <pre><code>The Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n</code></pre></p> <p>One important aspect of improving the developing experience is to keep your Python version and dependencies up-to-date.</p> <p>The best place to start is by looking at the Tutorials</p>"},{"location":"design_patterns/","title":"Design Patterns","text":""},{"location":"design_patterns/#what-are-design-patterns","title":"What are Design Patterns?","text":"<p>In software engineering, a design pattern is a general repeatable solution to a commonly occurring problem in software design.  </p> <p>Design patterns can speed up the development process by providing tested, proven development paradigms. Design patterns help to prevent subtle issues that can cause major problems and improve code readability.</p> <p>Design patterns are usually classified into three major categories:  - Creational Patterns  - Structural Patterns  - Behavioral Patterns</p> <p>Each pattern type helps to solve a certain class of problems. We are not going to cover all the patterns in this document, but we will cover the ones we frequently use.</p>"},{"location":"design_patterns/#creational-patterns","title":"Creational Patterns","text":""},{"location":"design_patterns/#singleton","title":"Singleton","text":""},{"location":"design_patterns/#intent","title":"Intent","text":"<ul> <li>Ensure a class has only one instance, and provide a global point of access to it.</li> </ul>"},{"location":"design_patterns/#problem","title":"Problem","text":"<p>Applications sometimes need one and only one instance of an object during the lifetime of the application instance.</p>"},{"location":"design_patterns/#solution","title":"Solution","text":"<p>It is recommended to name all your singletons in a <code>singleton.py</code> file within your project. Optionally, you can also name the file whatever you prefer, but expose the singleton object via <code>__all__</code> of the module's <code>__init__.py</code>.</p> <p>Since we don't want to expose the ability to create a new instance of the object to external modules and Python modules are imported once and globally available after import, it is important to make sure the name of the class starts with <code>__</code> (dunder) and use <code>__all__</code> to limit the symbols that need to be exported.</p> <pre><code># in singleton.py\n\n# Only expose the singleton objects you want to be exposed for this module\n__all__ = [\"s3client\"]\n\nclass __S3Client:\n    def __init__(self, ...):\n        # Implementation of your object\n        pass\n\ns3client = __S3Client()\n</code></pre>"},{"location":"design_patterns/#structural-patterns","title":"Structural Patterns","text":""},{"location":"design_patterns/#adapter","title":"Adapter","text":""},{"location":"design_patterns/#intent_1","title":"Intent","text":"<ul> <li>Wrap an existing class with a new interface.</li> <li>Wrap an old component to a new system.</li> <li>Let classes work together that couldn't otherwise because of incompatible interfaces.</li> </ul>"},{"location":"design_patterns/#problem_1","title":"Problem","text":"<p>Take OCR as an example. Each provider calls the actual method that does OCR slightly differently. For example, EasyOCR uses a method called <code>readtext</code>, Google Vision SDK uses <code>document_text_detection</code>, and PyTesseract uses <code>ocr_client</code> to obtain the OCR result.</p>"},{"location":"design_patterns/#solution_1","title":"Solution","text":"<pre><code># in interface.py\n\nfrom abc import ABC, abstractmethod\n\n# Our expected interface\nclass OCRClient(ABC):\n    \"\"\"\n    Interface for OCR Client\n    \"\"\"\n\n    @abstractmethod\n    def ocr(self, image):\n        pass\n\n# in adapter.py\n\n# Adapter Class\nclass GVisionOCRClientAdapter(OCRClient):\n    def __init__(self):\n        self._client = GVisionOCRClientAdaptee()\n\n    def ocr(self, image):\n        return self._client.adaptee_ocr(image=image)\n\n# Adaptee Class\nclass GVisionOCRClientAdaptee:\n    def __init__(self):\n        self.private_client = vision.from_service_account_json('some_location.json')\n\n    def adaptee_ocr(self, image):\n        return self.private_client.document_text_detection(image)\n</code></pre>"},{"location":"design_patterns/#behavioral-patterns","title":"Behavioral Patterns","text":""},{"location":"design_patterns/#registry","title":"Registry","text":""},{"location":"design_patterns/#intent_2","title":"Intent","text":"<ul> <li>Keep track of all subclasses of a given class.</li> <li>Create objects of subclasses that have different behaviors.</li> </ul>"},{"location":"design_patterns/#problem_2","title":"Problem","text":"<p>The need for some kind of manager class to manage all related subclasses. You want the manager class to iterate over all available subclasses and call a particular method on each/some subclasses. You want to streamline a factory class, which takes an input and creates/returns an object of that type.</p>"},{"location":"design_patterns/#solution_2","title":"Solution","text":"<pre><code>from typing import Type\n\nclass Document(ABC):\n    _REGISTRY = {}\n\n    def __new__(cls, *args, **kwargs):\n        name = cls.__name__\n        prefix = \"Document\"\n        assert name.startswith(prefix)\n        form_type = name[len(prefix):].lower()\n        cls._REGISTRY[form_type] = cls\n\n    @classmethod\n    def factory(cls, form_type: str) -&gt; Type[Document]:\n        return cls._REGISTRY[form_type]()\n\n    @abstractmethod\n    def extract(self, *args, **kwargs):\n        pass\n\nclass Document1040(Document):\n    def extract(self, file: ByteIO, fields: Iterable[str] = None) -&gt; Document1040Results:\n        # some extract logic\n        pass\n\nclass Document1099K(Document):\n    def extract(self, file: ByteIO, fields: Iterable[str] = None) -&gt; Document1099KResult:\n        # different logic to extract this form\n        pass\n\n# To use this\ndoc_1040 = Document.factory(\"1040\")\nresult_1040 = doc_1040.extract([\"ssn\", \"agi\"])\n\ndoc_1099k = Document.factory(\"1099k\")\nresult_1099k = doc_1099k.extract()\n\n# A more powerful use case would be in a loop, you will not see large if/else blocks to \n# process files based on the file type.\nfor doc_content, doc_type in all_documents:\n    extractor = Document.factory(doc_type)\n    result = extractor.extract()\n</code></pre>"},{"location":"design_patterns/#chain-of-responsibility","title":"Chain of Responsibility","text":""},{"location":"design_patterns/#intent_3","title":"Intent","text":"<p>When an object needs to be processed by a potential chain of successive handlers. Each handler may process it, pass it over, or break the chain and stop the task from propagating to the next handler.</p>"},{"location":"design_patterns/#problem_3","title":"Problem","text":"<p>The need for decoupling the sender of a request and its receiver. It is unknown until runtime what kind of request the object is, therefore, it needs a different processing handler to handle the request. Sometimes, more than one handler needs to pass over the data.</p>"},{"location":"design_patterns/#solution_3","title":"Solution","text":"<pre><code>class NamedEntityHandler(ABC):\n    def __init__(self, successor: Optional[\"NamedEntityHandler\"] = None):\n        self._successor = successor\n\n    @abstractmethod\n    def handle(self, text: str) -&gt; Entity:\n        pass\n\nclass PersonNameHandler(NamedEntityHandler):\n    def handle(self, text: str) -&gt; Entity:\n        name = PersonNameEntityParser.parse(text)\n        if self._successor is not None:\n            name = self._successor.handle(name)\n        return name\n\nclass NameConfidenceCalibrationHandler(NamedEntityHandler):\n    def handle(self, name_entity: Entity) -&gt; Entity:\n        return NameConfidenceCalibrator.calibrate(name_entity)\n\nclass CompanyNameHandler(NamedEntityHandler):\n    def handle(self, text: str) -&gt; Entity:\n        return CompanyNameEntityParser.parse(text)\n\nclass DateHandler(NamedEntityHandler): \n    def handle(self, text: str) -&gt; Entity:\n        return DateParser.parse(text)\n\nclass AmountHandler(NamedEntityHandler):\n    def handle(self, text: str) -&gt; Entity:\n        return AmountParser.parse(text)\n\nclass NamedEntityExtractor:\n    def __init__(self):\n        self._person_name_handler = PersonNameHandler(NameConfidenceCalibrationHandler())\n        self._company_name_handler = CompanyNameHandler()\n        self._address_handler = AddressHandler()\n        self._date_handler = DateHandler()\n        self._amount_handler = AmountHandler()\n        self._doc_handler = {\n            DocTypeEnum.F_1040: [self._person_name_handler, self._address_handler, self._date_handler, self._amount_handler],\n            DocTypeEnum.F_W2: [self._person_name_handler, self._address_handler, self._amount_handler],\n            DocTypeEnum.F_BILL: [self._company_name_handler, self._date_handler, self._amount_handler]\n        }\n\n    def parse_entity(self, text: str, doc_type: DocTypeEnum) -&gt; List[Entity]:\n        parse_chain = self._doc_handler.get(doc_type)\n        entity_list = [parser.handle(text) for parser in parse_chain]\n        return list(filter(lambda x: x is not None, entity_list))\n</code></pre>"},{"location":"design_patterns/#strategy","title":"Strategy","text":""},{"location":"design_patterns/#intent_4","title":"Intent","text":"<p>Define a family of interchangeable algorithms by a client (client decides which algorithm to use either dynamically or statically).</p>"},{"location":"design_patterns/#problem_4","title":"Problem","text":"<p>There are multiple ways of solving the same problem, each of which has its own advantages/disadvantages. To be able to quickly change the behavior of the program.</p>"},{"location":"docker/","title":"Docker Best Practice","text":"<p>In the realm of machine learning, reproducibility is paramount. Ensuring that experiments can be replicated reliably across different environments is crucial for scientific rigor and collaboration. Docker, a powerful containerization tool, has emerged as a vital component in modern ML workflows. By encapsulating applications and their dependencies into standardized containers, Docker provides a consistent and portable environment for ML development, deployment, and collaboration.</p>"},{"location":"docker/#general-best-practices","title":"General Best Practices","text":""},{"location":"docker/#use-officil-base-images","title":"Use Officil Base Images","text":"<p>Start with official Python images to ensure compatibility and security.  It is important to use the latest Python version to ensure the system is up to date. </p> <pre><code>FROM python:3.12-slim\n</code></pre>"},{"location":"docker/#use-a-dockerignore-file","title":"Use a .dockerignore File","text":"<p>Exclude unnecessary files from the build context.</p> <pre><code>__pycache__\n*.pyc\n*.pyo\n*.pyd\n.Python\nenv\npip-log.txt\npip-delete-this-directory.txt\n.tox\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.log\n.git\n.mypy_cache\n.pytest_cache\n.hypothesis\n</code></pre>"},{"location":"docker/#minimize-the-number-of-layers","title":"Minimize the Number of Layers","text":"<p>Combine related commands into a single RUN instruction to reduce the number of layers and image size. Intead of running the <code>RUN</code> command on every input, you can combine them to create a single layer. </p> <pre><code>FROM ubuntu:20.04\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    python3 \\\n    python3-pip \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n</code></pre>"},{"location":"docker/#run-as-non-root-user","title":"Run as Non-Root User","text":"<p>Container escaping is possible due to bugs in the code/host.  Create and use a non-root user for better security. </p> <pre><code>RUN adduser --disabled-password --gecos '' appuser\nUSER appuser\nWORKDIR /home/appuser/app\n</code></pre>"},{"location":"docker/#use-enviornment-variables-and-arguments","title":"Use Enviornment Variables and Arguments","text":"<p>Environment variables and arguments in Dockerfiles are powerful tools for creating flexible and customizable container images. They allow you to dynamically configure your application's behavior without rebuilding the image. This is particularly useful in machine learning workflows where you may need to experiment with different hyperparameters, datasets, or models.</p> <pre><code># Build-time argument for the base image\nARG PYTHON_VERSION=3.11-slim-buster\n\n# Base image\nFROM python:${PYTHON_VERSION}\n\n# Build-time argument for the dataset URL\nARG DATASET_URL=https://example.com/dataset.zip\n\nARG MODEL_ENV=test\n# Environment variables\nENV PYTHONUNBUFFERED 1\nENV MODEL_DIR /model\nENV DATASET_DIR /data\nENV MODEL_ENV=$MODEL_ENV\n\n# Set the working directory\nWORKDIR /app\n\n# Download and extract the dataset (using a script)\nRUN bash -c \"ds_loader -q $DATASET_URL &amp;&amp; unzip dataset.zip -d $DATASET_DIR\"\n\n# Install dependencies\nCOPY requirements.txt requirements.txt\nRUN pip install -r requirements.txt\n\n# Copy the application code\nCOPY . .\n\n# Define the command to run the training script\nCMD [\"python\", \"train.py\", \"--data_dir\", \"$DATASET_DIR\", \"--model_dir\", \"$MODEL_DIR\"]\n</code></pre> <p>In the above example. The user could control the version of the <code>PYTHON_VERSION</code> and the <code>MODEL_ENV</code> parameter to alter the code behavior in <code>train.py</code></p> <p>an example use would be</p> <p><code>docker build --build-arg PYTHON_VERSION=latest --build-arg MODEL_ENV=production --no-cache .</code> This example changes the base docker to the latest python version and set the environment variable to production.</p>"},{"location":"docker/#optimize-caching","title":"Optimize Caching","text":"<p>Order Dockerfile instructions from least to most frequently changing to optimize the caching.  This will result in a faster docker build when changes needs to happen.</p> <pre><code>FROM python:3.12-slim\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\nCOPY . .\nCMD [\"python\", \"app.py\"]\n</code></pre> <p>In the above example we assume <code>requirements.txt</code> is relatively stable only the application code is changing often. Then all of the dependencies installed by the pip command is already cached in the layer that will not need to be re-installed again</p>"},{"location":"docker/#use-multi-stage-builds","title":"Use Multi-stage Builds","text":"<p>Take advantage of multi-stage builds to create leaner, more secure Docker images.</p> <p>Multi-stage Docker builds allow you to break up your Dockerfiles into several stages. For example, you can have a stage for compiling and building your application, which can then be copied to subsequent stages. Since only the final stage is used to create the image, the dependencies and tools associated with building your application are discarded, leaving a lean and modular production-ready image.</p> <pre><code># builder stage\nFROM python:3.12.2-slim as builder\n\nWORKDIR /app\n\nENV PYTHONDONTWRITEBYTECODE 1\nENV PYTHONUNBUFFERED 1\n\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends gcc\n\nCOPY requirements.txt .\nRUN pip wheel --no-cache-dir --no-deps --wheel-dir /app/wheels -r requirements.txt\n\n\n# production stage\nFROM python:3.12.2-slim\n\nWORKDIR /app\n\nCOPY --from=builder /app/wheels /wheels\nCOPY --from=builder /app/requirements.txt .\n\nRUN pip install --no-cache /wheels/*\n</code></pre> <p>a Data Science multi-stage docker could looks like</p> <pre><code># development stage\nFROM python:3.12.2 as builder\n\nRUN pip wheel --no-cache-dir --no-deps --wheel-dir /wheels jupyter pandas\n\n\n# final stage\nFROM python:3.12.2-slim\n\nWORKDIR /notebooks\n\nCOPY --from=builder /wheels /wheels\nRUN pip install --no-cache /wheels/*\n</code></pre>"},{"location":"docker/#use-healthchecks","title":"Use Healthchecks","text":"<p>A Docker healthcheck is a useful tool to monitor the health of a container. For long-running training jobs, a custom healthcheck can provide valuable insights into the job's progress and potential issues. This is not limited to the long running tasks, it can be used to attempt to reach an application endpoint or check a process\u2019s status. If the health check command returns a failure due to an application crash, the container is marked as unhealthy. You can then restart the container or stop it and shift traffic to other instances.</p> <p><pre><code>FROM tensorflow/tensorflow:latest\n\nWORKDIR /app\n\n# dependencies installed for the job\n# COPY requirements.txt requirements.txt\n# RUN pip install -r requirements.txt\n\nCOPY . .\n\n# ... other build instructions ...\n\nHEALTHCHECK --interval=30s --timeout=10s CMD if [ -f /app/train.log ] &amp;&amp; [ $(stat -c %s /app/train.log) -gt 0 ]; then exit 0; else exit 1; fi\n</code></pre> The above example checks the health every 30 seconds, times out the check after 10 seconds. The command to use for checking the health is checks if the train.log file exists and the file size is greater than 0. If both conditions are met, the healthcheck passes.</p>"},{"location":"docker/#ml-docker-container-best-practice","title":"ML Docker Container Best Practice","text":"<p>As an opinonated guide, I highly recommend to use cog to create your ML docker container</p>"},{"location":"docker/#use-gpu-enabled-base-images","title":"Use GPU-enabled Base Images.","text":"<p>For ML projects requiring GPU, use CUDA-enabled base images.  Whenever possible we should try to leverage the official base images because security fixes. </p> <pre><code>FROM nvidia/cuda:11.3.1-runtime-ubuntu20.04\n</code></pre>"},{"location":"docker/#install-ml-specific-libraries","title":"Install ML-specific libraries","text":"<p>Install commonly used ML libraries efficiently. To avoid cuda hell with other well known libraries, it is recommended to install the commonly used libraries for ML into the container.</p> <pre><code>RUN pip install --no-cache-dir numpy pandas scikit-learn torch torchvision\n</code></pre> <p>Note: the above didn't specify the version number of each dependnecies.  It is recommended to pin to a version instead of relying on pip to figure out the latest.</p>"},{"location":"docker/#manage-large-model-files-or-datasets","title":"Manage Large Model Files or datasets","text":"<p>Use volume mounts for large model files instead of including them in the image.</p> <pre><code>VOLUME /app/models\nVOLUME /app/datasets\n</code></pre> <p>This assumes the models or dataset could use <code>init-container</code> to be downloaded.  </p>"},{"location":"docker/#optimize-for-inference","title":"Optimize for Inference","text":"<p>For inference containers, focus on runtime dependencies only.</p> <pre><code>FROM python:3.12-slim\nCOPY --from=builder /app/model /app/model\nCOPY requirements-inference.txt .\nRUN pip install --no-cache-dir -r requirements-inference.txt\nCOPY inference.py .\nCMD [\"python\", \"inference.py\"]\n</code></pre> <p>Only install what is necessary for inference.</p>"},{"location":"docker/#version-control-for-models-and-data","title":"Version Control for Models and Data","text":"<p>Use version control systems for models and data, and reference specific versions in your Dockerfile.</p> <pre><code>ARG MODEL_VERSION=v1.2.3\nRUN wget https://model-repo.com/model-${MODEL_VERSION}.pkl -O /app/model.pkl\n</code></pre>"},{"location":"docker/#use-appropriate-concurrency","title":"Use Appropriate Concurrency","text":"<p>Set the number of workers based on available resources.</p> <p><pre><code>CMD gunicorn --workers 4 --threads 4 --bind 0.0.0.0:8000 app:app\n</code></pre> Since python is under 3.13 still have GIL in place. The command above assume you have a quad core(4) CPU and each core have hyperthreading enabled to fill the instruction pipeline of the CPU.</p>"},{"location":"docker/#optimize-memory","title":"Optimize memory","text":"<p><pre><code># Use an official Python runtime as a parent image\nFROM python:3.12-slim\n\n# Set environment variables for memory optimization\nENV PYTHONMALLOC=malloc            # Use the standard C malloc allocator\nENV PYTHONMALLOCSTATS=1            # Enable detailed memory allocation statistics\nENV MALLOC_TRIM_THRESHOLD_=100000  # Set the minimum block size to trigger memory trimming\nENV PYTHONHASHSEED=0               # Set a fixed hash seed for deterministic behavior\nENV PYTHONASYNCIODEBUG=1           # Enable debugging for asyncio\nENV PYTHONTRACEMALLOC=1            # Enable tracing of memory allocations\nENV PYTHONDEVMODE=1                # Enable Python's debug mode\nENV PYTHONMEMORY=4294967296        # Set a 4GB memory limit for Python\nENV PYTHONGC=1                     # Enable garbage collection\nENV OMP_NUM_THREADS=4              # Set the number of OpenMP threads to 4\n\n...\n\n# Run app.py when the container launches\nCMD [\"python\", \"app.py\"]\n</code></pre> The above is just an example, your usage may vary.</p> <p>Here's a breakdown of the environment variables used:</p> <ul> <li>PYTHONMALLOC=malloc: Uses the standard C malloc instead of Python's custom allocator.</li> <li>PYTHONMALLOCSTATS=1: Prints memory allocation statistics.</li> <li>MALLOC_TRIM_THRESHOLD_=100000: Sets a lower threshold for releasing memory back to the system.</li> <li>PYTHONHASHSEED=0: Makes memory usage more predictable across runs.</li> <li>PYTHONASYNCIODEBUG=1: Enables debugging mode for asyncio to help identify memory leaks in asynchronous code.</li> <li>PYTHONTRACEMALLOC=1: Enables tracemalloc to track memory allocations.</li> <li>PYTHONDEVMODE=1: Enables additional checks that can help catch memory-related issues earlier.</li> <li>PYTHONMEMORY=4294967296: Sets a memory limit of 4GB for Python processes.</li> <li>PYTHONGC=1: Enables garbage collection.</li> <li>OMP_NUM_THREADS=4: Limits OpenMP to 4 threads, which can help control memory usage in scientific computing libraries.</li> </ul>"},{"location":"libraries/","title":"Libraries &amp; Tools","text":"<p>The following is a curated list of python libraries that are recommended for  general use in python projects</p>"},{"location":"libraries/#open-source","title":"Open Source","text":""},{"location":"libraries/#code","title":"Code","text":"<ul> <li>black: Simple configuration-free code formatter.</li> <li>poetry: A package manager and dependency resolver.</li> <li>uv: An extremely fast Python package and project manager, written in Rust.</li> <li>cookiecutter: A tool to create projects like cookiecutters. (Project templates)</li> <li>sphinx: Python documentation generator.</li> <li>loguru: Simply logging.</li> <li>boltons: Pure python utilities that should be built-ins but bolt-ons instead.</li> </ul>"},{"location":"libraries/#libraries","title":"Libraries","text":"<ul> <li>hydra: Framework for elegantly configuring complex applications.</li> <li>pydantic: Data parsing, validation and settings management using python type annotations.</li> <li>typer: Build CLI appliactions using Python types.</li> <li>graphene: Building GraphQL schemas/types.</li> <li>httpx:  A fully featured HTTP client for Python 3, which provides sync and async APIs, and support for both HTTP/1.1 and HTTP/2.</li> <li>pyvips: A fast image processing library with low memory needs based on vips.</li> <li>PyMuPDF: A high-performance Python library for data extraction, analysis, conversion &amp; manipulation of PDF.</li> </ul>"},{"location":"libraries/#functional","title":"Functional","text":"<ul> <li>toolz: A set of utility functions for iterators, functions, and dictionaries.</li> <li>coconut: Simple, elegant, Pythonic functional programming.</li> </ul>"},{"location":"libraries/#profiling-debugging","title":"Profiling &amp; Debugging","text":"<ul> <li>py-spy: Sampling profiler, let you visualize your program without restarting the program.</li> <li>scalene: a high-performance, high-precision CPU, GPU, and memory profiler for Python with AI-powered optimization proposals.</li> <li>nvprof: Profiling for CUDA.</li> <li>nvidia-smi: Management and monitoring of NVIDIA GPU devices.</li> <li>strace: System call tracer for Linux .</li> <li>iotop: Top for IO.</li> <li>htop: Interactive system monitoring process viewer.</li> <li>nvtop: Task monitor for NVIDIA GPU.</li> </ul>"},{"location":"libraries/#ml","title":"ML","text":"<ul> <li>LazyPredict: help build a lot of basic models without much code and helps understand which models works better without any parameter tuning.</li> </ul>"},{"location":"libraries/#others","title":"Others","text":"<ul> <li>diagrams: A tool to help draw architectural diagrams.</li> <li>lux: Automatically visualize your pandas dataframe via a single print!</li> </ul>"},{"location":"pullrequests/","title":"Pull Request Etiquette","text":""},{"location":"pullrequests/#why-do-we-use-a-pull-request-workflow","title":"Why do we use a Pull Request workflow?","text":"<p>PRs are a great way of sharing information and informs us of the changes that are occuring in our codebase. They are also an excellent way of getting your work reviewed by a peer.</p> <p>The primary reason we use PRs is to encourage quality in the commits that are made to our code repositories</p> <p>When PRs done well, the commits (and their attached messages) contained within tell a story to people examining the code at a later date. If we are not careful to ensure the quality of these commits, we silently lose this ability.</p> <p>Poor quality code can be refactored. A terrible commit lasts forever.</p>"},{"location":"pullrequests/#what-constitutes-a-good-pr","title":"What constitutes a good PR?","text":"<p>A good quality PR will have the following characteristics:</p> <ul> <li>It will be a <code>complete piece of work</code> that adds value in some way.</li> <li>It will have a title that reflects the work within, and a summary that helps to understand the context of the change.</li> <li>There will be well written commit messages, with well crafted commits that tell the story of the development of this work.</li> <li>Ideally it will be small and easy to understand. Single commit PRs are usually easy to submit, review, and merge.</li> <li>The code contained within will meet the best practises set by the team wherever possible.</li> </ul> <p>A PR does not end at submission though. A code change is not made until it is merged and used in production.</p> <p>A good PR should be able to flow through a peer review system easily and quickly.</p>"},{"location":"pullrequests/#submitting-pull-requests","title":"Submitting Pull Requests","text":""},{"location":"pullrequests/#ensure-there-is-a-solid-title-and-summary","title":"Ensure there is a solid title and summary","text":"<p>PRs are a Github workflow tool, so it's important to understand that the PR title, summary and eventual discussion are not as trackable as the the commit history. If we ever move away from Github, we'll likely lose this infomation.</p> <p>That said however, they are a very useful aid in ensuring that PRs are handled quickly and effectively.</p> <p>Ensure that your PR title is scannable. People will read through the list of PRs attached to a repo, and must be able to distinguish between them based on title. Include a story/issue reference if possible, so the reviewer can get any extra context. Include a reference to the subsystem affected, if this is a large codebase.</p> <p>Use keywords in the title to help people understand your intention with the PR, eg [WIP] to indicate that it's still in progress, so should not be merged.  When using Github and have [WIP] tagged with your title, you should reframe from adding reviewers until you are ready to get it reviewed. You can directly shared your PR link to a reviewer after they agreed to look at your [WIP] PR.</p>"},{"location":"pullrequests/#rebase-before-you-make-the-pr-if-needed","title":"Rebase before you make the PR, if needed","text":"<p>Unless there is a good reason not to rebase - typically because more than one person has been working on the branch - it is often a good idea to rebase your branch to tidy up before submitting the PR. </p> <p>Use <code>git rebase -i master # or other reference, eg HEAD~5</code></p> <p>For example:</p> <ul> <li>Squash 'oops, fix typo/bug' into their parent commit. There is no reason to create and solve bugs within a PR, unless there is educational value in highlighting them.</li> <li>Reword your commit messages for clarity. Once a PR is submitted, any rewording of commits will involve a rebase, which can then mess up the conversation in the PR.</li> </ul>"},{"location":"pullrequests/#aim-for-one-succinct-commit","title":"Aim for one succinct commit","text":"<p>In an ideal world, your PR will be one small(ish) commit, doing one thing - in which case your life will be made easier, since the commit message and PR title/summary are equivalent.</p> <p>If your change contains more work than can be sensibly covered in a single commit though, do not try to squash it down into one. Commit history should tell a story, and if that story is long then it may require multiple commits to walk the reviewer through it.</p>"},{"location":"pullrequests/#describe-your-changes-well-in-each-commit","title":"Describe your changes well in each commit","text":"<p>Commit messages are invaluable to someone reading the history of the code base, and are critical for understanding why a change was made.</p> <p>Try to ensure that there is enough information in there for a person with no context or understanding of the code to make sense of the change.</p> <p>Where external information references are available - such as Issue/Story IDs, PR numbers - ensure that they are included in the commit message.</p> <p>Remember that your commit message must survive the ravages of time. Try to link to something that will be preserved equally well -- JIRA for example.</p> <p>** Each commit message should include the reason why this commit was made. Usually by adding a sentence completing the form 'So that we...' will give an amazing amount of context to the history that the code change itself cannot **</p>"},{"location":"pullrequests/#keep-it-small","title":"Keep it small","text":"<p>Try to only fix one issue or add one feature within a single pull request. The larger it is, the more complex it is to review and the more likely it will be delayed. Remember that reviewing PRs is taking time from someone else's day.</p> <p>If you must submit a large PR, try to at least make someone else aware of this fact, and arrange for their time to review and get the PR merged. It's not fair to the team to dump large pieces of work on their laps without warning.</p> <p>If you can rebase up a large PR into multiple smaller PRs, then do so.</p>"},{"location":"pullrequests/#reviewing-pull-requests","title":"Reviewing Pull Requests","text":"<p>It's a reviewers responsibility to ensure:</p> <ul> <li>Commit history is excellent</li> <li>Good changes are propagated quickly</li> <li>Code review is performed -- ideally with reasons and suggestions for changes requested.</li> <li>They understand what is being changed, from the perspective of someone examining the code in the future.</li> <li>Apply the same standards to all PRs.</li> <li>Provide the final action - ship it! or not</li> </ul> <p>It is a requester responsibility to ensure:</p> <ul> <li>Your PR is small.</li> <li>Your PR foucsing on one story or one bug only.</li> <li>Reviewed your changes before creating the PR.</li> <li>Done the necessary pre-checks -- Prettier to reformatted the code, Linter checks, Ran all of the necessary tests.</li> <li>Reply or react to every comment the reviewer have requested. </li> </ul>"},{"location":"pullrequests/#reviewers-are-the-guardians-of-the-commit-history","title":"Reviewers are the guardians of the commit history","text":"<p>The importance of ensuring a quality commit history cannot be stressed enough. It is the historical context of all of the work that we do, and is vital for understanding the reasons why changes were made in the past. What is obvious now, will not be obvious in the future.</p> <p>Without a decent commit history, we may as well be storing all our code in files ending yyyy-mm-dd. The commit history of a code base is what allows people to understand why a change was made - the when, what, and where are automatically evident.</p> <p>When looking at a commit message, ask yourself the question - from the perspective of someone looking at this change without any knowledge of the codebase - 'do I understand why this change was made?' </p> <p>If any commit within the PR does not meet this standard, the PR should be rebased until it does. We cannot fix a commit history once it is in place, unlike our ability to refactor crappy code or fix bugs.</p> <p>A useful tip is simply asking the submitter to add a sentence to the commit message completing the sentence 'So that we...'.</p>"},{"location":"pullrequests/#keep-the-flow-going","title":"Keep the flow going","text":"<p>Pull Requests are the fundamental unit of how we progress change. If PRs are getting clogged up in the system, either unreviewed or unmanaged, they are preventing a piece of work from being completed.</p> <p>As PRs clog up in the system, merges become more difficult, as other features and fixes are applied to the same codebase. This in turn slows them down further, and often completely blocks progress on a given codebase.</p> <p>There is a balance between flow and ensuring the quality of our PRs. As a reviewer you should make a call as to whether a code quality issue is sufficient enough to block the PR whilst the code is improved. Possibly it is more prudent to simply flag that the code needs rework, and raise an issue.</p> <p>Any quality issue that will obviously result in a bug should be fixed.</p>"},{"location":"pullrequests/#we-are-all-reviewers","title":"We are all reviewers","text":"<p>To make sure PRs flow through the system speedily, we must scale the PR review process. It is not sufficient (or fair!) to expect one or two people to review all PRs to our code. For starters, it creates a blocker every time those people are busy.</p> <p>Hopefully with the above guidelines, we can all start sharing the responsibility of being a reviewer.</p> <p>NB: With this in mind - if you are the first to comment on a PR, you are that PRs reviewer. If you feel that you can no longer be responsible for the subsequent merge or closure of the PR, then flag this up in the PR conversation, so someone else can take up the role.</p> <p>There's no reason why multiple people cannot comment on a PR and review it, and this is to be encouraged.</p>"},{"location":"pullrequests/#dont-add-to-the-pr-yourself","title":"Don't add to the PR yourself.","text":"<p>It's sometimes tempting to fix a bug in a PR yourself, or to rework a section to meet coding standards, or just to make a feature better fit your needs.</p> <p>If you do this, you are no longer the reviewer of the PR. You are a collaborator, and so should not merge the PR.</p> <p>It is of course possible to find a new reviewer, but generally change will be speedier if you require the original submitter to fix the code themselves. Alternatively, if the original PR is 'good enough', raise the changes you'd like to see as separate stories/issues, and rework in your own PR.</p>"},{"location":"pullrequests/#it-is-not-the-reviewers-responsibility-to-test-the-code","title":"It is not the reviewers responsibility to test the code","text":"<p>We are all busy people, and in the case of many PRs against our codebase we are not able or time-permitted to test the new code.</p> <p>We need to assume that the submitter has tested and formatted their code to the point of being happy with the work to be merged to master and subsequently released.</p> <p>If you, as a reviewer, are suspicious that the work in the PR has not been tested, raise this with the submitter. Find out how they have tested it, and refuse the work if they have not. They may not have a mechanism to test it, in which case you may need to help.</p> <p>If, as a submitter, you know that this change is not fully tested, highlight this in the PR text, and talk to the reviewer.  As a submitter, it is highly encouraged to add a screenshot of successful tests along with other screenshots to show you have done your diligence before requesting for a PR to be reviewed.</p>"},{"location":"pullrequests/#some-interesting-links","title":"Some interesting links","text":"<p>Amazing article on how to effectively use the commit history as an incredible documentation source:</p> <ul> <li>https://mislav.net/2014/02/hidden-documentation/</li> </ul> <p>Github's recommendations for PRs, which has some useful tips on notifying teams and collegues via the github @-syntax.</p> <ul> <li>https://github.com/blog/1943-how-to-write-the-perfect-pull-request </li> </ul> <p>The GOV.UK git styleguide:</p> <ul> <li>https://github.com/alphagov/styleguides/blob/master/git.md</li> </ul> <p>An example of an extremely good commit message, for a change that would be highly confusing otherwise:</p> <ul> <li>https://github.com/gds-operations/vcloud-edge_gateway/pull/111</li> </ul>"},{"location":"tutorials/","title":"Tutorials","text":""},{"location":"tutorials/#packaging","title":"Packaging","text":"<p>These tutorials describe how to create sharable libraries which can be used  across projects.</p> <ul> <li>Structure: How to structure a package.</li> <li>Makefile: Creating Makefile for your project.</li> <li>Code Style: Recommended code style tools   and, practices, and reference.</li> <li>Logging: How to configure logging.</li> <li>Testing: How to test a python package so   it is easy to test for all developers and CICD tools.</li> <li>Anti Patterns: Anti-Patterns often seen in code base.</li> <li>API Design: High level design principles for package python APIs (and how it differs from Java).</li> </ul>"},{"location":"tutorials/#patterns","title":"Patterns","text":"<p>The following are tutorials regarding how to use the best design patterns. (TBA)</p>"},{"location":"tutorials/#performance-optimization-tuning","title":"Performance Optimization &amp; Tuning","text":"<p>The following are tutorials regarding how to performance tune and optimize  critical portions of code. </p> <ul> <li>Lookup Tables: Performance of    different lookup table implementations for both  single and batch requests.</li> <li>Profiling Tools &amp; Strategy: A    general guide for how to profile python code.</li> <li>Multiprocessing: How   to efficiently use all machine resources to process data.</li> </ul>"},{"location":"tutorials/#libraries-and-tools","title":"Libraries and Tools","text":"<p>A list of recommended libraries and tools to use to make your development experience pleasant and painless.</p>"},{"location":"tutorials/packaging/anti_patterns/","title":"Anti Patterns","text":"<p>Summary</p> <p> Use as much of built-ins as possible.</p> <p> Use what the language has to offer.</p> <p> Avoid blanket try/except block.</p> <p> Avoid Repeat Yourself.</p>"},{"location":"tutorials/packaging/anti_patterns/#anti-patterns","title":"Anti-Patterns","text":"<p>Below is a list of anti-patterns that we should avoid in our code bases.</p>"},{"location":"tutorials/packaging/anti_patterns/#maintanance","title":"Maintanance","text":"<p>A program is said to be maintainable if it is easy to understand and modify as per the requirement.</p>"},{"location":"tutorials/packaging/anti_patterns/#blanket-tryexcept-block","title":"Blanket Try/Except Block","text":"<p>Avoid</p> <pre><code>try:\n    some_function()\nexcept Exception as e:\n    pass\n</code></pre> <p>Avoid</p> <pre><code>try:\n    some_function()\nexcept:\n    log()\n</code></pre> <p>Don't catch exceptions unless there is something you can do the fix it. It is usually a sign of badly designed API that uses exceptions to control flow. </p> <p>Use</p> <pre><code>try:\n    some_function(my_input)\nexcept FixableException as e: # Catch specific exception\n    fix_the_problem()\n    log.info(f\"fixing the problem encountered by {my_input=}\")\n</code></pre> <p>Use</p> <pre><code>try:\n    some_function(my_input)\nexcept NotFixableException as e:\n    log.error(f\"I can't do anything with {my_input=}\")\n    raise e # it is also good idea to wrap your own exception `raise MyNewException(e)`\n</code></pre>"},{"location":"tutorials/packaging/anti_patterns/#business-logic-in-__init__py","title":"Business logic in <code>__init__.py</code>","text":"<p><code>__init__.py</code> just like any files, it can contains any legal python code.  The primary use of <code>__init__.py</code> is for python to understand the package structure of your code.  A good use of  <code>__init__.py</code> is to use it to expose objects and types that others could use.  Do not have custom logics in this file.</p> <p>Avoid</p> <pre><code># inside __init__.py\nimport my_module\n\ndef some_function(lst: List[int]) -&gt; str:\n    return \"\".join(lst)\n\nclass MyObject:\n    def __init__(self):\n        ...\n    def call(self):\n        ...\n</code></pre> <p>Use</p> <pre><code># inside __init__.py\nfrom my_module import myfunc, MyClass\nfrom my_second_module import myfunc as my_second_func, MyClass as MySecondClass\nfrom some_other_module import create_singleton\n\nmy_singleton_obj = create_singleton()\n</code></pre>"},{"location":"tutorials/packaging/anti_patterns/#avoid-wildcard-imports-import-the-world","title":"Avoid wildcard imports (import the world)","text":"<p>During python's import mechanism any code in the module's import will be executed.</p> <p>Avoid</p> <pre><code>from my_module import *\n</code></pre> <p>Use</p> <pre><code>from my_module import (\n    my_func,\n    MyClass\n)\n</code></pre>"},{"location":"tutorials/packaging/anti_patterns/#not-using-contextmanager-with-stateful-code","title":"Not using ContextManager with stateful code","text":"<p>Whenever you have code that is stateful with system resources like openning a file or create a network socket, you should use context manager to handle the context.  </p> <p>Avoid</p> <pre><code>f = open(path)\nfor l in f:\n    do_something(l)\nf.close()\n</code></pre> <p>Use</p> <pre><code>with open(path) as f:\n    for l in f:\n        do_something(l)\n</code></pre> <p>Another important thing to note is that context manager is created for handling potential failures of system resources. Do not abuse it by using the context manager for things that doesn't require entering a context and exiting the context when done. </p>"},{"location":"tutorials/packaging/anti_patterns/#returning-multiple-variable-types","title":"Returning multiple variable types","text":"<p>If a function is suppose to return a given type (e.g. List, Tuple, Dict, MyObject) suddenly returns something else (e.g. <code>None</code>)</p> <p>Avoid</p> <pre><code>def get_meaning_of_life(question: str) -&gt; str:\n    if question != \"The Hitchhiker's Guide to the Galaxy\":\n        return None\n    else:\n        return \"42\"\n</code></pre> <p>Avoid</p> <pre><code>def parse_value(try_value: str) -&gt; Union[int, str]:\n    try:\n        return int(try_value)\n    except ValueError as e: \n        pass  # this is also anti-pattern, using exception as flow control\n\n    if not try_value.isalpha():\n        return None\n\n    return try_value\n</code></pre> <p>Use</p> <pre><code>def get_meaning_of_life(question: str) -&gt; str:\n    if question != \"The Hitchhiker's Guide to the Galaxy\":\n        raise UnknownQuestionException(\"Marvin!\")\n    else:\n        return \"42\"\n</code></pre>"},{"location":"tutorials/packaging/anti_patterns/#using-single-letter-or-abbeviations-in-your-variable-name","title":"Using single letter or abbeviations in your variable name","text":"<p>variable names should follow the principle of least astonishment.  One-character names should generally be avoided, because they contain little to no information about what they refer to. However, there are a couple of exceptions that make some sense in their given contexts.</p> <p>Avoid</p> <pre><code>passwd = \"abc\"\ncust = fname = \"John\"\ncomp = \"Intuit\"\n\nd = {\"key\": \"val\"}\nl = [1,2,3]\nt = (1,2)\n</code></pre> <p>Use</p> <pre><code>password = \"abc\"\ncustomer = \"John\"\ncompany = \"Intuit\"\n\nlookup = {\"key\", \"val\"}\nshort_ints = [1,2,3]\ntemp_slice = (1,2)\n\nfor key, value in lookup:\n    if key == \"data\":\n        print(f\"{key=}, {value=}\"\n</code></pre>"},{"location":"tutorials/packaging/anti_patterns/#passing-implicity-data-shapes-arounds","title":"Passing implicity data shapes arounds","text":"<p>Generic containers is great for simple data types but should be avoided to hold nested values.</p> <p>Avoid</p> <pre><code>def process(data:Dict) -&gt; Dict:\n    data_container = data[\"my_key\"][0][-1]\n    pretty_data = prettify(data_container)\n    data[\"my_key\"][0][-1][\"Data\"] = pretty_data\n    return data\n</code></pre> <p>You should use a dataclass or pydantic datamodel to model the object you want to operate on.</p> <p>Use</p> <pre><code>@dataclass\nclass LikedLocation:\n    places: List[POI]\n    likes: List[Number]\n\n    def get_nth_places(self, n:int) -&gt; POI:\n        return self.places[n]\n\n    def get_first_place(self) -&gt; POI:\n        return self.places[0]\n\n@dataclass\nclass POI:\n    visited_by: List[Person]\n\n    def get_nth_visited(self, n:int) -&gt; Person:\n        return self.visited_by[n]\n\n    def get_last_visited(self) -&gt; Person:\n        return self.get_nth_visited(-1)\n\n@dataclass\nclass Person:\n    first_name: str\n    last_name: str\n    date_of_birth: Datetime\n\ndef format_last_visited(locations:LikedLocation) -&gt; `DisplayablePerson`:\n    last_visited_by = locations.get_first_place().get_last_visited()\n    # assume prettify returns an object of `DisplayablePerson` type\n    return prettify(last_visited_by) \n</code></pre>"},{"location":"tutorials/packaging/anti_patterns/#access-protected-members-from-outside-the-class","title":"Access protected members from outside the class","text":"<p>Variables with leading <code>_</code> are considered protected members. Access to this variable directly from outside of the class is dangerous and potential to run time errors.</p> <p>Avoid</p> <pre><code>class Rectangle(object):\n    def __init__(self, width, height):\n        self._width = width\n        self._height = height\n\nrectangle = Rectangle(5, 6)\ncalculate(rectangle._width, rectangle._height)\n</code></pre> <p>This is also an example of over-engineering.  If all you need is some data that stores values. you can simple create dataclasses.</p> <p>Use</p> <pre><code>@dataclass\nclass Rectangle:\n    width: Number\n    height: Number\n\n    def area(self) -&gt; Number:\n        return self.width * self.height\n</code></pre>"},{"location":"tutorials/packaging/anti_patterns/#assigning-to-built-in-reserved-keywards","title":"Assigning to built-in (reserved) keywards","text":"<p>Python has a number of built-in functions that are always accessible in the interpreter. Unless you have a special reason, you should neither overwrite these functions nor assign a value to a variable that has the same name as a built-in function. Overwriting a built-in might have undesired side effects or can cause runtime errors. Python developers usually use built-ins \u2018as-is\u2019. If their behaviour is changed, it can be very tricky to trace back the actual error.</p> <p>Avoid</p> <pre><code>list = [1, 2, 3]\nmy_list = list() # Error: TypeError: 'list' object is not callable\n\nsum = len # reassigned sum to be len operator\nsum(range(10)) # instead of 45, you get 10\n</code></pre>"},{"location":"tutorials/packaging/anti_patterns/#python-is-not-java","title":"Python is not Java","text":"<p>Both Python and Java is object oriented programming langauge. However, there is Java way of doing things and Pythonic way of doing things.  Do not translate directly from Java to Python.</p> <p>Avoid</p> <pre><code>class AreaCalculator:\n\n    @staticmethod\n    def calc_rectangle(width:Number, height: Number) -&gt; Number:\n        return width*height\n</code></pre> <p>the the above example the method <code>calc_rectangle</code> can just be a simple function since it does not require any internal states of the <code>AreaCalculator</code>.</p> <p>Avoid</p> <pre><code>class Rectangle:\n\n    def set_width(self, width:Number):\n        self._width = width\n    def set_height(self, height:Number):\n        self._width = width\n    def get_width(self) -&gt; Number:\n        return self._width\n    def set_heigh(self) -&gt; Number:\n        return self._height\n</code></pre>"},{"location":"tutorials/packaging/anti_patterns/#mixing-positional-and-keyword-arguments-in-function","title":"Mixing positional and keyword arguments in function.","text":"<p>prior to Python3.8 mixing positional argument with keyword arguments can lead to confusing results.</p> <p>Avoid</p> <pre><code>def calculate_compound_interest(principle: Number, \n                                rate: Number, \n                                terms: Number, \n                                compounded_monthly:bool=False, \n                                to_string:bool=False):\n    ...\ninterest = calculate_compound_interest(1_000_000, 2.5, 10, True, False)\ninterest2 = calculate_compound_interest(1_000_000, 2.5, 10, to_string=True, compounded_monthly=True)\n</code></pre> <p>the caller of the function have to be aware of the keyword argument's order because it can be ambigous on which one is first or second.</p> <p>Use</p> <pre><code>def calculate_compound_interest(principle: Number, \n                                rate: Number, \n                                terms: Number, \n                                 /, *, # Changed to indicate positional arguments ends\n                                compounded_monthly:bool=False, \n                                to_string:bool=False):\n    ...\n# interest = calculate_compound_interest(1_000_000, 2.5, 10, True, False) # this will fail\ninterest2 = calculate_compound_interest(1_000_000, 2.5, 10, to_string=True, compounded_monthly=True)\n</code></pre>"},{"location":"tutorials/packaging/anti_patterns/#redundant-context","title":"Redundant Context","text":"<p>Do not add unnecessary data to variable names, especially if the context is already clear</p> <p>Avoid</p> <pre><code>class Person:\n    def __init__(self, person_first_name, person_last_name, person_age):\n        self.person_first_name = person_first_name\n        self.person_last_name = person_last_name\n        self.person_age_name = person_age_name\n</code></pre> <p>Use</p> <pre><code>class Person:\n    def __init__(self, first_name, last_name, age):\n        self.first_name = first_name\n        self.last_name = last_name\n        self.age_name = age_name\n</code></pre>"},{"location":"tutorials/packaging/anti_patterns/#mixing-configs-and-constants","title":"Mixing Configs and Constants","text":"<p>Do not mix stuff that can change vs the values that doesn't change in one file.  If your configs is getting longer than one page, you should consider externalize the config into a file. </p> <p>Avoid</p> <pre><code># inside config.py\nMY_CONST = 123\nMY_DICT = {\n    \"my_key\": \"myvalue\",\n    ...\n    \"last_key\": \"lastvalue\"\n}\n...\n# more stuff\nsome_variable = os.enviorn[\"ENV_VAR_NAME\"]\n</code></pre> <p>Use</p> <pre><code># inside constant.py\nMY_CONST = 123\nMY_DICT = {\n    \"my_key\": \"myvalue\",\n    ...\n    \"last_key\": \"lastvalue\"\n}\n...\n# more stuff\n\n# inside config.py\n# If you only have a few options here\nsome_variable = os.enviorn[\"ENV_VAR_NAME\"]\n</code></pre> <p>Use</p> <pre><code># inside constant.py\nMY_CONST = 123\nMY_DICT = {\n    \"my_key\": \"myvalue\",\n    ...\n    \"last_key\": \"lastvalue\"\n}\n...\n# more stuff\n\n# inside config.py\n# if you have business logic or longer than 5 options\nclass FeatureIsNoneError(Exception):\n    pass\n\nclass TorchModelSetting(BaseModel):\n    version: str\n    device_count: \n\nclass InterpolationSetting(BaseModel):\n    enable_gpu: Optional[bool]\n    model_settings: TorchModelSetting\n    interpolation_factor: Optional[conint(gt=2)]\n    interpolation_method: Optional[str]\n    interpolate_on_integral: Optional[bool]\n\n    class Config:\n        extra = \"forbid\"\n</code></pre>"},{"location":"tutorials/packaging/anti_patterns/#using-in-to-check-containment-in-a-large-list","title":"Using in to check containment in a (large) list","text":"<p>lack of understanding of the internals usually results in unexpected performance impacts. Checking if an element is contained in a list using the <code>in</code> operator might be slow for large lists.  If you must check this often, consider to change the list to a set or use <code>bisect</code>.</p> <p>Avoid</p> <pre><code>list_of_large_items = [...]\nfor key in some_other_iterable:\n    if key in list_of_large_items:\n        do_something()\n</code></pre> <p>Use</p> <pre><code>list_of_large_items = [...]\nlarge_items = set(list_of_large_items)\nfor key in some_other_relatively_large_iterable:\n    if key in large_items:\n        do_something()\n</code></pre> <p>if you only need a few checks, it is better to use <code>bisect</code></p> <p>Use</p> <pre><code>list_of_a_handful_items = [...]\nlist_of_large_items = [...]\n\ndef contains(a, x):\n    'Locate the leftmost value exactly equal to x'\n    i = bisect_left(a, x)\n    if i != len(a) and a[i] == x:\n        return True\n    return False\n\nfor key in list_of_a_handful_items:\n    if contains(list_of_large_items, key):\n        do_something()\n</code></pre>"},{"location":"tutorials/packaging/anti_patterns/#flattening-arrow-code","title":"Flattening Arrow Code","text":"<p>Stacked and nested if (conditional) statements make it hard to follow the code logic. Instead of nesting conditions you can combine them with boolean operators </p> <p>Avoid</p> <pre><code>user = \"Snorlax\"\nage = 30\njob = \"data scientist\"\n\nif age &gt; 30:\n    if user == \"Snorlax\":\n        if job == \"data scientist\":\n            do_science_work()\n        else:\n            do_work()\n</code></pre> <p>Use</p> <pre><code>def is_data_scienstist(age:int, user:str, job:str) -&gt; bool:\n    ...\n\nif is_data_scienstist():\n    do_science_work()\nelse:\n    do_work()\n</code></pre> <p>if the number of conditions is simple you can use boolean operator to connect</p> <p>Use</p> <pre><code>if  age &gt; 30 and user == \"Snorlax\" and job == \"data scientist\":\n    do_science_work()\nelse:\n    do_work()\n</code></pre>"},{"location":"tutorials/packaging/anti_patterns/#string-manipulation-and-formatting","title":"String manipulation and formatting","text":"<p>String is very versatille type and often you need to convert the data into string.  Not using f-string can lead to a lot of code that is also error prune.</p> <p>Using f-string it is usually faster than the <code>format()</code>  or string subsitution method with <code>%</code>.</p> <p>Rounding numbers to lesser digit</p> <p>Avoid</p> <pre><code>pi = 3.1415926\npi_3_sifig = str(math.round(pi, 2))\n</code></pre> <p>Use</p> <pre><code>pi = 3.1415926\npi_3_sifig = f'{pi:.2f}'\n</code></pre> <p>Use</p> <pre><code>interest_rate = 0.14159\ninterest_rate_percentage = f'{interest_rate:.2%}' # will output '14.16%'\n</code></pre> <p>Create leading digits</p> <p>Avoid</p> <pre><code>month = 1\nmonth_printable = \"0\" + str(month) if month &lt; 10 else str(month)\n</code></pre> <p>Use</p> <pre><code>month = 1\nmonth_printable = f'{month:02d}'\n</code></pre> <p>adding separator to stringify your number for presentation</p> <p>Avoid</p> <pre><code>revenue = 1000000000 \nrevenue_printable = some_function_formatter(revenue, separator=\",\") # this will return 1,000,000,000\n</code></pre> <p>Use</p> <pre><code>revenue = 1000000000 \nrevenue_printable = f'{revenue:,d}' # you can replace `,` with whatever separate.\n# If you need to pass dynamic separtor, you can do `f\"{N: {sep}d}\"` where sep is a variable\n</code></pre> <p>If you are using <code>python 3.8</code> or newer You no longer needs to have duplicated logic for constructing text that looks like <code>name=value</code></p> <p>Avoid</p> <pre><code>cost = \"$1,000\"\nprint(f\"cost={cost}\") # or print(\"cost={cost}\".format(cost=cost))\n# or god forbidden \n# print(\"cost =\" + cost)\nprint(\"cost=\" + (10-len(cost)*\" \" + cost) # to print 'cost=      $1,000'\n</code></pre> <p>Use</p> <pre><code>cost = \"$1,000\"\nprint(f\"{cost=}\") # this is a short hand \n# if you need to add spaces before the value\nprint(f\"{cost= :&gt; 10}\") # will print, &gt; is right align 'cost=      $1,000'\n</code></pre> <p>Converting datetime or part of it to string</p> <p>Avoid</p> <pre><code>today = datetime.datetime(year=2021, month=8, day=8)\ntoday_ISO8601_parts = today.strftime(\"%Y-%m-%d\").split(\"-\")\nprint(f\"today in discouraged format: {today_ISO8601_parts[1]}/{today_ISO8601_parts[2]}/{today_ISO8601_parts[0]}\")\n</code></pre> <p>Use</p> <pre><code>today = datetime.datetime(year=2021, month=8, day=8)\nprint(f\"today in discouraged format: {today:%m/%d/%Y}, ISO8601 format: {today:%Y-%m-%d}\")\n</code></pre>"},{"location":"tutorials/packaging/anti_patterns/#not-using-defaultdict-or-counter-instead-of-dict","title":"Not using <code>defaultdict</code> or <code>Counter</code> instead of <code>dict</code>","text":"<p>Python's <code>dict</code> is a very versatile container, but it shouldn't be used in certain cases.</p> <ul> <li>You are trying to count things</li> <li>You are manually adding default values</li> </ul> <p>Avoid</p> <pre><code>text = \"some long text you are trying to count the frequency of words.\"\n\nword_count_dict = {}\nfor w in text.split(\" \"):\n    if w in word_count_dict:\n        word_count_dict[w]+=1\n    else:\n        word_count_dict[w]=1\n</code></pre> <p>Use</p> <pre><code>from collections import defaultdict\nword_count_dict = defaultdict(int)\nfor w in text.split(\" \"):\n    word_count_dict[w]+=1\n</code></pre> <p>Or even better to use <code>Counter</code></p> <p>Use</p> <pre><code>from collections import Counter\nword_counts = Counter(text.split(\" \"))\n</code></pre>"},{"location":"tutorials/packaging/anti_patterns/#using-dict-or-mutable-type-for-immutable-data","title":"Using <code>dict</code> or mutable type for immutable data","text":"<p>Often <code>dict</code> is used as general container of data like config values and etc.  However, if the data is suppose to be immutable, then you really should use the correct immutable types to avoid accidental data overwrite. </p> <p>Avoid</p> <pre><code>config = {}\n\nconfig[\"service\"] = \"https://www.intuit.com/prod/service_name\"\nconfig[\"rate\"] = 25\nconfig[\"country\"] = \"US\"\n</code></pre> <p>Use</p> <pre><code>from dataclasses import dataclass\n\n@dataclass(frozen=True)\nclass Config:\n    service: str\n    rate: int\n    country: str\n</code></pre>"},{"location":"tutorials/packaging/anti_patterns/#using-string-instead-of-enum","title":"Using string instead of enum","text":"<p>String types are a poor choice when the list of possibility is fairly small and finite.   Using string type also could leads to risk of misspelling, escaping exhaustive checks with linters or pattern matching and code duplications.</p> <p>Avoid</p> <pre><code>def train(classifier=\"tree\"):\n\n    if classifier == \"tree\":\n        pass\n    elif classifier == \"forest\":\n        pass\n    elif classifier == \"cnn\":\n        pass\n    else:\n        raise ValueError\n</code></pre> <p>Use</p> <pre><code>from enum import Enum\n\nclass Classifier(Enum):\n    TREE = 0\n    FOREST = 1\n    CNN = 2\n\ndef train(classifer: Classifier = Classifier.LBFGS):\n\n    if classifer == Classifier.TREE:\n        pass\n    elif classifer == Classifier.FOREST:\n        pass\n    elif classifer == Classifier.CNN:\n        pass\n    else:\n        raise ValueError\n</code></pre> <p>with python3.10, you can do pattern matching.</p>"},{"location":"tutorials/packaging/api_design/","title":"API Design","text":"<p>Summary</p> <p> Use a flat namespace for packages/modules.</p> <p> Use primitive-typed functions/methods.</p> <p> High cohesion &amp; low coupling.</p> <p> Avoid Low cohension &amp; high coupling.</p> <p> Avoid pandas objects as arguments and returns.</p> <p> Avoid exposing classes from packages/modules.</p>"},{"location":"tutorials/packaging/api_design/#api-design","title":"API Design","text":"<p>This guide uses the term \"API\" to refer to the interface of a package, module,  class or function. It outlines specific recommendations for each type, but general rules apply to all.</p> <p>API design is arguably the most important part of writing clean reusable code. A well-designed API should be intuitive and straightforward for users, requiring minimal reference to documentation.</p> <p>This topic is particularly relevant to data science code because API design often becomes an afterthought compared to the actual functionality and data modeling. This leads to code that's often single-use, even when the core functionality is quite general. Additionally, duplicate functionality gets implemented in separate repositories by different developers who find it not worthwhile to adapt hardcoded/specialized logic to their projects.</p> <p>This problem can also exist within a single project/repository. Modules can be tailored so specifically to a dataset or model architecture that using them outside of that context becomes impossible.</p>"},{"location":"tutorials/packaging/api_design/#goals-of-a-good-api","title":"Goals of a Good API","text":"<p>A well-designed API should exhibit the following properties:</p> <ul> <li>Legibility: Users should understand how to utilize an API immediately. The meaning of arguments and return values should be obvious, requiring minimal documentation.</li> <li>State Simplicity: Users should readily comprehend the state's lifetime within an API. There should be no hidden caching behavior, allowing users to deallocate state on demand.</li> <li>Interoperability: Users should immediately understand how to utilize library functions/classes with functions/classes from other libraries, even outside of the given library.</li> </ul>"},{"location":"tutorials/packaging/api_design/#example-libraries","title":"Example Libraries","text":"<ul> <li>Python Standard Library: This library embodies a consistent style and uses patterns that make it very user-friendly.</li> <li>NumPy: This function-oriented library exposes one primary class and numerous functions. For most <code>numpy</code> functions, the first argument is typically a <code>np.ndarray</code>, and the output is also a <code>np.ndarray</code>. This means that understanding the vast array of functions in the library only necessitates familiarity with one object type.</li> <li>Scikit-learn: This object-oriented library exposes many model classes. Most <code>scikit-learn</code> objects inherit from a small subset of base classes with well-defined methods. Nearly all <code>scikit-learn</code> objects have a <code>fit(X, y)</code> method and a <code>predict(X)</code> method. This consistency allows most scikit-learn objects to be interchangeable within user application code.</li> </ul> <p>The libraries mentioned above are all heavily used within data science code and they follow design patterns that we can learn from. We can derive a base  set of guidelines from the patterns we see in these interfaces.</p>"},{"location":"tutorials/packaging/api_design/#design-guidelines","title":"Design Guidelines","text":"<p>These guidelines are principles to strive for when designing an API, but not absolute rules. Exceptions exist:</p> <ol> <li>Use a flat namespace for packages/modules.</li> <li>Use primitive types in function/method interfaces.<ul> <li>Avoid dictionary-oriented interfaces.</li> <li>Avoid pandas-oriented interfaces.</li> </ul> </li> <li>Avoid variable length keyword arguments (**kwargs)</li> <li>Avoid exposing stateful objects. Use mostly functions.</li> <li>Minimize disk access and serialization.</li> </ol>"},{"location":"tutorials/packaging/api_design/#namespace","title":"Use a flat namespace for packages/modules","text":"<p>This makes it so that a developer coming into your code will always know what  publicly accessible identifiers are available at the library level. This also avoids having to make multiple imports from the same library and polluting application code namespace with library objects/modules. Furthermore this  discourages users from aliasing imports since the library only exposes a single  namespace.</p> <p>Use</p> <pre><code>import my_library\n\nresult = my_library.submodule1.function1()\nobj = my_library.submodule1.Class1()\n\nresult2 = my_library.submodule2.function2()\nobj2 = my_library.submodule2.Class2()\n</code></pre> <p>Avoid</p> <pre><code>from my_library.submodule1 import function as function1, Class1\nfrom my_library.submodule2 import function as function2, Class2\n\nresult = function1()\nobj = Class1()\n\nresult2 = function2()\nobj2 = Class2()\n</code></pre> <p>An example of a library that does this extremely well is numpy.</p>"},{"location":"tutorials/packaging/api_design/#types","title":"Use primitive types in function/method interfaces","text":"<p>The goal is to create simple and intuitive function/method interfaces by primarily using primitive data types (e.g., <code>str</code>, <code>int</code>, <code>float</code>, <code>bool</code>). This promotes code readability, maintainability, and testability.</p> <p>Why Primitive Types? - Clarity: Functions with clear input and output types are easier to understand and use. - Testability: Unit tests can be written more easily when dealing with simple data types. - Flexibility: Functions that rely on primitive types are more adaptable to different use cases.</p>"},{"location":"tutorials/packaging/api_design/#when-to-consider-complex-data-structures","title":"When to Consider Complex Data Structures","text":"<p>While primitive types are generally preferred, there are situations where complex data structures like dictionaries or DataFrames might be necessary:</p> <ul> <li>Large Data Sets: For performance reasons, using NumPy arrays or Pandas DataFrames can be more efficient.</li> <li>Structured Data: Dictionaries can be useful for representing key-value pairs, especially when the structure is not fixed.</li> <li>Domain-Specific Data: In some cases, using custom data classes might be appropriate to model complex domain-specific concepts.</li> </ul> <p>However, even when using complex data structures, strive to minimize their complexity and expose a simple interface to the user.</p>"},{"location":"tutorials/packaging/api_design/#best-practices","title":"Best Practices","text":"<ol> <li>Prioritize Primitive Types: Always consider using primitive types as the primary building blocks of your API.</li> <li>Use Type Hints: Employ Python's type hinting system to improve code readability and catch potential type errors early.</li> <li>Minimize Dictionary Usage: If dictionaries are necessary, consider using namedtuples or custom data classes for structured data. 4. Optimize Object Usage: When working with Object, focus on exposing a simple interface that hides the underlying complexity.</li> <li>Handle Errors Gracefully: Implement robust error handling mechanisms to prevent unexpected behavior and provide informative error messages. By following these guidelines, you can create APIs that are both powerful and easy to use.</li> </ol>"},{"location":"tutorials/packaging/api_design/#dictionary-types","title":"Dictionary-Oriented Interfaces","text":"<p>A common pattern is to use dictionary arguments and return values. The problem  with this kind of design is it often creates an implicit API which is not  immediately obvious from the function signature. A function that can be  understood from inspecting the name and arguments is generally better than one that requires reading the docstring.</p>"},{"location":"tutorials/packaging/api_design/#dictionary-arguments","title":"Dictionary Arguments","text":"<p>Often times deeply nested structures are required as input  arguments. These structures are nearly always unnecessary to perform the core  logic/utility provided by the code. The structural requirements of the  dictionary cannot be known by inspecting the function arguments. The user of  the function must look at the documentation or the function body to understand  how to use it.</p> <p>Avoid</p> <pre><code>def join_first_and_last(parts, sep=' '):\n    return parts[\"first\"] + sep + parts[\"last\"]\n\n# Example call\ndata = {\"first\": \"hello\", \"last\": \"world\"}\nresult = join_first_and_last(data)\n</code></pre> <p>Solution: Force the user to destructure the data prior to passing it in to  the function. This is a good practice because it limits the scope of what each  function \"knows\" about. If the function can be written in a way that the  context in which is it called is unimportant, this makes the code more  reusable and easier to test.</p> <p>Use</p> <pre><code>def join_first_and_last(first, last, sep=' '):\n    return first + sep + last\n\n# Example call\ndata = {\"first\": \"hello\", \"last\": \"world\"}\nresult = join_first_and_last(data[\"first\"], data[\"last\"])\n</code></pre> <p>The most common exceptions to using a dictionary as an argument  is when the dictionary is meant to be iterated over within the function or used as a lookup table.</p> <p>Exception</p> <pre><code># Iteration usage\ndef max_key_length(dictionary: dict):\n    return max(map(len, dictionary.keys()))\n\n# Lookup table usage\ndef largest_zip_code_population(zip_codes: list, zip_to_population: dict):\n    return max(zip_to_population.get(code, -1) for code in zip_codes)\n</code></pre>"},{"location":"tutorials/packaging/api_design/#dictionary-returns","title":"Dictionary Returns","text":"<p>When dictionaries are used as return values, this causes a  similar problem to using them as an arguments. The user of the function cannot determine the structure of the dictionary by inspecting the function signature.  This also nearly always requires that the user to destructure the return value in order to use it.</p> <p>Avoid</p> <pre><code>def precision_recall(model, x_test, y_test):\n    precision = model.precision(x_test, y_test)\n    recall = model.recall(x_test, y_test)\n    return {\"precision\": precision, \"recall\": recall}\n\n# Example call\nresult = precision_recall(...)\nprecision = result[\"precision\"]\nrecall = result[\"recall\"]\n</code></pre> <p>Use tuples as returns. This avoids forcing the user to  destructure the return value. Notice that in the above example the dictionary  keys needed to be duplicated in the calling code in order to access the data. </p> <p>Use</p> <pre><code>def precision_recall(model, x_test, y_test):\n    precision = model.precision(x_test, y_test)\n    recall = model.recall(x_test, y_test)\n    return precision, recall\n\n# Example call\nprecision, recall = precision_recall(...)\n</code></pre> <p>The most common exception to using a dictionary as a return type  is when the  dictionary is meant to be iterated over by the calling code or used as a lookup table.</p> <p>Exception</p> <pre><code>def get_column_types():\n    return {\n        'zip_code': str,\n        'salaries_and_wages': int,\n        'flag_old_or_blind': bool,\n    }\n</code></pre>"},{"location":"tutorials/packaging/api_design/#pandas-types","title":"Pandas-Oriented Interfaces","text":"<p>The worst functions are those that consume DataFrames as arguments and have very  specific requirements on what columns are present. This is one of the most common anti-patterns found in data science python code. This creates an implicit  API that in the worst case (and most common case) requires that the developer  read the entire function body to understand how to use it. Imagine if you needed to read the body of every function included in the python standard library in  order to understand it. We take it for granted that the API is so simple and easy to understand.</p>"},{"location":"tutorials/packaging/api_design/#pandas-arguments","title":"Pandas Arguments","text":"<p>This is the most common type of function in data science codebases (often  significantly more complex).</p> <p>Avoid</p> <pre><code>def typecast_zip_codes(df: pd.DataFrame):\n    df['zip'] = df['zip'].astype(int)\n\n# Example call\ntypecast_zip_codes(df)\n</code></pre> <p>It is a function designed for a specific dataset that assumes that the calling  code has a DataFrame with a specific column. It is unclear from the signature that the DataFrame is modified in-place. It is unclear from the signature if there is a return that should be used.</p> <p>Solution: Make the API oriented around a series/array and to leave the  deconstruction of the DataFrame to the calling code.</p> <p>Use</p> <pre><code>def typecast_zip_codes(array: np.ndarray):\n    return array.astype(int)\n\n# Example call\ndf['zip'] = typecast_zip_codes(df['zip'].values)\n</code></pre> <p>The calling code is burdened with the responsibilty of deconstructing/modifying the DataFrame but this is a good thing. This allows the calling code to fully understand the side-effects of the call without having to look at the  implementation. This is easier to test and much more maintainable. If this kind  of API is used throughout the codebase, then there will be very few places that  have assumptions about DataFrame structure. </p>"},{"location":"tutorials/packaging/api_design/#pandas-class-arguments","title":"Pandas Class Arguments","text":"<p>A similar problem occurs in object-oriented library design but to a worse  degree. A common anti-pattern in data science code is to unnecessarily save  entire datasets as a class member (or within multiple classes). This is  generally done so that the class can later reference that member variable  from a method without including it in the method argument list.</p> <p>Avoid</p> <pre><code>class MyPredictor:\n    def __init__(self, df: pd.DataFrame, config: dict):\n        self.train_df = df\n        self.iterations = config.get('iterations', 0)\n    def train(self):\n        ...\n    def predict(self):\n        ...\n</code></pre> <p>In this code it is confusing what the lifetime of the <code>df</code> argument is and what it is even used for. An object should only track state that must be present  during the entire lifetime of the object. If the <code>df</code> is only used in the  <code>train</code> method then it should not be passed to the constructor.</p> <p>A second issue here is that the config dictionary argument creates an implicit  API. The user of the class cannot tell what the contents of the config  dictionary should be without looking at the implementation of the constructor.  This is why frameworks like <code>sklearn</code> make all of their hyperparameters explicit  in their constructors. It allows you to be able to tell at the call-site if you  are configuring the estimator correctly. Imagine if the caller had made a typo  in a dictionary key for the example above. The mistake would be silently  ignored unless an explict check was implemented. An explicit check would  introduce unnecessary complexity when a language feature (arguments) is already available to handle this.</p> <p>Solution: Colocate the DataFrame with its usage and make the class  constructor take explicit arguments.</p> <p>Use</p> <pre><code>class MyPredictor:\n    def __init__(self, n_iterations: int):\n        self.n_iterations = n_iterations\n    def train(self, X: np.ndarray, y: np.ndarray):\n        ...\n    def predict(self, X: np.ndarray):\n        ...\n</code></pre>"},{"location":"tutorials/packaging/api_design/#kwargs","title":"Avoid variable length keyword arguments (**kwargs)","text":"<p>Variable length keyword arguments should almost never be used because they hide  what is actually being done with them. Whenever they are used, the user will almost always have to read the documentation to understand how they are being used.</p> <p>Avoid</p> <pre><code>def example(arg, **kwargs):\n    if 'foo' in kwargs: \n        ...\n</code></pre> <p>The only time when keyword arguments reduce ambiguity is if your library is  wrapping a call to another library which is highly configurable. Rather than  replicating all of the keyword arguments in the calling function signature,  forward the configuration parameters with kwargs.</p> <p>Use</p> <pre><code>import foo_library\ndef example(arg, **foo_params):\n    result = foo_library.example_call(**foo_params)\n    ...\n</code></pre> <p>Another potential use-case where keyword arguments are useful is if the  intention is to iterate over the key-value pairs  (for example to use as metadata). Because this actually changes the call syntax of the function, it is generally more explicit to pass this kind of object in as a dictionary instead.</p> <p>Avoid</p> <pre><code>def example(**tags):\n    ...\n</code></pre> <p>Use</p> <pre><code>def example(tags: dict):\n    ...\n</code></pre>"},{"location":"tutorials/packaging/api_design/#classes","title":"Avoid exposing stateful objects","text":"<p>Only expose classes when a library/module must manage state. Object-oriented data science code is very useful when complex state cannot be abstracted into procedures without exposing many different internal state variables to the end user. However, nearly all object-oriented code can be written in a  procedure-oriented style. The benefit of procedure-oriented code is that it  makes the lifetime and usage of in-memory state well-understood.</p>"},{"location":"tutorials/packaging/api_design/#single-method-classes","title":"Single Method Classes","text":"<p>The most common object-oriented anti-pattern is when a class has a constructor and only a single public method. This can almost always be replaced with a  function call with zero cost.</p> <p>Avoid</p> <pre><code>class Greeter:\n    def __init__(self, name):\n        self.name = name\n\n    def greet(self):\n        return f'Hello, {self.name}!'\n</code></pre> <p>Use</p> <pre><code>def greet(name):\n    return f'Hello, {name}!'\n</code></pre>"},{"location":"tutorials/packaging/api_design/#argument-avoidance","title":"Argument Avoidance","text":"<p>Another common anti-pattern is the use of classes to reduce the number of  arguments on each method call. Rather than scoping arguments to the functions  they are relevant to, all arguments for all methods are passed to the  constructor and then 0-argument methods called. This design pattern often occurs  when developers attempt to define a process with an object.</p> <p>Avoid</p> <pre><code>from sklearn.tree import DecisionTreeRegressor\n\nclass MyModel:\n    def __init__(self, X_train, y_train, X_test, y_test, hyperparams):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.X_test = X_test\n        self.y_test = y_test\n        self.hyperparams = hyperparams\n        self.model = None\n\n    def create_model(self):\n        self.model = DecisionTreeRegressor(**self.hyperparams)\n\n    def train(self):\n        if not self.model:\n            self.create_model()\n        self.model.fit(self.X_train, self.y_train)\n\n    def score(self):\n        return self.model.score(self.X_test, self.y_test)\n</code></pre> <p>A common problem with this type of design is that the class methods have a  required call order that is not explicit in the API. In the example above, this is remedied by checking if member variables have been set. This complicates the  logic, adds almost no value, and distracts from the actual logical operations being performed.</p> <p>Use</p> <pre><code>from sklearn.tree import DecisionTreeRegressor\n\ndef create_model(hyperparams):\n    return DecisionTreeRegressor(**hyperparams)\n\ndef train(model, X_train, y_train):\n    model.fit(X_train, y_train)\n\ndef score(model, X_test, y_test):\n    return model.score(X_test, y_test)\n</code></pre>"},{"location":"tutorials/packaging/api_design/#serialization","title":"Minimize disk access and serialization.","text":"<p>It is common for data science code to produce and consume many artifacts during both training and prediction. This causes many codebases to become saturated  with disk access to these artifacts and hardcoded paths in every single module.  In codebases where this is left unchecked, disk access can be nearly everywhere  and cause huge performance issues. It is common to find a class which has a  path as a constuctor argument to load some  config or data.</p> <p>Functions/classes should almost never read from disk to execute the provided  logic. The logic and the state management should always be separated.</p> <p>The following example may seem like an egregious use of serialization, but  these patterns can all be commonly found in code.</p> <p>Avoid</p> <pre><code>import numpy as np\nimport yaml\n\nclass Encoder:\n    def __init__(self, config_filename: str, categories_filename: str):    \n        config = yaml.load(config_filename, Loader=yaml.FullLoader)\n        self.min_frequency = config.get('min_frequency', 10)\n        self.default = config.get('default', -1)\n\n        self.categories = None\n        if categories_filename is not None:\n            self.categories = np.load(categories_filename)\n\n    def train(self, training_filename: str):\n        values = np.load(training_filename)\n        unique, counts = np.unique(values, return_counts=True)\n        categories = unique[counts &gt;= self.min_frequency]\n        self.categories = categories\n\n    def predict(self, test_filename):\n        values = np.load(test_filename)\n        lookup_table = dict(zip(self.categories, range(len(self.categories))))\n        return np.array([lookup_table.get(item, self.default) for item in values])\n\n    def save(self, output_filename):\n        np.save(output_filename, self.categories)\n</code></pre> <p>The constructor expects two arguments, one for configuration and one for a  previously saved state. Unless the function body were read, it would be unclear that the second argument was used to load previously saved state. Next, each function takes a filename as input instead of an in-memory object which makes  the object far more difficult to use.</p> <p>Solution: The exposed API should always assume that configurations and parameters will be in-memory (unless they absolutely cannot be). This vastly simplifies the usage of the object.</p> <p>Use</p> <pre><code>import numpy as np\n\nclass Encoder:\n    def __init__(self, min_frequency=10, default=-1):    \n        self.min_frequency = min_frequency\n        self.default = default\n        self.lookup_table = None\n\n    def train(self, values: np.ndarray):\n        unique, counts = np.unique(values, return_counts=True)\n        categories = unique[counts &gt;= self.min_frequency]\n        self.lookup_table = dict(zip(categories, range(len(categories))))\n\n    def predict(self, values):\n        return np.array([self.lookup_table.get(item, self.default) for item in values])\n</code></pre> <p>This design requires that the <code>Encoder</code> object would be serialized by  user. This is preferable to implementing custom serialization logic as part of the class since the user can use common shared serialization facilities  (e.g. pickle) to manage disk access.</p>"},{"location":"tutorials/packaging/api_design/#cohesion-coupling","title":"Cohesion &amp; Coupling","text":"<p>High cohesion is when you have a class that does a well defined job.  Low cohesion is when a class does a lot of jobs that does't have much in common.</p> <p>High coupling is when modules are tightly interconnected via many complex interfaces and information flows. Low coupling is when modules are loosely interconnected and isolated from the implementation details of each other.  One module can be changed or replaced without impacting other modules. </p> <p>Coupling and cohension goes in pairs like the two faces of a coin.  </p> <p>Cohension is like an onion -- it requires layers to keep things separate and distinct.  It is not just about the size of your functions but also the relationships with each other. </p> <p>Coupling is like a knife -- it cuts through the layers and connects multiple facets. It is not just about the number of objects but also the degree of mutual interdependence.</p> <p>A good book for this is described in Chapter 10 of code like a pro book.</p>"},{"location":"tutorials/packaging/code_style/","title":"Code Style","text":"<p>Summary</p> <p> Use ruff or black code formatting.</p> <p> Use either napoleon docstring type annotations or PEP-484 built-in type annotations.</p> <p> Use ruff for linting (static code analysis).</p> <p> Use mypy for type checking.</p>"},{"location":"tutorials/packaging/code_style/#code-style","title":"Code Style","text":"<p>A consistent code style makes code easier to read and understand. The main  benefit of increased legibility and beautiful code is saving developer time. A  consistent code style allows anyone to quickly familiarize themselves with a  codebase even if it is being worked on by many people. A recommended read is the clean code python. </p> <p>There are many tools/guidelines which can be used to for code style in python  so a common question is which one to use. The reality is that most of the  tools/guidelines are fairly good and have generally useful default behavior. </p> <p>In practice, for most tools, warnings end up being ignored or specific  linting options are turned off because they are too restrictive. If a developer  of a repository disagrees with an option, then they will just remove it  entirely so that it does not bother them anymore.</p> <p>When using a set of guidelines which does not have a tool associated with it,  rules are often forgotten or implemented differently by different developers.</p>"},{"location":"tutorials/packaging/code_style/#goals","title":"Goals","text":"<p>Due to the reasons above, the goal of a styling specification would be to  have the following properties:</p> <ul> <li>Automation: The code style must have a tool. It must have automated      formatting so that users do not need to worry about styling the code      manually.</li> <li>Minimal Configuration: Minimize options so that users do not need to make     decisions about style. More decisions leads to more configuration and more     inconsistencies in code style.</li> <li>Customizable Rules: The tool should allow leads to set the type of rules     going to be enforced and rules can be ignored.</li> </ul>"},{"location":"tutorials/packaging/code_style/#example-tools","title":"Example Tools","text":"<p>The following are the most commonly used tools for python code quality and ensure consistency.</p>"},{"location":"tutorials/packaging/code_style/#linters","title":"Linters","text":"<p>Linters are usually broken into two types: Logical and Stylistic linters.</p> <p>Logical Linter:</p> <ul> <li>Code Errors</li> <li>Code with potentially unintended results</li> <li>Dangrous code patterns</li> </ul> <p>Stylistic Liners:</p> <ul> <li>Code not conforming to defined conventions.</li> </ul> <p>Below are some of the popular tools</p> <ul> <li> <p>ruff; A modern(Rust) linter that combines the best features of flake8, pylint, and isort. It's highly customizable and offers fast performance.</p> </li> <li> <p>flake8: the wrapper which verifies PEP-8, pyflakes, and circular complexity \u201c. It has a low rate of false positives.</p> </li> </ul>"},{"location":"tutorials/packaging/code_style/#formatters","title":"Formatters","text":"<p>Formatters will format the actual python file based on rules.</p> <ul> <li> <p>ruff A drop-in replacement for black, but much faster.</p> </li> <li> <p>black An automated code formatter with no configuration options.</p> <p>A huge benefit of black is that unlike the previously mentioned tools this  does not have formatting options. Black does not strictly follow  PEP-8 guidelines but is generally compliant.  It is also important to  understand that black is only an opinionated <code>formatter</code>, it does not check all of the styles problem that other <code>linters</code> do. </p> </li> <li> <p>isort Format imports order.</p> <p>Sort imports alphabetically, and automatically separated into sections and by type.</p> </li> </ul> <p>In a very controlled development environment, each may be useful tools by just enabling the default behavior and forbidding custom options.</p>"},{"location":"tutorials/packaging/code_style/#recommendation","title":"Recommendation","text":"<p>For a comprehensive and efficient code style and linting solution, we highly recommend using <code>ruff</code>. It offers a powerful combination of features and performance benefits:</p> <ul> <li>Fast Performance: Leverages Rust for speed, making it ideal for large codebases.</li> <li>Comprehensive Linting: Checks for a wide range of style issues, potential bugs, and performance problems.</li> <li>Customization: Easily tailor the linting rules to your specific needs.</li> <li>Compatibility: Seamlessly integrates with popular tools like <code>black</code> and <code>isort</code>.</li> </ul> <p>For formatting, <code>black</code> remains a solid choice for its simplicity and strict adherence to a specific style. However, for those seeking a faster alternative, <code>ruff</code> can also be used as a drop-in replacement.</p> <p>To ensure type safety, <code>mypy</code> should be used to statically type-check your code.</p>"},{"location":"tutorials/packaging/code_style/#style-guides","title":"Style Guides","text":"<p>Beyond automated checks and warnings for code, style guides provide good  recommendations for how you should write code.</p> <p>Three style guides in particular provide almost universally useful advice:</p> <ul> <li>PEP-8: The official python style guide provides the basis for almost all    formatters and other style guides. This provides the broadest and most    foundational rules for writing legible python code.  </li> <li>Google Style Guide: This has a lot of useful recommendations, however many of    their guidelines were chosen to maximize compatibility between python 2 and    python 3. Since python 2 is officially deprecated, all of the compatibility   guidelines should be ignored. For these reasons the main</li> <li>Hitchhiker's Guide to Python: This provides its own style guide which   is universally regarded as good practice.</li> </ul> <p>Though none of these style guides can programmatically format code for you, they are essential reference for how to write clean, simple python code.</p>"},{"location":"tutorials/packaging/code_style/#type-annotations","title":"Type Annotations","text":"<p>Type annotations of some kind are recommended but can be implemented in  different ways. Some form of type annotation is always recommended so that code usage is less ambiguous.</p> <p>The recommended strategies for annotating the types of your code:</p>"},{"location":"tutorials/packaging/code_style/#pep-484-style-type-annotations","title":"PEP-484 Style type annotations","text":"<p>This places the type annotation directly in code. </p> <p>Pros:</p> <ul> <li>Code can be checked using automated code checkers (See: mypy &amp; pyre-check)</li> </ul> <p>Cons:</p> <ul> <li>Tends to make function declarations verbose</li> <li>Can be difficult to document data structures (Requires importing typing)</li> </ul> <pre><code>def subtract(minuend: int, subtrahend: int) -&gt; int:\n    \"\"\"\n    Subtract the subtrahend from the minuend.\n\n    Args:\n        minuend: The basis to subtract from.\n        subtrahend: The value to subtract.\n    Result:\n        difference: The difference between the numbers.\n    \"\"\"\n    return minuend - subtrahend\n</code></pre>"},{"location":"tutorials/packaging/code_style/#variable-naming","title":"Variable Naming","text":"<p>In the style guides mentioned above, the naming conventions are primarily concerned with the case used (snake_case vs CamelCase, etc.) However, this section is in regards to the actual words which should be used in a name.</p> <p>Naming is one of the hardest problems in programming. Good naming can  drastically increase the readability of your code. To name things well,  variables should have the following properties:</p> <ul> <li>Descriptive</li> <li>Unambiguous</li> <li>Should not contain the type. This is what type annotations are for</li> <li>Should be short if possible. Long names make code more difficult to read</li> </ul> <p>Abbrivations for names is ok if it is well-known but should be reframed. ie. <code>n</code>, <code>sz</code>, <code>cnt</code>, <code>idx</code>, <code>dt</code>, <code>ts</code>, <code>env</code>, <code>cfg</code>, <code>ctx</code> etc. </p>"},{"location":"tutorials/packaging/code_style/#dictionaries","title":"Dictionaries","text":"<p>Dictionaries should have information about both the key and value in  the name. </p> <p>Acceptable Forms:</p> <ul> <li><code>{singular-key}_{plural-values}</code> (Preferred) Somewhat ambiguous, but succinct</li> <li><code>{key}_to_{value}</code> Somewhat verbose</li> <li><code>{value}_by_{key}</code> Somewhat verbose, less direct than <code>{key}_to_{value}</code></li> <li><code>{key}_{value}_lookup</code> Somewhat verbose, but describes how it should be used</li> </ul> <p>Each of the forms can be more appropriate in different scenarios where the  meaning is more clear in context.</p> <p>Use</p> <pre><code>word_lengths = {\"hello\": 5, \"world\": 5}\nindex_to_word = {1: \"hello\", 2: \"world\"}\nword_by_index = {1: \"hello\", 2: \"world\"}\nindex_word_lookup = {1: \"hello\", 2: \"world\"}\n</code></pre> <p>Avoid</p> <pre><code>words = {1: \"hello\", 2: \"world\"}        # Ambiguous, No key information\nword_lookup = {1: \"hello\", 2: \"world\"}  # No key information\nword_dict = {1: \"hello\", 2: \"world\"}    # Type included in name\n</code></pre> <p>In some cases there are well-understood mappings that are meant to be iterated over. In these cases it makes sense to just use the pluralized version of the  word and name the variable after the contents. Anything that is meant to be  iterated over should be pluralized.</p> <p>Exception</p> <pre><code>headers = {\"Content-Type\": \"application/json\"}\ncookies = {\"tz\": \"America/Los_Angeles\"}\nhyperparameters = {\"min_samples_leaf\": 50}\ngunicorn_options = {\"workers\": 8} # Gunicorn uses `options`\n</code></pre> <p>Avoid</p> <pre><code>header_name_to_value = {\"Content-Type\": \"application/json\"}  # Too verbose\ncookie_name_to_value = {\"tz\": \"America/Los_Angeles\"}         # Too verbose\nhyperparameter_name_to_value = {\"min_samples_leaf\": 50}      # Too verbose\n</code></pre> <p>In other cases, there are libraries that have predefined names for their  arguments that do not follow the conventions above. In this case it is  acceptable to follow their conventions when interacting with their code. This makes the code less ambiguous because the library name and the application-code  variable name match.</p> <p>Exception</p> <pre><code>feed_dict = {\"name\": tensor} # TensorFlow uses this name so it is acceptable\n</code></pre>"},{"location":"tutorials/packaging/code_style/#listsseriesarrayssets","title":"Lists/Series/Arrays/Sets","text":"<p>Collections (non-dictionary) should always be the plural version of whatever is  contained within. In the case where the value type is ambiguous, try to name the collection so it is possible to determine what is contained within.</p> <p>Use</p> <p><code>`python zip_codes = [92127, 12345] names = {\"Johnny\", \"Lisa\", \"Mark\", \"Denny\"} column_names = [\"zip_code\", \"wages\"] # `names` suffix indicates string value</code></p> <p>Avoid</p> <pre><code>zip_list = [92127, 12345]        # Do not include type\nlist = [92127, 12345]            # Shadows built-in list, Ambiguous\nitems = [92127, 12345]           # Ambiguous\ncolumns = [\"zip_code\", \"wages\"]  # Value type is ambiguous\n</code></pre>"},{"location":"tutorials/packaging/code_style/#integersfloats","title":"Integers/Floats","text":"<p>When possible, number names should indicate the how the value should be  used. Contrary to the rules regarding collections, there are a many  well-understood values that indicate a number which should be unambiguous. </p> <p>Use</p> <pre><code>limit = 10\nthreshold = 0.7\nsize = 10       # Potentially ambiguous\nindex = 10      # Potentially ambiguous\ncount = 10      # Potentially ambiguous\nn_items = 10    # `n` prefix always indicates a number, preferred `num_items`\nmin_items = 10  # `min` prefix always indicates a number\nmax_items = 10  # `max` prefix always indicates a number\nbuy_price = 10  # Domain specific words always indicate a number \n</code></pre> <p>Avoid</p> <pre><code>items = 10      # Ambiguous, could indicate a collection\nn = 10          # Not Descriptive\nn_value = 10    # `value` suffix is not descriptive\nn_quantity = 10 # `quantity` suffix is not descriptive\n</code></pre>"},{"location":"tutorials/packaging/code_style/#strings","title":"Strings","text":"<p>When possible, string names should indicate how the value should be used. The naming conventions of strings are similar to the conventions of numbers. There  are many well-understood names that could indicate a string type.</p> <p>Common Forms:</p> <ul> <li><code>{descriptor}_key</code> May indicate lookup key</li> <li><code>{descriptor}_name</code> May indicate lookup key</li> </ul> <p>Use</p> <pre><code>environment_name = 'cdev'\nenvironment_key = 'cdev'\nenvironment = 'cdev'      # Well-known string value, Potentially ambiguous\nenv = 'cdev'              # Well-known abbreviation, Potentially ambiguous\n</code></pre> <p>Avoid</p> <pre><code>value = 'cdev'  # Ambiguous\nconfig = 'cdev' # Ambiguous could indicate an object\n</code></pre>"},{"location":"tutorials/packaging/code_style/#classes","title":"Classes","text":"<p>Class instances should always be named after the class itself. For the purposes of describing the forms which a variable should be named the following class name will be used <code>DescriptionNoun</code>.</p> <p>Acceptable Forms:</p> <ul> <li><code>{description}_{noun}</code> (Preferred) CamelCase to snake_case conversion</li> <li><code>{noun}</code> More succinct, potentially ambiguous</li> </ul> <p>Use</p> <pre><code>class ExampleContext:\n    pass\n\nexample_context = ExampleContext()\ncontext = ExampleContext()\n</code></pre>"},{"location":"tutorials/packaging/code_style/#functions","title":"Functions","text":"<p>Function should always be named in lower case and separated with underscore (<code>_</code>). The words that you use to name your function should clearly describe the function\u2019s intent (what the function does).  All functions should clearly indicates the input and output variable types. When returning from function avoid generic container types like <code>List</code> or <code>Dict</code>, use additional hints if you must return such types, ie. <code>List[int]</code> or <code>Dict[str, int]</code>.  </p> <p>Acceptable Forms:</p> <ul> <li><code>def {verb}_{intent}({nouns}:T) -&gt; U:</code> (Preferred)</li> <li><code>def {verb}({noun}:T}) -&gt; U:</code> More succinct, potentially ambiguous</li> </ul> <p>Use</p> <pre><code>def sum(iterable: Iterable) -&gt; Number:\n</code></pre> <pre><code>def remove_underscore(s: str) -&gt; str:\n</code></pre> <p>Avoid</p> <pre><code>def foo(a):\n</code></pre> <pre><code>def process(parameters:Dict) -&gt; Dict:\n</code></pre> <pre><code>def calculate(results:Dict[Any, Dict]) -&gt; Dict:\n</code></pre>"},{"location":"tutorials/packaging/code_style/#ruff-setup","title":"Ruff Setup","text":"<p>It is recommend to setup ruff in your <code>pyproject.toml</code> file. </p> <pre><code>[tool.ruff]\ntarget-version = \"py312\"\n\n# Line length configuration\nline-length = 100  # Black default to 88 \nindent-width = 4\n\n# Same as Black.\n[tool.ruff.format]\nquote-style = \"double\"\nindent-style = \"space\"\nskip-magic-trailing-comma = false\ndocstring-code-format = true\nline-ending = \"auto\"\n\n[tool.ruff.lint]\nselect = [\n    \"A\",  # Assignment expressions\n    \"ARG\",  # Argument-related issues\n    \"B\",  # Builtin usage\n    \"C\",  # Complexity\n    \"COM\",  # Comments\n    \"DTZ\",  # Datetime and time zone issues\n    \"E\",  # Pythonic style guide violations\n    \"EM\",  # Empty blocks\n    \"F\",  # Formatting\n    \"FBT\",  # False positives and benign issues\n    \"I\",  # Imports\n    \"ICN\",  # Inconsistent naming\n    \"ISC\",  # Inconsistent spacing\n    \"N\",  # Naming conventions\n    \"PLC\",  # Potential logical errors\n    \"PLE\",  # Potential library errors\n    \"PLR\",  # Potential runtime errors\n    \"PLW\",  # Potential performance warnings\n    \"Q\",  # Quality of life suggestions\n    \"RUF\",  # Ruff-specific checks\n    \"TID\",  # Type checker issues\n    \"UP\",  # Unused code\n    \"W\",  # Warnings\n    \"YTT\",  # Yield type hints\n]\nignore = [\n    \"FBT003\",  # Ignore a specific false positive or benign issue (boolean-positional-value-in-call)\n]\n\n# Allow autofix for all enabled rules (when `--fix`) is provided.\nfixable = [\"ALL\"]\nunfixable = []\n\n# Files to exclude\nexclude = [\n    \".bzr\",\n    \".direnv\",\n    \".eggs\",\n    \".git\",\n    \".git-rewrite\",\n    \".hg\",\n    \".mypy_cache\",\n    \".nox\",\n    \".pants.d\",\n    \".pytype\",\n    \".ruff_cache\",\n    \".svn\",\n    \".tox\",\n    \".venv\",\n    \"__pypackages__\",\n    \"_build\",\n    \"buck-out\",\n    \"build\",\n    \"dist\",\n    \"node_modules\",\n    \"venv\",\n]\n\n\n[tool.ruff.lint.per-file-ignores]\n# Ignore `E402` (import violations) in all `__init__.py` files\n\"__init__.py\" = [\"E402\"]\n\"tests/**/*\" = [\"D100\", \"D101\", \"D102\", \"D103\", \"D104\"]\n\n[tool.ruff.lint.mccabe]\nmax-complexity = 10\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"  # Follows Google-style docstrings\n\n[tool.ruff.lint.pylint]\nmax-args = 5  # Maximum number of arguments for functions\nmax-returns = 5  # Maximum number of return statements\n\n[tool.ruff.lint.pycodestyle]\nmax-doc-length = 100  # Same as line length\n\n[tool.mypy]\nfiles = [\"your_modules\", \"tests\"]\nno_implicit_optional = true\ncheck_untyped_defs = true\n</code></pre>"},{"location":"tutorials/packaging/logging/","title":"Logging","text":"<p>Summary</p> <p> Use the Loguru loguru library if possible.</p> <p> Use the standard python logging library otherwise.</p> <p> Expose single logger accessible at package level.</p> <p> Expose logging configuration method.</p> <p> Avoid multiple loggers.</p> <p> Avoid using <code>print</code>.</p> <p> Avoid LOG ANY SENSITIVE INFORMATION.</p>"},{"location":"tutorials/packaging/logging/#logging","title":"Logging","text":"<p>With good program architecture debugging is a breeze,   because bugs will be where they should be.</p> <p>-- David May</p> <ul> <li>use a single logger.</li> <li>use <code>lazy</code> logging option if supported instead of checking if logger is enabled for a certain level. </li> <li>consistent naming. The recommended name for your logger is <code>logger</code>.</li> <li>Use the Correct Levels When Logging.</li> <li>Include a Timestamp for each log entry.</li> <li>Adopt the ISO-8601 Format for Timestamps.</li> <li>DO NOT create new methods for handling logs. </li> </ul>"},{"location":"tutorials/packaging/logging/#use-the-correct-levels-when-logging","title":"Use the correct levels when logging","text":"<ul> <li>DEBUG: You should use this level for debugging purposes in development.</li> <li>INFO: You should use this level when something interesting\u2014but expected\u2014happens (e.g., a user sends a new document of certain type to our application).</li> <li>WARNING: You should use this level when something unexpected or unusual happens. It\u2019s not an error, but you should pay attention to it.</li> <li>ERROR: This level is for things that go wrong but are usually recoverable (e.g., internal exceptions you can handle or APIs returning error results).</li> <li>CRITICAL: You should use this level in a doomsday scenario. The application is unusable. At this level, someone should be woken up at 2 a.m.</li> </ul>"},{"location":"tutorials/packaging/logging/#dont-reinvnet-the-wheel","title":"DON'T REINVNET THE WHEEL","text":"<p><code>Print</code> statement is so easy but it comes with a price. </p> <p>You rarely need to subclass the logging module.  Most of the time, you can achieve what you need through <code>Structured logging</code> or creating a new <code>Record</code> for the logger or create a new <code>Handler</code> to handle the record. </p>"},{"location":"tutorials/packaging/logging/#when-to-log","title":"When to log","text":"<p>A rule of thumb when it comes to wehn to log is to think of logs as a story.  If you are trying to tell a story, you should have a beginning, middle and end section.  </p> <ul> <li>Begining of an operation. (e.g. Preparing connection to a service)</li> <li>Middle of an operation. (e.g. update relevant progress on download/upload files, etc.)</li> <li>End of an operation. (e.g. conclude an operation is either succeeded or failed.)</li> </ul>"},{"location":"tutorials/packaging/logging/#what-to-log","title":"What to log","text":"<p>Logs are stories, what to log usually boils down to one of the two themes (Auditing or Diagnostic purpose). </p> <p><code>If I read this log, I know what is going on internally with the system?</code> </p> <p>or </p> <p><code>If I read this log, I know what I need to do to next.</code></p> <p>A word of advice is knowning what you are logging and not log anything senstive.  Assuming what you log is a public record all the time. </p>"},{"location":"tutorials/packaging/logging/#loguru-common-configuration-parameters","title":"Loguru common configuration parameters","text":"<ul> <li>sink\uff1a You can pass in a file object \uff08file-like object\uff09, Or a str String or pathlib.Path object , Or a way \uff08coroutine function\uff09, or logging Modular Handler\uff08logging.Handler\uff09.</li> <li>level (int or str, optional) \uff1a The lowest severity level that recorded messages should be sent to the receiver .</li> <li>format (str or callable, optional) \uff1a Format module , Before sending it to the receiver , Use templates to format recorded messages .</li> <li>filter (callable, str or dict, optional) \uff1a Used to determine whether each recorded message should be sent to the receiver .</li> <li>colorize (bool, optional) : \u2013 Whether the color tags contained in the formatted message should be converted to Ansi Code , Or otherwise . If None, According to whether the sink is TTY Make a choice automatically .</li> <li>serialize (bool, optional) \uff1a Before sending it to the receiver , Whether the recorded message and its record should first be converted to JSON character string .</li> <li>backtrace (bool, optional) \uff1a Whether the formatted exception trace should be extended up , Beyond capture point , To display the full stack trace of the build error .</li> <li>diagnose (bool, optional) \uff1a Whether exception tracking should display variable values to simplify debugging . In production , This should be set to \u201cFalse\u201d, To avoid leaking sensitive data .</li> <li>enqueue (bool, optional) \uff1a Whether the message to be recorded should pass through the multiprocess secure queue before reaching the receiver . When logging to a file through multiple processes , This is very useful . This also has the advantage of making log calls non blocking .</li> <li>catch (bool, optional) \uff1a Whether the errors that occur when the receiver processes log messages should be automatically captured . If True The exception message is displayed on the sys.stderr. however , The exception does not propagate to the caller , To prevent the application from crashing .</li> </ul>"},{"location":"tutorials/packaging/logging/#loguru-formatting-keys","title":"Loguru formatting keys","text":"<p>formatting template properties as follows \uff1a</p> Key Description <code>elapsed</code> The time difference from the beginning of the program <code>exception</code> Formatting exception ( If there is ), Otherwise ' None ' <code>extra</code> User bound property Dictionary ( See bind()) <code>file</code> The file that makes the logging call <code>function</code> Functions for logging calls <code>level</code> Used to record the severity of the message <code>line</code> Line number in source code <code>message</code> Recorded messages ( It's not formatted yet ) <code>module</code> Used to record the severity of the message <code>name</code> Logging calls name <code>process</code> The name of the process making the logging call <code>thread</code> The name of the thread making the logging call <code>time</code> The perceived local time when a log call is made"},{"location":"tutorials/packaging/logging/#recommendated-loguru-formatting-string","title":"Recommendated Loguru formatting string","text":"<pre><code>from loguru import logger, _defaults as loguru_defaults\n\ndef log_format(record):\n    if 'tid' in record[\"extra\"]:\n        return (\n                \"[{time:YYYY-MM-DD HH:mm:ss.SSS}] | \"\n                \"[{extra[tid]}] | \"\n                \"[{level: &lt;8}] | \" \n                \"{process} - {thread} | \"\n                \"{name}:{function: &lt;15}:{line} | \"\n                \"- {message} | \"\n                \"{extra}\"\n            )\n    else:\n        return loguru_defaults.LOGURU_FORMAT+'\\n'\n\nlogger.add(sys.stdout, format=log_format)\n\nconfig = {\n    \"handlers\": [\n        {\n            \"sink\": sys.stderr, \n            \"format\": log_format,\n            \"backtrace\": False,\n            \"diagnose\": True,\n            \"encoding\": \"utf-8\",\n            'level': 'DEBUG',\n        },\n    ],\n     \"extra\": {\n         \"version\": \"GITHASH\"\n     },\n}\nlogger.configure(**config)\n\nwith logger.contextualize(tid=current_tid):\n    logger.info(...)\n</code></pre>"},{"location":"tutorials/packaging/makefile/","title":"Makefile","text":"<p>Summary</p> <p> Assume <code>make</code> is executed inside the virtual environment. </p> <p> Wrap all virtual environment class inside <code>make</code>.</p> <p> Avoid using calls that leaks the environment to system/global. </p>"},{"location":"tutorials/packaging/makefile/#tips","title":"Tips","text":""},{"location":"tutorials/packaging/makefile/#the-basics","title":"The basics","text":"<p>Using <code>Makefile</code>, everything is based on the dependencies and timestamps.  If a dependnecy's timestamp is more recent than the target, then the rule is executed.</p> <p>Take a look of this basic example.</p> <pre><code>.venv/bin/python:\n        python3 -m venv .venv\n\n.venv/.install.stamp: .venv/bin/python pyproject.toml\n        .venv/bin/python -m poetry install\n        touch .venv/.install.stamp\n\ntest: .venv/.install.stamp\n        .venv/bin/python -m pytest tests/\n</code></pre> <p>If the <code>pyproject.toml</code> file is changed when running <code>make test</code>, it will test the rule <code>.venv/.install.stamp</code> and detects there is a new timestamp, which cause this rule to be executed.  The dependency DAGs are traversed recursively. </p>"},{"location":"tutorials/packaging/makefile/#use-variables","title":"Use variables","text":"<pre><code>VENV := .venv\nINSTALL_STAMP := $(VENV)/.install.stamp\nPYTHON := $(VENV)/bin/python\n\n$(PYTHON):\n        python3 -m venv $(VENV)\n\n$(INSTALL_STAMP): $(PYTHON) pyproject.toml\n        $(PYTHON) -m poetry install\n        touch $(INSTALL_STAMP)\n\ntest: $(INSTALL_STAMP)\n        $(PYTHON) -m pytest ./tests/\n</code></pre>"},{"location":"tutorials/packaging/makefile/#environment-variables-with-defaults","title":"Environment variables with defaults","text":"<p>Instead of hardcoding the name of your virtualenv folder, you can read it from the current shell environment and use a default value:</p> <pre><code>VENV := $(shell echo $${VIRTUAL_ENV-.venv})\n</code></pre> <p>In Shell scripts, <code>${VAR_NAME-val}</code> will first try to get <code>$VAR_NAME</code> from the environment, if it can it will use the value that is set by the enviornment, otherwise, it will take the default <code>val</code>. </p> <p>You can pass this variable from the commandline as well</p> <pre><code>LOG_FORMAT=json make test\n\n# or use export to have the variable in env:\n\nexport LOG_FORMAT=json &amp;&amp; make test\n</code></pre>"},{"location":"tutorials/packaging/makefile/#check-if-a-command-is-avaliable","title":"Check if a command is avaliable","text":"<p>It is nice to check if certain command available before blindly execute them in run-time to discover it fails. </p> <pre><code>PY3 := $(shell command -v python3 2&gt; /dev/null)\n\n$(PYTHON):\n        @if [ -z $(PY3) ]; then echo \"python3 could not be found. See https://docs.python.org/3/\"; exit 2; fi\n        python3 -m venv $(VENV)\n</code></pre>"},{"location":"tutorials/packaging/makefile/#list-avaiable-targets","title":"List avaiable targets","text":"<p>when running <code>make</code>, by default it tries to execute <code>all</code> target.  This might not be intended.  We can customize this by doing</p> <pre><code>.DEFAULT_GOAL := help\n\nhelp:\n        @echo \"Please use 'make &lt;target&gt;' where &lt;target&gt; is one of\"\n        @echo \"\"\n        @echo \"  install     install packages and prepare environment\"\n        @echo \"  format      reformat code\"\n        @echo \"  lint        run the code linters\"\n        @echo \"  test        run all the tests\"\n        @echo \"  clean       remove *.pyc files and __pycache__ directory\"\n        @echo \"\"\n        @echo \"Check the Makefile to know exactly what each target is doing.\"\n</code></pre> <p>with <code>.DEFAULT_GOAL</code> it will be executing <code>help</code> instead of the <code>all</code>.</p> <p>with a little shell script magic you can don't have to maintain the help message manually.</p> <pre><code>.DEFAULT_GOAL := help\n\nhelp:  ## Display this help\n        @awk 'BEGIN {FS = \":.*##\"; printf \"\\nUsage:\\n  make \\033[36m\\033[0m\\n\\nTargets:\\n\"} /^[a-zA-Z_-]+:.*?##/ { printf \"\\033[36m%-10s\\033[0m %s\\n\", $$1, $$2 }' $(MAKEFILE_LIST)\n\ndeps:  ## Check dependencies\n        ...\n\nclean: ## Cleanup the project folders\n        ...\n\nbuild: clean deps ## Build the project\n        ...\n</code></pre>"},{"location":"tutorials/packaging/makefile/#phony","title":"PHONY","text":"<p><code>Make</code> by default assumes the target of a rule is a file.  If you have rules (tasks) that does not produce files on disk. (eg. <code>make clean</code> or <code>make test</code>), then you can mark them as <code>.PHONY</code>.  When a target is maked as <code>PHONY</code>, the targets are assumes to be never up-to-date and will always run when invoked.  </p> <pre><code>.PHONY: clean\nclean:\n        rm -rf $(VENV)\n\n.PHONY: test\ntest: ...\n</code></pre>"},{"location":"tutorials/packaging/makefile/#example","title":"Example","text":"<pre><code>NAME := ${PROJECT_NAME-myproject}\nINSTALL_STAMP := .install.stamp\nPOETRY := $(shell command -v poetry 2&gt; /dev/null)\n\n.DEFAULT_GOAL := help\n\n.PHONY: help\nhelp:\n        @awk 'BEGIN {FS = \":.*##\"; printf \"\\nUsage:\\n  make \\033[36m\\033[0m\\n\\nTargets:\\n\"} /^[a-zA-Z_-]+:.*?##/ { printf \"\\033[36m%-10s\\033[0m %s\\n\", $$1, $$2 }' $(MAKEFILE_LIST)\n\ninstall: $(INSTALL_STAMP)\n$(INSTALL_STAMP): pyproject.toml poetry.lock\n        @if [ -z $(POETRY) ]; then echo \"Poetry could not be found. See https://python-poetry.org/docs/\"; exit 2; fi\n        $(POETRY) install\n        touch $(INSTALL_STAMP)\n\n.PHONY: clean\nclean:\n        find . -type d -name \"__pycache__\" | xargs rm -rf {};\n        rm -rf $(INSTALL_STAMP) .coverage .mypy_cache\n\n.PHONY: lint\nlint: $(INSTALL_STAMP)\n        $(POETRY) run isort --profile=black --lines-after-imports=2 --check-only ./tests/ $(NAME)\n        $(POETRY) run black --check ./tests/ $(NAME) --diff\n        $(POETRY) run flake8 --ignore=W503,E501 ./tests/ $(NAME)\n        $(POETRY) run mypy ./tests/ $(NAME) --ignore-missing-imports\n        $(POETRY) run bandit -r $(NAME) -s B608\n\n.PHONY: format\nformat: $(INSTALL_STAMP)\n        $(POETRY) run isort --profile=black --lines-after-imports=2 ./tests/ $(NAME)\n        $(POETRY) run black ./tests/ $(NAME)\n\n.PHONY: test\ntest: $(INSTALL_STAMP)\n        $(POETRY) run pytest ./tests/ --cov-report term-missing --cov-fail-under 100 --cov $(NAME)\n</code></pre>"},{"location":"tutorials/packaging/structure/","title":"Structure","text":"<p>Summary</p> <p> Use pyproject.toml for project and dependency management</p> <p> Use poetry or uv for managing dependency and versions</p> <p> Use tox or nox for environment automation in testing</p> <p> Use pytest for testing framework</p> <p> Avoid deeply nested namespaces (e.g. Java-style)</p>"},{"location":"tutorials/packaging/structure/#structure","title":"Structure","text":"<p>for simple projects.</p> <pre><code>.\n\u251c\u2500\u2500 app\n\u251c\u2500\u2500 tests\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 unit_tests\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 integration_tests\n\u251c\u2500\u2500 .flake8\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 .pre-commit-config.yaml\n\u251c\u2500\u2500 .python-version\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 Makefile\n</code></pre> <p>If your project is rather large, you may opt for the monorepo pattern</p> <pre><code>.\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 libs\n\u2502   \u251c\u2500\u2500 common-lib\n\u2502   \u251c\u2500\u2500 lib-one\n\u2502   \u2514\u2500\u2500 lib-one\n\u251c\u2500\u2500 uv_or_poerty.lock\n\u251c\u2500\u2500 projects\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 project-one\n\u2502   \u2514\u2500\u2500 project-two\n\u2514\u2500\u2500 pyproject.toml\n</code></pre> <p>under <code>/projects</code></p> <p>Project code (Python modules) go here. Each project has its own dependencies.  Each project is considered to have its own releasable artifact.</p> <p>under <code>/libs</code></p> <p>Each lib has its own dependencies. Each lib can optionally depends on the <code>common-lib</code> if there one.</p> <p>project under <code>/projects</code> will use path import of the lib under <code>/libs</code>.</p>"},{"location":"tutorials/packaging/testing/","title":"Testing","text":"<p>Summary</p> <p> Use tox or nox for automation.</p> <p> Use pytest for testing.</p> <p> Use pytest-mock for mocking.</p> <p> Use pytest markers to limit test set.</p> <p> Use mutmut for mutation testing.</p> <p> Use <code># pragma: no cover</code> for lines doesn't need coverage. ie. config lines.</p>"},{"location":"tutorials/packaging/testing/#testing-best-practices","title":"Testing Best Practices","text":"<p>This guide outlines recommended practices for writing effective tests in Python.</p> <ul> <li>Use tox or nox for automation: These tools help manage different environments and dependencies for your tests, simplifying the testing process.</li> <li>Use pytest for testing: Pytest is a popular and powerful testing framework that provides features like parametrization, fixtures, and various plugins for extended functionality.</li> <li>Use pytest-mock for mocking: Mocking allows you to isolate the code under test from its dependencies, making tests more reliable.</li> <li>Use Fixtures in <code>conftest.py</code> to setup your test and building the basic building blocks.<ul> <li>Define reusable test setups in conftest.py using fixtures.</li> <li>Leverage different scopes (session, module, class, and function) for fixtures based on their purpose.</li> </ul> </li> <li>Use parameterized tests to test different input use cases.</li> <li>Ensure a good code coverage.  Preferred to have over 80% coverage.  </li> <li>Mutation test can help you figure out if your unit test is good enough.</li> <li>Pytest have vast amount of plug-ins to help you make your tests better.</li> </ul>"},{"location":"tutorials/packaging/testing/#test-discovery","title":"Test Discovery","text":"<p>Pytest will automatically discover tests matching patterns like <code>test_*.py</code> or <code>*_test.py</code>. Troubleshooting Tip: If tests are missing, use <code>pytest --collect-only</code> to verify that pytest detects all tests and fixtures correctly.</p>"},{"location":"tutorials/packaging/testing/#example-of-using-pytest-plugins","title":"Example of Using Pytest Plugins","text":"<p>Pytest plugins can enhance your testing experience.</p> <pre><code># pyproject.toml\n[pytest]\naddopts = --cov=your_package --cov-report=term-missing\n</code></pre>"},{"location":"tutorials/packaging/testing/#writing-effective-tests","title":"Writing Effective Tests","text":""},{"location":"tutorials/packaging/testing/#fixtures-for-resuable-setups","title":"Fixtures for resuable setups","text":"<p>Fixtures are how test setups (other helpers) are shared between tests. </p> <ul> <li>Can build on top of each other to model complext functionality.</li> <li>Customize functionality by overriding other fixtures .</li> <li>Can be parametrized.</li> <li>Define reusable setups in <code>conftest.py</code> using fixtures.</li> <li>Use different fixture scopes (<code>session</code>, <code>module</code>, <code>class</code>, <code>function</code>) based on purpose:<ul> <li>Session: Set up once per session (e.g., database setup).</li> <li>Module: Set up once per module (e.g., module-level constants).</li> <li>Class: Set up once per test class (e.g., reusable states).</li> <li>Function: Set up for individual tests.</li> </ul> </li> </ul>"},{"location":"tutorials/packaging/testing/#yield-fixture-aka-context-fixture","title":"Yield Fixture aka. Context Fixture","text":"<p>This is a fixture that is created using the yield statement.</p> <pre><code>import pytest\n\n@pytest.fixture\ndef db_connection():\n    connection = setup_database_connection()\n    yield connection\n    teardown_database_connection(connection)\n</code></pre> <p>The code above the <code>yield</code> is executed as setup for the fixture, while the code after the yield is executed as clean-up.   The value yielded is the fixture value received by the user.  Just like all Context-Managers, every call pushes the context on the stack, it follows the <code>LIFO</code> order.  </p>"},{"location":"tutorials/packaging/testing/#fixture-resolution","title":"Fixture resolution","text":"<p>Pytest uses an in-memory DAG (Directed Acyclic Graph) to figure out what is the order of the fixture a test needs.  Each fixture that is required for execution is run <code>once</code>;  The value of the fixture is stored and use to compute the other dependent fixture.  </p>"},{"location":"tutorials/packaging/testing/#execution","title":"Execution","text":"<p>When fixture code is executed, it follows the following rules.</p> <ul> <li>Session scoped fixtures are executed if they have not already been executed in this test run.  Otherwise, the results of previous execution are used.</li> <li>Module scoped fixtures are executed if they have not already been executed as part of this test module in this test run.  Otherwise, the results of the previous execution are used.</li> <li>Class scoped fixture are executed if they have not already been executed as part of this class in the test run. Otherwise, the results of previous execution are used.</li> <li>Function scoped fixtures are executed.</li> </ul> <p>Once all the fixtures are evaluated, the test function is called with the values for the fixtures filled in. </p>"},{"location":"tutorials/packaging/testing/#dont-modify-fixture-values-in-other-fixtures","title":"Don't modify fixture values in other fixtures","text":"<p>Pytest test cases are usually executed in parallel but fixtures are usually executed only once.  When multiple fixtures may depend on the same upstream fixture. If any one of these modifies the upstream fixture\u2019s value, all others will also see the modified value; this will lead to unexpected behavior.</p> <p>If you must override certain values of the parent fixtures, you should make a deepcopy of the data. </p> <p>Avoid</p> <pre><code>@pytest.fixture\ndef engineer():\n    return {\n        \"name\": \"Alex\",\n        \"team\": \"Intuit-AI\",\n    }\n\n@pytest.fixture\ndef ds(engineer):\n    engineer[\"name\"] = \"Joy\"\n    return engineer\n\ndef test_antipattern(engineer, ds):\n    assert engineer == {\"name\": \"Alex\", \"team\": \"Intuit-AI\"}\n    assert ds == {\"name\": \"Joy\", \"team\": \"Intuit-AI\"}\n</code></pre> <p>In the above case, since ds modified the value, all of them will have the name <code>Joy</code> instead.</p> <p>Use</p> <pre><code>@pytest.fixture\ndef engineer():\n    return {\n        \"name\": \"Alex\",\n        \"team\": \"Intuit-AI\",\n    }\n\n@pytest.fixture\ndef ds(engineer):\n    joy = deepcopy(engineer)  # Create a deepcopy of the object\n    joy[\"name\"] = \"Joy\"\n    return joy\n\ndef test_pattern(engineer, ds):\n    assert engineer == {\"name\": \"Alex\", \"team\": \"Intuit-AI\"}\n    assert ds == {\"name\": \"Joy\", \"team\": \"Intuit-AI\"}\n</code></pre>"},{"location":"tutorials/packaging/testing/#test-collection","title":"Test collection","text":"<p>During test collection, every test module, test class, test function in the test-discovery is picked up.  In parallel, every fixture also picked up by inspecting the <code>conftest.py</code> files as well as test modules.  If you ever encountered a program during your test and zero test cases ran, it is usually some problem with this phase.  Check your fixture and paramterized code to see if there is any mistakes there.</p>"},{"location":"tutorials/packaging/testing/#patterns","title":"Patterns","text":"<p>The following patterns should be considered when developing tests.</p>"},{"location":"tutorials/packaging/testing/#fixtures","title":"Fixtures","text":"<p>You should places all your fixtures in various scopes in the correct locations. session scoped fixtures should be located in <code>conftest.py</code> at the package or root level. class scoped and module scoped fixture should be residing the the <code>conftest.py</code> at the same level as your module is defined. function scoped fixture should be in the same file as the actual test cases. </p>"},{"location":"tutorials/packaging/testing/#example-of-parameterized-tests","title":"Example of Parameterized Tests","text":"<p>Parameterized tests allow you to run the same test with different inputs.</p> <pre><code>import pytest\n\n@pytest.mark.parametrize(\"input, expected\", [\n    (1, 2),\n    (2, 3),\n    (3, 4),\n])\ndef test_increment(input, expected):\n    assert input + 1 == expected\n</code></pre>"},{"location":"tutorials/packaging/testing/#example-of-using-mocking","title":"Example of Using Mocking","text":""},{"location":"tutorials/packaging/testing/#using-mocks-effectively","title":"Using Mocks Effectively","text":"<ul> <li>Mocking with pytest-mock: Mock dependencies using <code>pytest-mock</code> to ensure test isolation. This prevents external dependencies (e.g., APIs or databases) from impacting test outcomes.</li> <li>Use spy functions with <code>pytest-mock</code> when you need to monitor call behavior without replacing the function entirely.</li> <li>Eliminate flaky test due to \"mock leak\" when a test does not reset a patch.</li> <li>Reduce boilerplate code</li> </ul> <p>Simple example <pre><code>def test_example_function(mocker):\n    mock_function = mocker.spy(module, 'function_name')\n    result = module.example_function()\n    mock_function.assert_called_once()\n</code></pre> A more complex example <pre><code># inside of test_mocker.py\ndef test_fn():\n    return 42\n\n\nclass TestMockerFixtureSuite(object):\n    \"\"\"\n    Only use a class when you want to organize tests into a suite\n    \"\"\"\n\n    @pytest.mark.xfail(strict=True, msg=\"We want this test to fail.\")\n    def test_mocker(self, mocker):\n        mocker.patch(\"test_mocker.test_fn\", return_value=84)\n        assert another_test_fn() == 84\n        assert False\n\n    def test_mocker_follow_up(self):\n        assert test_fn() == 42    \n\n    @pytest.fixture\n    def mock_fn(self, mocker):\n        return mocker.patch(\"test_mocker.test_fn\", return_value=84)\n\n    def test_mocker_with_fixture(self, mock_fn): # notice this function depends on `mock_fn` to override fixture value\n        assert another_test_fn() == 84\n</code></pre></p>"},{"location":"tutorials/packaging/testing/#prefer-response-frameworks-over-manual-mocking","title":"Prefer Response Frameworks over Manual Mocking","text":"<p>Instead of manually creating HTTP response objects, use tools like responses (for <code>requests</code> library) or respx (for HTTPX) to mock HTTP requests. This improves accuracy in simulating real responses and helps avoid errors in test setup.</p> <p>Never manually create Response objects for tests; instead use the responses library (if using request library) to define what the expected raw API response is. If using any other http library, you can use HTTPretty instead or respx if needs HTTPX support.</p>"},{"location":"tutorials/packaging/testing/#assertions-validations","title":"Assertions &amp; Validations","text":""},{"location":"tutorials/packaging/testing/#parameterized-tests-for-multiple-inputs","title":"Parameterized Tests for Multiple Inputs","text":"<p>Parameterization allows us to asserting the same behavior with various inputs and expected outputs.  Make separate tests for distinct behaviors.  </p> <ul> <li>Copy-pasting code in multiple tests increase boilerplate - use parametrize to reduce this.</li> <li>Never loop over test cases inside a test</li> <li>Parameterize heterogenous behaviors can lead to complex branching codes and bugs.</li> </ul> <pre><code>import pytest\n\n@pytest.mark.parametrize(\"input, expected\", [\n    (1, 2),\n    (2, 3),\n    (3, 4),\n])\ndef test_increment(input, expected):\n    assert input + 1 == expected\n</code></pre>"},{"location":"tutorials/packaging/testing/#clear-error-handling-in-test","title":"Clear Error Handling in Test","text":"<p>Separate test cases for valid and invalid cases. This avoids complex branching within tests and makes errors easier to trace.</p> <pre><code># util.py\ndef divide(a, b):\n    return a / b\n</code></pre> <p>Avoid</p> <pre><code>@pytest.mark.parametrize(\"a, b, expected, is_error\", [\n    (1, 1, 1, False),\n    (42, 1, 42, False),\n    (84, 2, 42, False),\n    (42, \"b\", TypeError, True),\n    (\"a\", 42, TypeError, True),\n    (42, 0, ZeroDivisionError, True),\n])\ndef test_divide_antipattern(a, b, expected, is_error):\n    if is_error:\n        with pytest.raises(expected):\n            divide(a, b)\n    else:\n        assert divide(a, b) == expected\n</code></pre> <p>Use</p> <pre><code>@pytest.mark.parametrize(\"a, b, expected\", [\n    (1, 1, 1),\n    (42, 1, 42),\n    (84, 2, 42),\n])\ndef test_divide_ok(a, b, expected):\n    assert divide(a, b) == expected\n\n\n@pytest.mark.parametrize(\"a, b, expected\", [\n    (42, \"b\", TypeError),\n    (\"a\", 42, TypeError),\n    (42, 0, ZeroDivisionError),\n])\ndef test_divide_error(a, b, expected):\n    with pytest.raises(expected):\n        divide(a, b)\n</code></pre>"},{"location":"tutorials/packaging/testing/#prefer-tmpdir-over-static-locations","title":"Prefer tmpdir over static locations","text":"<p>Sometimes in testing you need a directory or files that you can work with to test out your code.  Don't create files in static or predefined locations on your filesystem.  You should use the tmpdir fixture and create the fiels on-the-fly to test.</p> <pre><code>    # util.py\n    def process_file(fp:IO) -&gt; List[int]:\n        \"\"\"Toy function that returns an array of line lengths.\"\"\"\n        return [len(l.strip()) for l in fp.readlines()]\n</code></pre> <p>Avoid</p> <pre><code>@pytest.mark.parametrize(\"filename, expected\", [\n    (\"first.txt\", [3, 3, 3]),\n    (\"second.txt\", [5, 5]),\n])\ndef test_antipattern(filename, expected):\n    with open(\"resources/\" + filename) as fp:\n        assert process_file(fp) == expected\n</code></pre> <p>Use</p> <pre><code>@pytest.mark.parametrize(\"contents, expected\", [\n    (\"foo\\nbar\\nbaz\", [3, 3, 3]),\n    (\"hello\\nworld\", [5, 5]),\n])\ndef test_pattern(tmpdir, contents, expected): # tmpdir is build-in to global pytest fixture\n    tmp_file = tmpdir.join(\"testfile.txt\")\n    tmp_file.write(contents)\n    with tmp_file.open() as fp:\n        assert process_file(fp) == expected\n</code></pre>"},{"location":"tutorials/packaging/testing/#coverage-and-mutation-testing","title":"Coverage and Mutation Testing","text":""},{"location":"tutorials/packaging/testing/#code-coverage-goals","title":"Code Coverage Goals","text":"<p>Aim for over 80% code coverage, using pytest-cov to generate reports. Exclude configuration code and error handlers using # pragma: no cover.</p>"},{"location":"tutorials/packaging/testing/#mutation-testing","title":"Mutation Testing","text":"<p>Mutation testing with mutmut checks test effectiveness by introducing small changes (mutations) in the code. This ensures that tests detect when code behavior changes, offering more insight than code coverage alone.</p>"},{"location":"tutorials/packaging/testing/#plugins","title":"Plugins","text":"<ul> <li>pytest-xdist: Use pytest-xdist for parallel test execution to speed up CI/CD pipelines. This can reduce overall test time, especially for larger test suites.</li> <li>pytest-randomly: run tests randomly (useful to catch weird bugs that runs sequentially).</li> <li>pytest-sugar: shows failures and errors instantly and shows a progress bar.</li> <li>pytest-icdiff: better diff when asserting error happens. (also pytest-clarity)</li> <li>pytest-html: get a html base report of the test.</li> <li>pytest-instafail: shows failures and errors instantly instead of waiting until the end of test session.</li> <li>pytest-timeout: terminate tests after a certain timeout.</li> <li>pytest-parallel: for parallel and concurrent testing.</li> <li>pytest-picked: Run the tests related to the unstaged files or the current branch (according to Git).</li> <li>pytest-benchmark: fixture for benchmarking code.</li> <li>pytest-cov: Code coverage.  (MUST HAVE!)</li> <li>pytest-lazy-fixture: allows you to get values of <code>fixture</code> from <code>paratmertized</code> method.</li> <li>pytest-freezegun: Freeze time! </li> <li>pytest-leaks: Find resource leaks.</li> <li>pytest-deadfixtures: find out fixtures that are not used or duplicated.</li> <li>pytest-responses: fixture for <code>requests</code> library mocking.</li> </ul>"},{"location":"tutorials/performance/lookup_tables/","title":"Lookup Tables","text":"<p>Summary</p> <p> (Almost) Always use dictionaries</p> <p> Use a unique index</p> <p> Avoid code heavily oriented around pandas (especially indexing)</p>"},{"location":"tutorials/performance/lookup_tables/#lookup-tables","title":"Lookup Tables","text":"<p>lookup tables or cache is what computer scientist calls a space-time trade off.  What does that mean is you are using additional memory (RAM) to get faster access to data.  </p>"},{"location":"tutorials/performance/lookup_tables/#why-dictionary-is-faster-than-list-or-tuple","title":"Why dictionary is faster than list or tuple?","text":"<p>When lookup for items, dictionaries have constant time complexity, O(1) while lists have linear time complexity, O(n).</p> <p>However, dictionaries have much bigger space complexity compare to lists eventhough both have O(n) space complexity in terms of big-O. </p>"},{"location":"tutorials/performance/lookup_tables/#i-really-want-to-use-list","title":"I really want to use list","text":"<p>before you do that, you should be aware of the cost of the methods.</p> Method List Dict <code>INSERT</code> (Head or Tail) O(1) O(1)/O(N) <code>INSERT</code> (Random) O(N) O(1)/O(N) <code>DELETE</code> (Head or Tail) O(1) O(1)/O(N) <code>DELETE</code> (Random) O(N) O(1)/O(N) <code>GET</code> O(1) O(1)/O(N) <code>Search</code> (Sorted) O(log N) O(N)/O(N) <code>Search</code> (Random) O(N) O(N)/O(N) <code>COPY</code> O(N) O(N)/O(N) <p>the amortized worst case scenario for dict is O(N). So becareful just because you have a dict, doesn't mean your get O(1) runtime behavior. </p> <p>when you do  <pre><code>    # This is `Search` O(N)\n    if x in lst: \n        print(x)\n\n    # To get O(log N) in search for lst you can do.\n    # assume lst is sorted\n    index = bisect_left(lst,x)    \n    if index != len(l) and x == lst[index]:\n        print(x)\n</code></pre></p>"},{"location":"tutorials/performance/lookup_tables/#my-key-is-not-hashable","title":"My key is not hashable","text":"<p>If you just want check for existence of object you can do one of the two things here</p> <pre><code>    list_of_names = [...] # this is a really large list\n\n    # if you only want to check for existince a few times\n    names_existence_lookup = set(list_of_names)\n    for candidate in list_of_candidates:\n        if candidate in names_existence_lookup:\n            print(f\"{candidate} on the short-list\")\n\n    # can also create a dict with fixed value with the sentient that is special in python\n    names_existence_lookup = { name:None for name in list_of_names}\n    for candidate in list_of_candidates:\n        if candidate in names_existence_lookup:\n            print(f\"{candidate} on the short-list\")\n</code></pre> <p>In python <code>list</code>, <code>set</code>, and <code>dict</code> are not hashable objects.  The reason these can't be hashed is because the content can change therefore, it is important to convert these into an immutable datatype before use for hashable objects. </p>"},{"location":"tutorials/performance/multiprocessing/","title":"Multi-Processing","text":"<p>Summary</p> <p> Partition data into independent batches/chunks.</p> <p> Use shared memory for large data.</p> <p> Avoid naive element-wise processing.</p> <p> Avoid Using multithreading for CPU-bound tasks.</p> <p> Avoid Using multiprocessing for IO-bound tasks.</p>"},{"location":"tutorials/performance/multiprocessing/#multi-processing","title":"Multi-Processing","text":"<p>Due to the infamous python GIL (Python 3.13 have flags to disable it in experiment), when you need more CPU power to crunch some data, multi-processing is the way to go.   Python has a built-in multiprocessing library that make this feature avaiable out of the box. </p>"},{"location":"tutorials/performance/multiprocessing/#multiprocessing-library","title":"Multiprocessing library","text":"<p>Let's take a look of the basic example of using multiprocessing in python.</p> <p><pre><code>import multiprocessing\ndef cpu_bound_processing(data):\n    pass\n\nprocess = multiprocessing.Process(target=cpu_bound_processing, args=(x, y, z))\nprocess.start()\nprocess.join() # any process finishes but not been joined becomes zombie process.\n</code></pre> with this example you can now utitlize more CPU cores to do CPU bound tasks.</p>"},{"location":"tutorials/performance/multiprocessing/#shared-memory","title":"Shared Memory","text":"<p>when the input size is large, it is in-efficient to make copies of the object to be passed to each sub-process.  In this case, we should create one shared memory block of the original large object. </p> <p>Python have some capabilities build-in for this.  You can use shared_memory module to do that.</p> <p>As mentioned above, when doing concurrent programming it is usually best to avoid using shared state as far as possible. This is particularly true when using multiple processes.</p> <p>However, if you really do need to use some shared data then multiprocessing provides a couple of ways of doing so.</p> <p>Below is an example program that divide the image into smaller chunks and process each chunk in separate process using shared memory, then combine the result.</p> <pre><code>import multiprocessing as mp\nimport numpy as np\nfrom PIL import Image\n\ndef process_chunk(shared_array, start, end):\n    # Access the chunk from the shared array\n    chunk = shared_array[start:end]\n\n    # Simulate image processing: invert colors\n    chunk = 255 - chunk\n\n    # Write the processed chunk back to the shared array\n    shared_array[start:end] = chunk\n\nif __name__ == '__main__':\n    # Load a large image\n    img = np.array(Image.open('large_image.jpg'))\n\n    # Create a shared memory block for the image\n    shared_img = mp.Array('B', img.flatten())\n\n    # Create multiple processes\n    num_processes = 4\n    chunk_size = len(img.flatten()) // num_processes\n    processes = []\n    for i in range(num_processes):\n        start = i * chunk_size\n        end = (i + 1) * chunk_size\n        p = mp.Process(target=process_chunk, args=(shared_img, start, end))\n        processes.append(p)\n        p.start()\n\n    # Wait for all processes to finish\n    for p in processes:\n        p.join()\n\n    # Convert the shared array back to an image\n    result_img = np.frombuffer(shared_img.get_obj()).reshape(img.shape)\n    Image.fromarray(result_img).save('processed_image.jpg')\n</code></pre>"},{"location":"tutorials/performance/multiprocessing/#numpy-multiprocessing","title":"Numpy multiprocessing","text":"<p>Often we uses NumPy arrays to represent image or video data.  This is a great candidate for multiprocessing.  Everything can be done with the build-in shared_memory module above, but the SharedArray library it is a wrapper around <code>shm_open</code>, <code>mmap</code> and friends function in C with python binding to makes this more user friendly when working with numpy. </p> <p>SharedArray has a few key functions:</p> <ul> <li><code>SharedArray.create(name, shape, dtype=float)</code> creates a shared memory array</li> <li><code>SharedArray.attach(name)</code> attaches a previously created shared memory array to a variable</li> <li><code>SharedArray.delete(name)</code> deletes a shared memory array, however, existing attachments remain valid</li> </ul>"},{"location":"tutorials/performance/multiprocessing/#example","title":"Example","text":"<p>With SharedArray. </p> <pre><code>import SharedArray\n\ndef multiprocess_load_images(num_workers = 12):\n    files = os.listdir(\"train_images\")\n\n    # this assumes each file with 1400x1200 resolution with 3 (RBG) channels\n    number_of_files = len(files)\n    data = SharedArray.create('data', (number_of_files, 1400, 2100, 3))\n\n    worker_amount = int(number_of_files/num_workers)\n    residual = number_of_files - (num_works * worker_amount)\n\n    def load_images(i, n):\n        # Chucking the potential inputs. this could be optmized\n        to_load = files[i: i+n]\n        for j, file in enumerate(to_load):\n            data[i + j] = cv2.imread(\"train_images/\" + file)\n\n    processes = []\n    for worker_num in range(num_workers):\n        run_worker_amount = worker_amount\n        if worker_num == num_workers - 1 and residual != 0:\n            run_worker_amount += residual\n        process = multiprocessing.Process(target=load_images, args=(worker_amount*worker_num, run_worker_amount))\n        processes.append(process)\n        process.start()\n\n    for process in processes:\n        process.join()\n\n    return data\n</code></pre> <p>An example with SharedMemory directly</p> <pre><code>from multiprocessing.shared_memory import SharedMemory\nfrom multiprocessing.managers import SharedMemoryManager\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nfrom multiprocessing import current_process, cpu_count, Process\nimport numpy as np\n\ndef work_with_shared_memory(shm_name, shape, dtype):\n    print(f'With SharedMemory: {current_process()=}')\n    # Locate the shared memory by its name\n    shm = SharedMemory(shm_name)\n    # Create the np.recarray from the buffer of the shared memory\n    np_array = np.recarray(shape=shape, dtype=dtype, buf=shm.buf)\n    return np.nansum(np_array.val)\n\n# Some way to create that numpy array.\nnp_array = ...\nshape, dtype = np_array.shape, np_array.dtype\nwith SharedMemoryManager() as smm:\n        # Create a shared memory of size np_arry.nbytes\n        shm = smm.SharedMemory(np_array.nbytes)\n        # Create a np.recarray using the buffer of shm\n        shm_np_array = np.recarray(shape=shape, dtype=dtype, buf=shm.buf)\n        # Copy the data into the shared memory\n        np.copyto(shm_np_array, np_array)\n        # Spawn some processes to do some work\n        with ProcessPoolExecutor(cpu_count()) as exe:\n            fs = [exe.submit(work_with_shared_memory, shm.name, shape, dtype)\n                  for _ in range(cpu_count())]\n            for _ in as_completed(fs):\n                pass\n</code></pre>"},{"location":"tutorials/performance/multiprocessing/#huge-arrays-in-numpy","title":"Huge arrays in numpy","text":"<p>Python is notoriously known as a memory hogger and when you need to work with large amount of data it could result in out of memory error.  One trick we can use in this case is memory mapped file in numpy. </p> <pre><code>from concurrent.futures import ProcessPoolExecutor, as_completed\nfrom multiprocessing import Process\n\nimport numpy as np\n\nworker, nrows, ncols = 10, 1_000_000, 100\n\ndef split_size_iter(total_size:int , num_chunks: int) -&gt; Iterator[Tuple[int, int]]:\n    ...\n\ndef print_matrix(filename, worker_index_start, worker_index_end):\n    matrix = np.memmap(filename, dtype=np.float32, mode='r+', shape=(worker, nrows, ncols))\n    print matrix[worker_index_start: worker_index_end]\n\n\ndef main():\n    matrix = np.memmap('test.dat', dtype=np.float32, mode='w+', shape=(worker, nrows, ncols))\n    # some code to fill this matrix\n\n    with ProcessPoolExecutor(cworker) as exe:\n        fs = [exe.submit(print_matrix, 'test.dat', start, end) \n                for start,end in split_size_iter(worker, 4)]\n        for _ in as_completed(fs):\n                    pass\n</code></pre>"},{"location":"tutorials/performance/multiprocessing/#ray","title":"Ray","text":"<p>Ray is a powerful open source platform that makes it easy to write distributed Python programs and seamlessly scale them from your laptop to a cluster.  Ray comes with support for the <code>mulitprocessing.Pool</code> API out of the box when importing <code>ray.util.multiprocessing</code>. </p>"},{"location":"tutorials/performance/multiprocessing/#example_1","title":"Example","text":"<pre><code>import math\nimport random\nimport time\n\ndef sample(num_samples):\n    num_inside = 0\n    for _ in range(num_samples):\n        x, y = random.uniform(-1, 1), random.uniform(-1, 1)\n        if math.hypot(x, y) &lt;= 1:\n            num_inside += 1\n    return num_inside\n\ndef approximate_pi_distributed(num_samples):\n    from ray.util.multiprocessing.pool import Pool # NOTE: Only the import statement is changed.\n    pool = Pool()\n\n    start = time.time()\n    num_inside = 0\n    sample_batch_size = 100000\n    for result in pool.map(sample, [sample_batch_size for _ in range(num_samples//sample_batch_size)]):\n        num_inside += result\n\n    print(\"pi ~= {}\".format((4*num_inside)/num_samples))\n    print(\"Finished in: {:.2f}s\".format(time.time()-start))\n</code></pre>"},{"location":"tutorials/performance/multiprocessing/#dask","title":"Dask","text":"<p>Dask is a Python parallel computing library geared towards scaling analytics and scientific computing workloads.</p> <p>Dask is composed of two parts: Dynamic task scheduling for optimized computation and Big Data collections such as like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments, which run on top of dynamic task schedulers.</p>"},{"location":"tutorials/performance/multiprocessing/#example_2","title":"Example","text":"<pre><code># Setup the cluster and client for Dask\nfrom dask.distributed import Client, LocalCluster\n\nn_workers = 3\ncluster = LocalCluster(n_workers=n_workers, threads_per_worker=1, memory_limit='4GB')\nclient = Client(cluster)\nclient.wait_for_workers(n_workers)\n\n# Load dataset\nfrom dask_ml.datasets import make_classification as make_classification_dask\n\nX_dask, y_dask = make_classification_dask(n_samples=10000, n_features=10, n_informative=5, n_redundant=5, random_state=1, chunks=10)\n\n# Build a model\nimport dask_lightgbm.core as dlgbm\n\ndmodel = dlgbm.LGBMClassifier(n_estimators=400)\ndmodel.fit(X_dask, y_dask)\n\n# make a single prediction\nrow = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]]\nyhat = dmodel.predict(da.from_array(np.array(row)))\nprint('Prediction: %d' % yhat[0])\n\ny_pred = dmodel.predict(X_dask, client=client)\n\nacc_score = (y_dask == y_pred).sum() / len(y_dask)\nacc_score = acc_score.compute()\nprint(acc_score)\n</code></pre>"},{"location":"tutorials/performance/profiling_tools/","title":"Profiling Tools & Strategy","text":"<p>Summary</p> <p> Use the standard python cProfile profiler</p> <p> Use PyCharm (Professional Edition) <code>.pstat</code> profile viewer</p> <p> Ensure entire program can be easily tested for performance</p>"},{"location":"tutorials/performance/profiling_tools/#profiling-tools-strategy","title":"Profiling Tools &amp; Strategy","text":"<p>Profiling code is a hugely important part of data science code since machine learning and AI algorithms are typically compute heavy processes. Performance tuning is relevant during both training and prediction time.</p> <p>During training time, performant code allows developers to have shorter  development/training cycles. The difference between a function call that takes 10 seconds vs 1 second has a huge impact on the development experience. An  example that many people have likely run into is when loading huge word vector  models into memory. A load of GoogleNews W2V using Gensim can take multiple  minutes before even being able to do a computation.</p> <p>During prediction time, performance typically dictates how much a model will cost to host. Performance is often ignored in favor of scaling out horizontally and just paying the cost of hosting. Besides saving on hosting costs, the  biggest motivating factor for performance tuning code would be the ability to have a better/faster development experience (just like during training).</p>"},{"location":"tutorials/performance/profiling_tools/#tools","title":"Tools","text":"<p>There are a variety of profiling tools for python which fall under two  different categories:</p> <ul> <li> <p>Scope-Based Profilers: These profilers generally use the built-in      sys.setprofile or sys.settrace python library calls to track the      duration of each stack frame. Every time a function enters a scope, a start     time is recorded, and every time the function exits, the end time is      recorded. This allows the profiler to record a hierarchical view of where      time is spent. These types of profilers may also introduce a lot of      overhead since every single scope entrance/exit is tracked. For programs     which have many function calls (such as mapping a function over a giant      collection) the overhead of a scope-based profiler may give inflated     measurements. Another limitation is that many python functions do not      generate a stack frame (for example built-in functions). This means that     scope-based trackers may not give a granular enough idea of where     time is being spent. Scope-based profilers cannot give line-level profiling     information.</p> </li> <li> <p>Sampling Profilers: These profilers generally use the built-in      sys._getframe python library call to get the current state of the      application. In contrast to a scoped-based profiler, sampling profilers only     periodically query the running python application to determine where time is     being spent. Rather than tracking the duration of a function scope, a      sampling profiler tracks the number of times it sees a specific stack frame.     This means that a sampling profiler will not track a very accurate measure     of the overall method time. A huge benefit of a sampling profiler is that     it has the ability to give line-level profile information. It also has      almost none of the overhead issues that a scope-based profiler has. Some     sampling profilers are even designed to be run on long-running applications      for production monitoring.</p> </li> </ul> <p>The most widely used tools are the following:</p> <ul> <li>cProfile (Scope-Based): The python standard library profiler.</li> <li>pprofile (Scope-Based &amp; Sampling): A pure python profiler with optional      sampling.</li> <li>yappi (Scope-Based &amp; Sampling): A profiler (written in C) with multi-threading support.</li> <li>scalene (Sampling): A profiler (written in C++) with built-in memory usage profiling.</li> </ul> <p>In general, it is recommended to start with cProfile and move on to other  tools only if there is a need. Almost all use-cases are well-suited to the most  basic scoped-based profiler and there are only infrequent times when more  specialized profilers should be used. For example yappi may be well suited to  testing a multi-threaded web server with extremely high traffic. Though  sampling profilers can give extremely granular information, it is unlikely  that the issues will not show up clearly in a scope-based profile.</p>"},{"location":"tutorials/performance/profiling_tools/#strategy","title":"Strategy","text":"<p>Warning</p> <p>To follow along with the tutorial you must have PyCharm Professional  Edition. This tutorial makes extensive use of the <code>.pstat</code> profile viewer which is not available in the Community Edition</p> <p>In order to describe the process of profiling code, the following section will  use cProfile and PyCharm to go through the steps of optimizing an unoptimized function. The example function will compute a categorical embedding. The  function implements mapping an occupation code to an embedding vector. In the  example the occupation code will be a number within the range  <code>(0, n_occupations)</code> and the embedding will have the shape  <code>(n_occupations, n_components)</code>.</p> <pre><code>import pandas as pd\nimport numpy as np\n\n\n# ------------------------------------------------------------------------------\n# Example Datasets\n# ------------------------------------------------------------------------------\n\nn_occupations = 100000  # The number of possible occupation codes\nn_components = 100      # The number of components in the occupation embedding\nn_samples = 10000       # The number of users\n\n\ndef user_dataframe():\n    example_values = np.random.randint(0, n_occupations * 2, n_samples)\n    return pd.DataFrame({\n        \"occupation_code\": example_values,\n        \"age\": example_values,\n        \"zip_code\": example_values,\n    })\n\n\ndef occupation_code_embedding():\n    index = np.arange(n_occupations)\n    np.random.shuffle(index)\n    values = np.random.random((n_occupations, n_components))\n    return pd.DataFrame(values, index=index)\n\n\n# ------------------------------------------------------------------------------\n# Example Function Implementation\n# ------------------------------------------------------------------------------\n\ndef occupation_embedding_lookup(user_df, embedding_df):\n    result = pd.DataFrame(columns=embedding_df.columns)\n    for index, row in user_df.iterrows():\n        value = row[\"occupation_code\"]\n\n        # Check if the current occupation code is in the embedding index\n        if embedding_df.index.isin([value]).any():\n            # Fetch the embedding for the given occupation code\n            vector = embedding_df.loc[value]\n        else:\n            # Select 0th (default) vector if occupation code is not found\n            vector = embedding_df.iloc[0]\n        result.loc[index] = vector\n\n    # Concatenate all occupation embeddings for all users\n    return result\n\n\n# ------------------------------------------------------------------------------\n# Main\n# ------------------------------------------------------------------------------\n\ndef main():\n    # Build an example user data and embedding\n    user_df = user_dataframe()\n    embedding_df = occupation_code_embedding()\n\n    # Compute the embedding from the user feature\n    result = occupation_embedding_lookup(user_df, embedding_df)\n\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>In the above example, the <code>occupation_embedding_lookup</code> will be the focus of the profile. The <code>user_dataframe</code> and <code>occupation_code_embedding</code> functions will show up in future profiles but should be ignored since they are only used  to generate example datasets.</p> <p>Question</p> <p>To demonstrate the importance of profiling, briefly look at the  <code>occupation_embedding_lookup</code> function and come up with an guess for which lines will have the worst performance problems.</p> <p>What do you think will be the most time-consuming part of the function? </p> <p>How long do you think it will take the function to run?</p> <p>In pycharm, open the project, and file and then right click the within the  editor panel. From the dropdown menu, select the <code>Profile '&lt;your_file&gt;'</code> option. This will invoke cProfile on the file.</p> <p></p> <p>When the profile is complete, the PyCharm <code>.pstat</code> viewer will open the  resulting profile. The results of any profiler that is capable of generating a  <code>.pstat</code> file can be viewed in PyCharm (even if the profile was not generated within PyCharm).</p> <p></p> <p>On this <code>.pstat</code> viewer panel, the <code>Statistics</code> tab is selected by default. Each row corresponds to a stack frame and its corresponding run time. This  view can be useful, but it is hard to see exactly where bottleneck originated from. Notice that the most time consuming call is <code>numpy.concatenate</code> which is  never directly called in the code example above. This usually indicates that this call is embedded in a library call.</p> <p>The get a more informative view of the problem areas in the call-stack, select  the <code>Call Graph</code> tab (outlined in red above).</p> <p></p> <p>This view presents a much more direct representation of what takes the most  time. When the call graph is first presented, it may be too small to read the individual nodes. The zoom can be used by pressing the buttons on the top left  of the panel (highlighted in red above).</p> <p>Each node in the PyCharm <code>.pstat</code> viewer represents a different stack frame. PyCharm color codes these stack frames from most time-consuming (red) to least time-consuming (green). </p> <p>Each stack frame tracks 3 things:</p> <ul> <li>Total time spent within the frame and all sub-frames (<code>Total</code>)</li> <li>Total time spent within the frame but not within a sub-frame (<code>Own</code>)</li> <li>Total number of times the frame was entered (<code>x&lt;number&gt;</code> in the top right)</li> </ul> <p>This is the information that should be used to guide where to optimize the  code. The first areas of focus should be the red boxes. The bottom 3 stack  frames are composed of:</p> <ul> <li><code>main.py</code> - The overall execution of the file. The <code>Total</code> will always equal     the execution time of the program. In the example profile, the program took     65.8 seconds to execute.</li> <li><code>main</code> - This is the <code>def main()</code> function defined to drive the program.</li> <li><code>occupation_embedding_lookup</code> - This is the function that performs the      embedding. In this view it becomes clear that the embedding computation      takes up nearly all of the execution time (99%). The remaining execution      time is spent in the initialization of data.</li> </ul> <p>None of this is very helpful since it is known that very little computation is occuring outside of the embedding function. The next most time consuming  portion of the code is the <code>__setitem__</code> call. Just from looking at the name, it is not clear where this is occuring. PyCharm can directly show the source code of the problem by right clicking on the node and selecting the \"Navigate To Source\" option (highlighted in red above).</p> <p>Upon inspecting the source code it becomes clear that this call is the  <code>DataFrame.__setitem__</code> method. Further inspection of the profile call graph chart reveals that this method calls into <code>DataFrame.append</code> which then calls into <code>numpy.concatenate</code> (The most time consuming individual function).</p> <p>There is only one place in the code where a DataFrame set item occurs in  the <code>occupation_embedding_lookup</code> function:</p> <pre><code>def occupation_embedding_lookup(user_df, embedding_df):\n    result = pd.DataFrame(columns=embedding_df.columns)\n    for index, row in user_df.iterrows():\n        value = row[\"occupation_code\"]\n        if embedding_df.index.isin([value]).any():\n            vector = embedding_df.loc[value]\n        else:\n            vector = embedding_df.iloc[0]\n        result.loc[index] = vector\n    return result\n</code></pre> <p>From the profile it becomes clear that every call to  <code>result.loc[index] = vector</code> causes an array concatenation to be performed. This means that a completely new array is allocated every time this line is  executed. </p> <p>To alleviate the issue, modify the function to collect the results in a structure designed for concatenation. A python list is a good candidate for  this since it is tuned to handle appending.</p> <pre><code>def occupation_embedding_lookup(user_df, embedding_df):\n    result = list()\n    for index, row in user_df.iterrows():\n        value = row[\"occupation_code\"]\n        if embedding_df.index.isin([value]).any():\n            vector = embedding_df.loc[value]\n        else:\n            vector = embedding_df.iloc[0]\n        result.append(vector)\n    return pd.DataFrame(result, index=user_df.index, columns=embedding_df.columns)\n</code></pre> <p>Run the profile on the updated function to view the results.</p> <p></p> <p>The first thing to note is that the overall runtime has been drastically  reduced. The original profile of 65.8 seconds has been reduced to 11.4 seconds.</p> <p>The new profile now shows that the next most time consuming part of the  function is the check to see if the value is in the index.</p> <pre><code>def occupation_embedding_lookup(user_df, embedding_df):\n    result = list()\n    for index, row in user_df.iterrows():\n        value = row[\"occupation_code\"]\n        if embedding_df.index.isin([value]).any():\n            vector = embedding_df.loc[value]\n        else:\n            vector = embedding_df.iloc[0]\n        result.append(vector)\n    return pd.DataFrame(result, index=user_df.index, columns=embedding_df.columns)\n</code></pre> <p>The line <code>embedding_df.index.isin([value]).any()</code> ends up checking to see if each of the index values match the current occupation code. Rather than an  <code>O(1)</code> hash lookup, this performs an <code>0(n)</code> search on the embedding index for  every  single row in the <code>user_df</code>. This can be alleviated by simplifying the  logic and using the <code>in</code> operator on the <code>pd.Index</code> object.</p> <pre><code>def occupation_embedding_lookup(user_df, embedding_df):\n    result = list()\n    for index, row in user_df.iterrows():\n        value = row[\"occupation_code\"]\n        if value in embedding_df.index:\n            vector = embedding_df.loc[value]\n        else:\n            vector = embedding_df.iloc[0]\n        result.append(vector)\n    return pd.DataFrame(result, index=user_df.index, columns=embedding_df.columns)\n</code></pre> <p>Run the profile on the updated function to view the results.</p> <p></p> <p>The final profile now shows that the overall execution time is 4.1 seconds. This is a 16x speedup from the original profile!</p> <p>Even though there may still be performance issues with this function, the code  is already much more optimized than it was with just a few minor tweaks. </p> <p>Challenge</p> <p>Continue to profile the code and see if you can get a 1000x speedup!</p>"}]}