{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Best Python Practices \u00b6 A collection of python tutorials and patterns to use in data science projects. This guide is meant to address common pitfalls and difficult areas of python which are important in data science code and not easily solved with a simple stack overflow search. The tutorials explore topics such as organization and performance optimization. The Zen of Python is one of the guiding principles for this documentation: import this The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let's do more of those! One important aspect of improving the developing experience is to keep your Python version and dependencies up-to-date. The best place to start is by looking at the Tutorials","title":"Home"},{"location":"#best-python-practices","text":"A collection of python tutorials and patterns to use in data science projects. This guide is meant to address common pitfalls and difficult areas of python which are important in data science code and not easily solved with a simple stack overflow search. The tutorials explore topics such as organization and performance optimization. The Zen of Python is one of the guiding principles for this documentation: import this The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let's do more of those! One important aspect of improving the developing experience is to keep your Python version and dependencies up-to-date. The best place to start is by looking at the Tutorials","title":"Best Python Practices"},{"location":"design_patterns/","text":"Design Patterns \u00b6 What are Design Patterns? \u00b6 In software engineering, a design pattern is a general repeatable solution to a commonly occurring problem in software design. Design patterns can speed up the development process by providing tested, proven development paradigms. Design patterns help to prevent subtle issues that can cause major problems and improve code readability. Design patterns are usually classified into three major categories: - Creational Patterns - Structural Patterns - Behavioral Patterns Each pattern type helps to solve a certain class of problems. We are not going to cover all the patterns in this document, but we will cover the ones we frequently use. Creational Patterns \u00b6 Singleton \u00b6 Intent \u00b6 Ensure a class has only one instance, and provide a global point of access to it. Problem \u00b6 Applications sometimes need one and only one instance of an object during the lifetime of the application instance. Solution \u00b6 It is recommended to name all your singletons in a singleton.py file within your project. Optionally, you can also name the file whatever you prefer, but expose the singleton object via __all__ of the module's __init__.py . Since we don't want to expose the ability to create a new instance of the object to external modules and Python modules are imported once and globally available after import, it is important to make sure the name of the class starts with __ (dunder) and use __all__ to limit the symbols that need to be exported. # in singleton.py # Only expose the singleton objects you want to be exposed for this module __all__ = [ \"s3client\" ] class __S3Client : def __init__ ( self , ... ): # Implementation of your object pass s3client = __S3Client () Structural Patterns \u00b6 Adapter \u00b6 Intent \u00b6 Wrap an existing class with a new interface. Wrap an old component to a new system. Let classes work together that couldn't otherwise because of incompatible interfaces. Problem \u00b6 Take OCR as an example. Each provider calls the actual method that does OCR slightly differently. For example, EasyOCR uses a method called readtext , Google Vision SDK uses document_text_detection , and PyTesseract uses ocr_client to obtain the OCR result. Solution \u00b6 # in interface.py from abc import ABC , abstractmethod # Our expected interface class OCRClient ( ABC ): \"\"\" Interface for OCR Client \"\"\" @abstractmethod def ocr ( self , image ): pass # in adapter.py # Adapter Class class GVisionOCRClientAdapter ( OCRClient ): def __init__ ( self ): self . _client = GVisionOCRClientAdaptee () def ocr ( self , image ): return self . _client . adaptee_ocr ( image = image ) # Adaptee Class class GVisionOCRClientAdaptee : def __init__ ( self ): self . private_client = vision . from_service_account_json ( 'some_location.json' ) def adaptee_ocr ( self , image ): return self . private_client . document_text_detection ( image ) Behavioral Patterns \u00b6 Registry \u00b6 Intent \u00b6 Keep track of all subclasses of a given class. Create objects of subclasses that have different behaviors. Problem \u00b6 The need for some kind of manager class to manage all related subclasses. You want the manager class to iterate over all available subclasses and call a particular method on each/some subclasses. You want to streamline a factory class, which takes an input and creates/returns an object of that type. Solution \u00b6 from typing import Type class Document ( ABC ): _REGISTRY = {} def __new__ ( cls , * args , ** kwargs ): name = cls . __name__ prefix = \"Document\" assert name . startswith ( prefix ) form_type = name [ len ( prefix ):] . lower () cls . _REGISTRY [ form_type ] = cls @classmethod def factory ( cls , form_type : str ) -> Type [ Document ]: return cls . _REGISTRY [ form_type ]() @abstractmethod def extract ( self , * args , ** kwargs ): pass class Document1040 ( Document ): def extract ( self , file : ByteIO , fields : Iterable [ str ] = None ) -> Document1040Results : # some extract logic pass class Document1099K ( Document ): def extract ( self , file : ByteIO , fields : Iterable [ str ] = None ) -> Document1099KResult : # different logic to extract this form pass # To use this doc_1040 = Document . factory ( \"1040\" ) result_1040 = doc_1040 . extract ([ \"ssn\" , \"agi\" ]) doc_1099k = Document . factory ( \"1099k\" ) result_1099k = doc_1099k . extract () # A more powerful use case would be in a loop, you will not see large if/else blocks to # process files based on the file type. for doc_content , doc_type in all_documents : extractor = Document . factory ( doc_type ) result = extractor . extract () Chain of Responsibility \u00b6 Intent \u00b6 When an object needs to be processed by a potential chain of successive handlers. Each handler may process it, pass it over, or break the chain and stop the task from propagating to the next handler. Problem \u00b6 The need for decoupling the sender of a request and its receiver. It is unknown until runtime what kind of request the object is, therefore, it needs a different processing handler to handle the request. Sometimes, more than one handler needs to pass over the data. Solution \u00b6 class NamedEntityHandler ( ABC ): def __init__ ( self , successor : Optional [ \"NamedEntityHandler\" ] = None ): self . _successor = successor @abstractmethod def handle ( self , text : str ) -> Entity : pass class PersonNameHandler ( NamedEntityHandler ): def handle ( self , text : str ) -> Entity : name = PersonNameEntityParser . parse ( text ) if self . _successor is not None : name = self . _successor . handle ( name ) return name class NameConfidenceCalibrationHandler ( NamedEntityHandler ): def handle ( self , name_entity : Entity ) -> Entity : return NameConfidenceCalibrator . calibrate ( name_entity ) class CompanyNameHandler ( NamedEntityHandler ): def handle ( self , text : str ) -> Entity : return CompanyNameEntityParser . parse ( text ) class DateHandler ( NamedEntityHandler ): def handle ( self , text : str ) -> Entity : return DateParser . parse ( text ) class AmountHandler ( NamedEntityHandler ): def handle ( self , text : str ) -> Entity : return AmountParser . parse ( text ) class NamedEntityExtractor : def __init__ ( self ): self . _person_name_handler = PersonNameHandler ( NameConfidenceCalibrationHandler ()) self . _company_name_handler = CompanyNameHandler () self . _address_handler = AddressHandler () self . _date_handler = DateHandler () self . _amount_handler = AmountHandler () self . _doc_handler = { DocTypeEnum . F_1040 : [ self . _person_name_handler , self . _address_handler , self . _date_handler , self . _amount_handler ], DocTypeEnum . F_W2 : [ self . _person_name_handler , self . _address_handler , self . _amount_handler ], DocTypeEnum . F_BILL : [ self . _company_name_handler , self . _date_handler , self . _amount_handler ] } def parse_entity ( self , text : str , doc_type : DocTypeEnum ) -> List [ Entity ]: parse_chain = self . _doc_handler . get ( doc_type ) entity_list = [ parser . handle ( text ) for parser in parse_chain ] return list ( filter ( lambda x : x is not None , entity_list )) Strategy \u00b6 Intent \u00b6 Define a family of interchangeable algorithms by a client (client decides which algorithm to use either dynamically or statically). Problem \u00b6 There are multiple ways of solving the same problem, each of which has its own advantages/disadvantages. To be able to quickly change the behavior of the program.","title":"Design Patterns"},{"location":"design_patterns/#design-patterns","text":"","title":"Design Patterns"},{"location":"design_patterns/#what-are-design-patterns","text":"In software engineering, a design pattern is a general repeatable solution to a commonly occurring problem in software design. Design patterns can speed up the development process by providing tested, proven development paradigms. Design patterns help to prevent subtle issues that can cause major problems and improve code readability. Design patterns are usually classified into three major categories: - Creational Patterns - Structural Patterns - Behavioral Patterns Each pattern type helps to solve a certain class of problems. We are not going to cover all the patterns in this document, but we will cover the ones we frequently use.","title":"What are Design Patterns?"},{"location":"design_patterns/#creational-patterns","text":"","title":"Creational Patterns"},{"location":"design_patterns/#singleton","text":"","title":"Singleton"},{"location":"design_patterns/#intent","text":"Ensure a class has only one instance, and provide a global point of access to it.","title":"Intent"},{"location":"design_patterns/#problem","text":"Applications sometimes need one and only one instance of an object during the lifetime of the application instance.","title":"Problem"},{"location":"design_patterns/#solution","text":"It is recommended to name all your singletons in a singleton.py file within your project. Optionally, you can also name the file whatever you prefer, but expose the singleton object via __all__ of the module's __init__.py . Since we don't want to expose the ability to create a new instance of the object to external modules and Python modules are imported once and globally available after import, it is important to make sure the name of the class starts with __ (dunder) and use __all__ to limit the symbols that need to be exported. # in singleton.py # Only expose the singleton objects you want to be exposed for this module __all__ = [ \"s3client\" ] class __S3Client : def __init__ ( self , ... ): # Implementation of your object pass s3client = __S3Client ()","title":"Solution"},{"location":"design_patterns/#structural-patterns","text":"","title":"Structural Patterns"},{"location":"design_patterns/#adapter","text":"","title":"Adapter"},{"location":"design_patterns/#intent_1","text":"Wrap an existing class with a new interface. Wrap an old component to a new system. Let classes work together that couldn't otherwise because of incompatible interfaces.","title":"Intent"},{"location":"design_patterns/#problem_1","text":"Take OCR as an example. Each provider calls the actual method that does OCR slightly differently. For example, EasyOCR uses a method called readtext , Google Vision SDK uses document_text_detection , and PyTesseract uses ocr_client to obtain the OCR result.","title":"Problem"},{"location":"design_patterns/#solution_1","text":"# in interface.py from abc import ABC , abstractmethod # Our expected interface class OCRClient ( ABC ): \"\"\" Interface for OCR Client \"\"\" @abstractmethod def ocr ( self , image ): pass # in adapter.py # Adapter Class class GVisionOCRClientAdapter ( OCRClient ): def __init__ ( self ): self . _client = GVisionOCRClientAdaptee () def ocr ( self , image ): return self . _client . adaptee_ocr ( image = image ) # Adaptee Class class GVisionOCRClientAdaptee : def __init__ ( self ): self . private_client = vision . from_service_account_json ( 'some_location.json' ) def adaptee_ocr ( self , image ): return self . private_client . document_text_detection ( image )","title":"Solution"},{"location":"design_patterns/#behavioral-patterns","text":"","title":"Behavioral Patterns"},{"location":"design_patterns/#registry","text":"","title":"Registry"},{"location":"design_patterns/#intent_2","text":"Keep track of all subclasses of a given class. Create objects of subclasses that have different behaviors.","title":"Intent"},{"location":"design_patterns/#problem_2","text":"The need for some kind of manager class to manage all related subclasses. You want the manager class to iterate over all available subclasses and call a particular method on each/some subclasses. You want to streamline a factory class, which takes an input and creates/returns an object of that type.","title":"Problem"},{"location":"design_patterns/#solution_2","text":"from typing import Type class Document ( ABC ): _REGISTRY = {} def __new__ ( cls , * args , ** kwargs ): name = cls . __name__ prefix = \"Document\" assert name . startswith ( prefix ) form_type = name [ len ( prefix ):] . lower () cls . _REGISTRY [ form_type ] = cls @classmethod def factory ( cls , form_type : str ) -> Type [ Document ]: return cls . _REGISTRY [ form_type ]() @abstractmethod def extract ( self , * args , ** kwargs ): pass class Document1040 ( Document ): def extract ( self , file : ByteIO , fields : Iterable [ str ] = None ) -> Document1040Results : # some extract logic pass class Document1099K ( Document ): def extract ( self , file : ByteIO , fields : Iterable [ str ] = None ) -> Document1099KResult : # different logic to extract this form pass # To use this doc_1040 = Document . factory ( \"1040\" ) result_1040 = doc_1040 . extract ([ \"ssn\" , \"agi\" ]) doc_1099k = Document . factory ( \"1099k\" ) result_1099k = doc_1099k . extract () # A more powerful use case would be in a loop, you will not see large if/else blocks to # process files based on the file type. for doc_content , doc_type in all_documents : extractor = Document . factory ( doc_type ) result = extractor . extract ()","title":"Solution"},{"location":"design_patterns/#chain-of-responsibility","text":"","title":"Chain of Responsibility"},{"location":"design_patterns/#intent_3","text":"When an object needs to be processed by a potential chain of successive handlers. Each handler may process it, pass it over, or break the chain and stop the task from propagating to the next handler.","title":"Intent"},{"location":"design_patterns/#problem_3","text":"The need for decoupling the sender of a request and its receiver. It is unknown until runtime what kind of request the object is, therefore, it needs a different processing handler to handle the request. Sometimes, more than one handler needs to pass over the data.","title":"Problem"},{"location":"design_patterns/#solution_3","text":"class NamedEntityHandler ( ABC ): def __init__ ( self , successor : Optional [ \"NamedEntityHandler\" ] = None ): self . _successor = successor @abstractmethod def handle ( self , text : str ) -> Entity : pass class PersonNameHandler ( NamedEntityHandler ): def handle ( self , text : str ) -> Entity : name = PersonNameEntityParser . parse ( text ) if self . _successor is not None : name = self . _successor . handle ( name ) return name class NameConfidenceCalibrationHandler ( NamedEntityHandler ): def handle ( self , name_entity : Entity ) -> Entity : return NameConfidenceCalibrator . calibrate ( name_entity ) class CompanyNameHandler ( NamedEntityHandler ): def handle ( self , text : str ) -> Entity : return CompanyNameEntityParser . parse ( text ) class DateHandler ( NamedEntityHandler ): def handle ( self , text : str ) -> Entity : return DateParser . parse ( text ) class AmountHandler ( NamedEntityHandler ): def handle ( self , text : str ) -> Entity : return AmountParser . parse ( text ) class NamedEntityExtractor : def __init__ ( self ): self . _person_name_handler = PersonNameHandler ( NameConfidenceCalibrationHandler ()) self . _company_name_handler = CompanyNameHandler () self . _address_handler = AddressHandler () self . _date_handler = DateHandler () self . _amount_handler = AmountHandler () self . _doc_handler = { DocTypeEnum . F_1040 : [ self . _person_name_handler , self . _address_handler , self . _date_handler , self . _amount_handler ], DocTypeEnum . F_W2 : [ self . _person_name_handler , self . _address_handler , self . _amount_handler ], DocTypeEnum . F_BILL : [ self . _company_name_handler , self . _date_handler , self . _amount_handler ] } def parse_entity ( self , text : str , doc_type : DocTypeEnum ) -> List [ Entity ]: parse_chain = self . _doc_handler . get ( doc_type ) entity_list = [ parser . handle ( text ) for parser in parse_chain ] return list ( filter ( lambda x : x is not None , entity_list ))","title":"Solution"},{"location":"design_patterns/#strategy","text":"","title":"Strategy"},{"location":"design_patterns/#intent_4","text":"Define a family of interchangeable algorithms by a client (client decides which algorithm to use either dynamically or statically).","title":"Intent"},{"location":"design_patterns/#problem_4","text":"There are multiple ways of solving the same problem, each of which has its own advantages/disadvantages. To be able to quickly change the behavior of the program.","title":"Problem"},{"location":"libraries/","text":"Libraries & Tools \u00b6 The following is a curated list of python libraries that are recommended for general use in python projects Open Source \u00b6 Code \u00b6 black : Simple configuration-free code formatter. poetry : A package manager and dependency resolver. uv : An extremely fast Python package and project manager, written in Rust. cookiecutter : A tool to create projects like cookiecutters. (Project templates) sphinx : Python documentation generator. loguru : Simply logging. boltons : Pure python utilities that should be built-ins but bolt-ons instead. Libraries \u00b6 hydra : Framework for elegantly configuring complex applications. pydantic : Data parsing, validation and settings management using python type annotations. typer : Build CLI appliactions using Python types. graphene : Building GraphQL schemas/types. httpx : A fully featured HTTP client for Python 3, which provides sync and async APIs, and support for both HTTP/1.1 and HTTP/2. pyvips : A fast image processing library with low memory needs based on vips. PyMuPDF : A high-performance Python library for data extraction, analysis, conversion & manipulation of PDF. Functional \u00b6 toolz : A set of utility functions for iterators, functions, and dictionaries. coconut : Simple, elegant, Pythonic functional programming. Profiling & Debugging \u00b6 py-spy : Sampling profiler, let you visualize your program without restarting the program. scalene : a high-performance, high-precision CPU, GPU, and memory profiler for Python with AI-powered optimization proposals. nvprof : Profiling for CUDA. nvidia-smi : Management and monitoring of NVIDIA GPU devices. strace : System call tracer for Linux . iotop : Top for IO. htop : Interactive system monitoring process viewer. nvtop : Task monitor for NVIDIA GPU. ML \u00b6 LazyPredict : help build a lot of basic models without much code and helps understand which models works better without any parameter tuning. Others \u00b6 diagrams : A tool to help draw architectural diagrams. lux : Automatically visualize your pandas dataframe via a single print!","title":"Libraries & Tools"},{"location":"libraries/#libraries-tools","text":"The following is a curated list of python libraries that are recommended for general use in python projects","title":"Libraries &amp; Tools"},{"location":"libraries/#open-source","text":"","title":"Open Source"},{"location":"libraries/#code","text":"black : Simple configuration-free code formatter. poetry : A package manager and dependency resolver. uv : An extremely fast Python package and project manager, written in Rust. cookiecutter : A tool to create projects like cookiecutters. (Project templates) sphinx : Python documentation generator. loguru : Simply logging. boltons : Pure python utilities that should be built-ins but bolt-ons instead.","title":"Code"},{"location":"libraries/#libraries","text":"hydra : Framework for elegantly configuring complex applications. pydantic : Data parsing, validation and settings management using python type annotations. typer : Build CLI appliactions using Python types. graphene : Building GraphQL schemas/types. httpx : A fully featured HTTP client for Python 3, which provides sync and async APIs, and support for both HTTP/1.1 and HTTP/2. pyvips : A fast image processing library with low memory needs based on vips. PyMuPDF : A high-performance Python library for data extraction, analysis, conversion & manipulation of PDF.","title":"Libraries"},{"location":"libraries/#functional","text":"toolz : A set of utility functions for iterators, functions, and dictionaries. coconut : Simple, elegant, Pythonic functional programming.","title":"Functional"},{"location":"libraries/#profiling-debugging","text":"py-spy : Sampling profiler, let you visualize your program without restarting the program. scalene : a high-performance, high-precision CPU, GPU, and memory profiler for Python with AI-powered optimization proposals. nvprof : Profiling for CUDA. nvidia-smi : Management and monitoring of NVIDIA GPU devices. strace : System call tracer for Linux . iotop : Top for IO. htop : Interactive system monitoring process viewer. nvtop : Task monitor for NVIDIA GPU.","title":"Profiling &amp; Debugging"},{"location":"libraries/#ml","text":"LazyPredict : help build a lot of basic models without much code and helps understand which models works better without any parameter tuning.","title":"ML"},{"location":"libraries/#others","text":"diagrams : A tool to help draw architectural diagrams. lux : Automatically visualize your pandas dataframe via a single print!","title":"Others"},{"location":"pullrequests/","text":"Pull Request Etiquette \u00b6 Why do we use a Pull Request workflow? \u00b6 PRs are a great way of sharing information and informs us of the changes that are occuring in our codebase. They are also an excellent way of getting your work reviewed by a peer. The primary reason we use PRs is to encourage quality in the commits that are made to our code repositories When PRs done well, the commits (and their attached messages) contained within tell a story to people examining the code at a later date. If we are not careful to ensure the quality of these commits, we silently lose this ability. Poor quality code can be refactored. A terrible commit lasts forever. What constitutes a good PR? \u00b6 A good quality PR will have the following characteristics: It will be a complete piece of work that adds value in some way. It will have a title that reflects the work within, and a summary that helps to understand the context of the change. There will be well written commit messages, with well crafted commits that tell the story of the development of this work. Ideally it will be small and easy to understand. Single commit PRs are usually easy to submit, review, and merge. The code contained within will meet the best practises set by the team wherever possible. A PR does not end at submission though. A code change is not made until it is merged and used in production. A good PR should be able to flow through a peer review system easily and quickly. Submitting Pull Requests \u00b6 Ensure there is a solid title and summary \u00b6 PRs are a Github workflow tool, so it's important to understand that the PR title, summary and eventual discussion are not as trackable as the the commit history. If we ever move away from Github, we'll likely lose this infomation. That said however, they are a very useful aid in ensuring that PRs are handled quickly and effectively. Ensure that your PR title is scannable. People will read through the list of PRs attached to a repo, and must be able to distinguish between them based on title. Include a story/issue reference if possible, so the reviewer can get any extra context. Include a reference to the subsystem affected, if this is a large codebase. Use keywords in the title to help people understand your intention with the PR, eg [WIP] to indicate that it's still in progress, so should not be merged. When using Github and have [WIP] tagged with your title, you should reframe from adding reviewers until you are ready to get it reviewed. You can directly shared your PR link to a reviewer after they agreed to look at your [WIP] PR. Rebase before you make the PR, if needed \u00b6 Unless there is a good reason not to rebase - typically because more than one person has been working on the branch - it is often a good idea to rebase your branch to tidy up before submitting the PR. Use git rebase -i master # or other reference, eg HEAD~5 For example: Squash 'oops, fix typo/bug' into their parent commit. There is no reason to create and solve bugs within a PR, unless there is educational value in highlighting them . Reword your commit messages for clarity. Once a PR is submitted, any rewording of commits will involve a rebase, which can then mess up the conversation in the PR. Aim for one succinct commit \u00b6 In an ideal world, your PR will be one small(ish) commit, doing one thing - in which case your life will be made easier, since the commit message and PR title/summary are equivalent. If your change contains more work than can be sensibly covered in a single commit though, do not try to squash it down into one. Commit history should tell a story, and if that story is long then it may require multiple commits to walk the reviewer through it. Describe your changes well in each commit \u00b6 Commit messages are invaluable to someone reading the history of the code base, and are critical for understanding why a change was made. Try to ensure that there is enough information in there for a person with no context or understanding of the code to make sense of the change. Where external information references are available - such as Issue/Story IDs, PR numbers - ensure that they are included in the commit message. Remember that your commit message must survive the ravages of time. Try to link to something that will be preserved equally well -- JIRA for example. Each commit message should include the reason why this commit was made. Usually by adding a sentence completing the form 'So that we...' will give an amazing amount of context to the history that the code change itself cannot Keep it small \u00b6 Try to only fix one issue or add one feature within a single pull request. The larger it is, the more complex it is to review and the more likely it will be delayed. Remember that reviewing PRs is taking time from someone else's day. If you must submit a large PR, try to at least make someone else aware of this fact, and arrange for their time to review and get the PR merged. It's not fair to the team to dump large pieces of work on their laps without warning. If you can rebase up a large PR into multiple smaller PRs, then do so. Reviewing Pull Requests \u00b6 It's a reviewers responsibility to ensure: Commit history is excellent Good changes are propagated quickly Code review is performed -- ideally with reasons and suggestions for changes requested. They understand what is being changed, from the perspective of someone examining the code in the future. Apply the same standards to all PRs. Provide the final action - ship it! or not It is a requester responsibility to ensure: Your PR is small. Your PR foucsing on one story or one bug only. Reviewed your changes before creating the PR. Done the necessary pre-checks -- Prettier to reformatted the code, Linter checks, Ran all of the necessary tests. Reply or react to every comment the reviewer have requested. Reviewers are the guardians of the commit history \u00b6 The importance of ensuring a quality commit history cannot be stressed enough. It is the historical context of all of the work that we do, and is vital for understanding the reasons why changes were made in the past. What is obvious now, will not be obvious in the future. Without a decent commit history, we may as well be storing all our code in files ending yyyy-mm-dd. The commit history of a code base is what allows people to understand why a change was made - the when, what, and where are automatically evident. When looking at a commit message, ask yourself the question - from the perspective of someone looking at this change without any knowledge of the codebase - 'do I understand why this change was made?' If any commit within the PR does not meet this standard, the PR should be rebased until it does. We cannot fix a commit history once it is in place, unlike our ability to refactor crappy code or fix bugs. A useful tip is simply asking the submitter to add a sentence to the commit message completing the sentence 'So that we...'. Keep the flow going \u00b6 Pull Requests are the fundamental unit of how we progress change. If PRs are getting clogged up in the system, either unreviewed or unmanaged, they are preventing a piece of work from being completed. As PRs clog up in the system, merges become more difficult, as other features and fixes are applied to the same codebase. This in turn slows them down further, and often completely blocks progress on a given codebase. There is a balance between flow and ensuring the quality of our PRs. As a reviewer you should make a call as to whether a code quality issue is sufficient enough to block the PR whilst the code is improved. Possibly it is more prudent to simply flag that the code needs rework, and raise an issue. Any quality issue that will obviously result in a bug should be fixed. We are all reviewers \u00b6 To make sure PRs flow through the system speedily, we must scale the PR review process. It is not sufficient (or fair!) to expect one or two people to review all PRs to our code. For starters, it creates a blocker every time those people are busy. Hopefully with the above guidelines, we can all start sharing the responsibility of being a reviewer. NB: With this in mind - if you are the first to comment on a PR, you are that PRs reviewer. If you feel that you can no longer be responsible for the subsequent merge or closure of the PR, then flag this up in the PR conversation, so someone else can take up the role. There's no reason why multiple people cannot comment on a PR and review it, and this is to be encouraged. Don't add to the PR yourself. \u00b6 It's sometimes tempting to fix a bug in a PR yourself, or to rework a section to meet coding standards, or just to make a feature better fit your needs. If you do this, you are no longer the reviewer of the PR. You are a collaborator, and so should not merge the PR. It is of course possible to find a new reviewer, but generally change will be speedier if you require the original submitter to fix the code themselves. Alternatively, if the original PR is 'good enough', raise the changes you'd like to see as separate stories/issues, and rework in your own PR. It is not the reviewers responsibility to test the code \u00b6 We are all busy people, and in the case of many PRs against our codebase we are not able or time-permitted to test the new code. We need to assume that the submitter has tested and formatted their code to the point of being happy with the work to be merged to master and subsequently released. If you, as a reviewer, are suspicious that the work in the PR has not been tested, raise this with the submitter. Find out how they have tested it, and refuse the work if they have not. They may not have a mechanism to test it, in which case you may need to help. If, as a submitter, you know that this change is not fully tested, highlight this in the PR text, and talk to the reviewer. As a submitter, it is highly encouraged to add a screenshot of successful tests along with other screenshots to show you have done your diligence before requesting for a PR to be reviewed. Some interesting links \u00b6 Amazing article on how to effectively use the commit history as an incredible documentation source: https://mislav.net/2014/02/hidden-documentation/ Github's recommendations for PRs, which has some useful tips on notifying teams and collegues via the github @-syntax. https://github.com/blog/1943-how-to-write-the-perfect-pull-request The GOV.UK git styleguide: https://github.com/alphagov/styleguides/blob/master/git.md An example of an extremely good commit message, for a change that would be highly confusing otherwise: https://github.com/gds-operations/vcloud-edge_gateway/pull/111","title":"Pull Requests Etiquette"},{"location":"pullrequests/#pull-request-etiquette","text":"","title":"Pull Request Etiquette"},{"location":"pullrequests/#why-do-we-use-a-pull-request-workflow","text":"PRs are a great way of sharing information and informs us of the changes that are occuring in our codebase. They are also an excellent way of getting your work reviewed by a peer. The primary reason we use PRs is to encourage quality in the commits that are made to our code repositories When PRs done well, the commits (and their attached messages) contained within tell a story to people examining the code at a later date. If we are not careful to ensure the quality of these commits, we silently lose this ability. Poor quality code can be refactored. A terrible commit lasts forever.","title":"Why do we use a Pull Request workflow?"},{"location":"pullrequests/#what-constitutes-a-good-pr","text":"A good quality PR will have the following characteristics: It will be a complete piece of work that adds value in some way. It will have a title that reflects the work within, and a summary that helps to understand the context of the change. There will be well written commit messages, with well crafted commits that tell the story of the development of this work. Ideally it will be small and easy to understand. Single commit PRs are usually easy to submit, review, and merge. The code contained within will meet the best practises set by the team wherever possible. A PR does not end at submission though. A code change is not made until it is merged and used in production. A good PR should be able to flow through a peer review system easily and quickly.","title":"What constitutes a good PR?"},{"location":"pullrequests/#submitting-pull-requests","text":"","title":"Submitting Pull Requests"},{"location":"pullrequests/#ensure-there-is-a-solid-title-and-summary","text":"PRs are a Github workflow tool, so it's important to understand that the PR title, summary and eventual discussion are not as trackable as the the commit history. If we ever move away from Github, we'll likely lose this infomation. That said however, they are a very useful aid in ensuring that PRs are handled quickly and effectively. Ensure that your PR title is scannable. People will read through the list of PRs attached to a repo, and must be able to distinguish between them based on title. Include a story/issue reference if possible, so the reviewer can get any extra context. Include a reference to the subsystem affected, if this is a large codebase. Use keywords in the title to help people understand your intention with the PR, eg [WIP] to indicate that it's still in progress, so should not be merged. When using Github and have [WIP] tagged with your title, you should reframe from adding reviewers until you are ready to get it reviewed. You can directly shared your PR link to a reviewer after they agreed to look at your [WIP] PR.","title":"Ensure there is a solid title and summary"},{"location":"pullrequests/#rebase-before-you-make-the-pr-if-needed","text":"Unless there is a good reason not to rebase - typically because more than one person has been working on the branch - it is often a good idea to rebase your branch to tidy up before submitting the PR. Use git rebase -i master # or other reference, eg HEAD~5 For example: Squash 'oops, fix typo/bug' into their parent commit. There is no reason to create and solve bugs within a PR, unless there is educational value in highlighting them . Reword your commit messages for clarity. Once a PR is submitted, any rewording of commits will involve a rebase, which can then mess up the conversation in the PR.","title":"Rebase before you make the PR, if needed"},{"location":"pullrequests/#aim-for-one-succinct-commit","text":"In an ideal world, your PR will be one small(ish) commit, doing one thing - in which case your life will be made easier, since the commit message and PR title/summary are equivalent. If your change contains more work than can be sensibly covered in a single commit though, do not try to squash it down into one. Commit history should tell a story, and if that story is long then it may require multiple commits to walk the reviewer through it.","title":"Aim for one succinct commit"},{"location":"pullrequests/#describe-your-changes-well-in-each-commit","text":"Commit messages are invaluable to someone reading the history of the code base, and are critical for understanding why a change was made. Try to ensure that there is enough information in there for a person with no context or understanding of the code to make sense of the change. Where external information references are available - such as Issue/Story IDs, PR numbers - ensure that they are included in the commit message. Remember that your commit message must survive the ravages of time. Try to link to something that will be preserved equally well -- JIRA for example. Each commit message should include the reason why this commit was made. Usually by adding a sentence completing the form 'So that we...' will give an amazing amount of context to the history that the code change itself cannot","title":"Describe your changes well in each commit"},{"location":"pullrequests/#keep-it-small","text":"Try to only fix one issue or add one feature within a single pull request. The larger it is, the more complex it is to review and the more likely it will be delayed. Remember that reviewing PRs is taking time from someone else's day. If you must submit a large PR, try to at least make someone else aware of this fact, and arrange for their time to review and get the PR merged. It's not fair to the team to dump large pieces of work on their laps without warning. If you can rebase up a large PR into multiple smaller PRs, then do so.","title":"Keep it small"},{"location":"pullrequests/#reviewing-pull-requests","text":"It's a reviewers responsibility to ensure: Commit history is excellent Good changes are propagated quickly Code review is performed -- ideally with reasons and suggestions for changes requested. They understand what is being changed, from the perspective of someone examining the code in the future. Apply the same standards to all PRs. Provide the final action - ship it! or not It is a requester responsibility to ensure: Your PR is small. Your PR foucsing on one story or one bug only. Reviewed your changes before creating the PR. Done the necessary pre-checks -- Prettier to reformatted the code, Linter checks, Ran all of the necessary tests. Reply or react to every comment the reviewer have requested.","title":"Reviewing Pull Requests"},{"location":"pullrequests/#reviewers-are-the-guardians-of-the-commit-history","text":"The importance of ensuring a quality commit history cannot be stressed enough. It is the historical context of all of the work that we do, and is vital for understanding the reasons why changes were made in the past. What is obvious now, will not be obvious in the future. Without a decent commit history, we may as well be storing all our code in files ending yyyy-mm-dd. The commit history of a code base is what allows people to understand why a change was made - the when, what, and where are automatically evident. When looking at a commit message, ask yourself the question - from the perspective of someone looking at this change without any knowledge of the codebase - 'do I understand why this change was made?' If any commit within the PR does not meet this standard, the PR should be rebased until it does. We cannot fix a commit history once it is in place, unlike our ability to refactor crappy code or fix bugs. A useful tip is simply asking the submitter to add a sentence to the commit message completing the sentence 'So that we...'.","title":"Reviewers are the guardians of the commit history"},{"location":"pullrequests/#keep-the-flow-going","text":"Pull Requests are the fundamental unit of how we progress change. If PRs are getting clogged up in the system, either unreviewed or unmanaged, they are preventing a piece of work from being completed. As PRs clog up in the system, merges become more difficult, as other features and fixes are applied to the same codebase. This in turn slows them down further, and often completely blocks progress on a given codebase. There is a balance between flow and ensuring the quality of our PRs. As a reviewer you should make a call as to whether a code quality issue is sufficient enough to block the PR whilst the code is improved. Possibly it is more prudent to simply flag that the code needs rework, and raise an issue. Any quality issue that will obviously result in a bug should be fixed.","title":"Keep the flow going"},{"location":"pullrequests/#we-are-all-reviewers","text":"To make sure PRs flow through the system speedily, we must scale the PR review process. It is not sufficient (or fair!) to expect one or two people to review all PRs to our code. For starters, it creates a blocker every time those people are busy. Hopefully with the above guidelines, we can all start sharing the responsibility of being a reviewer. NB: With this in mind - if you are the first to comment on a PR, you are that PRs reviewer. If you feel that you can no longer be responsible for the subsequent merge or closure of the PR, then flag this up in the PR conversation, so someone else can take up the role. There's no reason why multiple people cannot comment on a PR and review it, and this is to be encouraged.","title":"We are all reviewers"},{"location":"pullrequests/#dont-add-to-the-pr-yourself","text":"It's sometimes tempting to fix a bug in a PR yourself, or to rework a section to meet coding standards, or just to make a feature better fit your needs. If you do this, you are no longer the reviewer of the PR. You are a collaborator, and so should not merge the PR. It is of course possible to find a new reviewer, but generally change will be speedier if you require the original submitter to fix the code themselves. Alternatively, if the original PR is 'good enough', raise the changes you'd like to see as separate stories/issues, and rework in your own PR.","title":"Don't add to the PR yourself."},{"location":"pullrequests/#it-is-not-the-reviewers-responsibility-to-test-the-code","text":"We are all busy people, and in the case of many PRs against our codebase we are not able or time-permitted to test the new code. We need to assume that the submitter has tested and formatted their code to the point of being happy with the work to be merged to master and subsequently released. If you, as a reviewer, are suspicious that the work in the PR has not been tested, raise this with the submitter. Find out how they have tested it, and refuse the work if they have not. They may not have a mechanism to test it, in which case you may need to help. If, as a submitter, you know that this change is not fully tested, highlight this in the PR text, and talk to the reviewer. As a submitter, it is highly encouraged to add a screenshot of successful tests along with other screenshots to show you have done your diligence before requesting for a PR to be reviewed.","title":"It is not the reviewers responsibility to test the code"},{"location":"pullrequests/#some-interesting-links","text":"Amazing article on how to effectively use the commit history as an incredible documentation source: https://mislav.net/2014/02/hidden-documentation/ Github's recommendations for PRs, which has some useful tips on notifying teams and collegues via the github @-syntax. https://github.com/blog/1943-how-to-write-the-perfect-pull-request The GOV.UK git styleguide: https://github.com/alphagov/styleguides/blob/master/git.md An example of an extremely good commit message, for a change that would be highly confusing otherwise: https://github.com/gds-operations/vcloud-edge_gateway/pull/111","title":"Some interesting links"},{"location":"tutorials/","text":"Tutorials \u00b6 Packaging \u00b6 These tutorials describe how to create sharable libraries which can be used across projects. Structure : How to structure a package. Makefile : Creating Makefile for your project. Code Style : Recommended code style tools and, practices, and reference. Logging : How to configure logging. Testing : How to test a python package so it is easy to test for all developers and CICD tools. Anti Patterns : Anti-Patterns often seen in code base. API Design : High level design principles for package python APIs (and how it differs from Java). Patterns \u00b6 The following are tutorials regarding how to use the best design patterns. (TBA) Performance Optimization & Tuning \u00b6 The following are tutorials regarding how to performance tune and optimize critical portions of code. Lookup Tables : Performance of different lookup table implementations for both single and batch requests. Profiling Tools & Strategy : A general guide for how to profile python code. Multiprocessing : How to efficiently use all machine resources to process data. Libraries and Tools \u00b6 A list of recommended libraries and tools to use to make your development experience pleasant and painless.","title":"Overview"},{"location":"tutorials/#tutorials","text":"","title":"Tutorials"},{"location":"tutorials/#packaging","text":"These tutorials describe how to create sharable libraries which can be used across projects. Structure : How to structure a package. Makefile : Creating Makefile for your project. Code Style : Recommended code style tools and, practices, and reference. Logging : How to configure logging. Testing : How to test a python package so it is easy to test for all developers and CICD tools. Anti Patterns : Anti-Patterns often seen in code base. API Design : High level design principles for package python APIs (and how it differs from Java).","title":"Packaging"},{"location":"tutorials/#patterns","text":"The following are tutorials regarding how to use the best design patterns. (TBA)","title":"Patterns"},{"location":"tutorials/#performance-optimization-tuning","text":"The following are tutorials regarding how to performance tune and optimize critical portions of code. Lookup Tables : Performance of different lookup table implementations for both single and batch requests. Profiling Tools & Strategy : A general guide for how to profile python code. Multiprocessing : How to efficiently use all machine resources to process data.","title":"Performance Optimization &amp; Tuning"},{"location":"tutorials/#libraries-and-tools","text":"A list of recommended libraries and tools to use to make your development experience pleasant and painless.","title":"Libraries and Tools"},{"location":"tutorials/packaging/anti_patterns/","text":"Summary Use as much of built-ins as possible. Use what the language has to offer. Avoid blanket try/except block. Avoid Repeat Yourself. Anti-Patterns \u00b6 Below is a list of anti-patterns that we should avoid in our code bases. Maintanance \u00b6 A program is said to be maintainable if it is easy to understand and modify as per the requirement. Blanket Try/Except Block \u00b6 Avoid try : some_function () except Exception as e : pass Avoid try : some_function () except : log () Don't catch exceptions unless there is something you can do the fix it. It is usually a sign of badly designed API that uses exceptions to control flow. Use try : some_function ( my_input ) except FixableException as e : # Catch specific exception fix_the_problem () log . info ( f \"fixing the problem encountered by { my_input =} \" ) Use try : some_function ( my_input ) except NotFixableException as e : log . error ( f \"I can't do anything with { my_input =} \" ) raise e # it is also good idea to wrap your own exception `raise MyNewException(e)` Business logic in __init__.py \u00b6 __init__.py just like any files, it can contains any legal python code. The primary use of __init__.py is for python to understand the package structure of your code. A good use of __init__.py is to use it to expose objects and types that others could use. Do not have custom logics in this file. Avoid # inside __init__.py import my_module def some_function ( lst : List [ int ]) -> str : return \"\" . join ( lst ) class MyObject : def __init__ ( self ): ... def call ( self ): ... Use # inside __init__.py from my_module import myfunc , MyClass from my_second_module import myfunc as my_second_func , MyClass as MySecondClass from some_other_module import create_singleton my_singleton_obj = create_singleton () Avoid wildcard imports (import the world) \u00b6 During python's import mechanism any code in the module's import will be executed. Avoid from my_module import * Use from my_module import ( my_func , MyClass ) Not using ContextManager with stateful code \u00b6 Whenever you have code that is stateful with system resources like openning a file or create a network socket, you should use context manager to handle the context. Avoid f = open ( path ) for l in f : do_something ( l ) f . close () Use with open ( path ) as f : for l in f : do_something ( l ) Another important thing to note is that context manager is created for handling potential failures of system resources. Do not abuse it by using the context manager for things that doesn't require entering a context and exiting the context when done. Returning multiple variable types \u00b6 If a function is suppose to return a given type (e.g. List, Tuple, Dict, MyObject) suddenly returns something else (e.g. None ) Avoid def get_meaning_of_life ( question : str ) -> str : if question != \"The Hitchhiker's Guide to the Galaxy\" : return None else : return \"42\" Avoid def parse_value ( try_value : str ) -> Union [ int , str ]: try : return int ( try_value ) except ValueError as e : pass # this is also anti-pattern, using exception as flow control if not try_value . isalpha (): return None return try_value Use def get_meaning_of_life ( question : str ) -> str : if question != \"The Hitchhiker's Guide to the Galaxy\" : raise UnknownQuestionException ( \"Marvin!\" ) else : return \"42\" Using single letter or abbeviations in your variable name \u00b6 variable names should follow the principle of least astonishment. One-character names should generally be avoided, because they contain little to no information about what they refer to. However, there are a couple of exceptions that make some sense in their given contexts. Avoid passwd = \"abc\" cust = fname = \"John\" comp = \"Intuit\" d = { \"key\" : \"val\" } l = [ 1 , 2 , 3 ] t = ( 1 , 2 ) Use password = \"abc\" customer = \"John\" company = \"Intuit\" lookup = { \"key\" , \"val\" } short_ints = [ 1 , 2 , 3 ] temp_slice = ( 1 , 2 ) for key , value in lookup : if key == \"data\" : print ( f \" { key =} , { value =} \" Passing implicity data shapes arounds \u00b6 Generic containers is great for simple data types but should be avoided to hold nested values. Avoid def process ( data : Dict ) -> Dict : data_container = data [ \"my_key\" ][ 0 ][ - 1 ] pretty_data = prettify ( data_container ) data [ \"my_key\" ][ 0 ][ - 1 ][ \"Data\" ] = pretty_data return data You should use a dataclass or pydantic datamodel to model the object you want to operate on. Use @dataclass class LikedLocation : places : List [ POI ] likes : List [ Number ] def get_nth_places ( self , n : int ) -> POI : return self . places [ n ] def get_first_place ( self ) -> POI : return self . places [ 0 ] @dataclass class POI : visited_by : List [ Person ] def get_nth_visited ( self , n : int ) -> Person : return self . visited_by [ n ] def get_last_visited ( self ) -> Person : return self . get_nth_visited ( - 1 ) @dataclass class Person : first_name : str last_name : str date_of_birth : Datetime def format_last_visited ( locations : LikedLocation ) -> ` DisplayablePerson ` : last_visited_by = locations . get_first_place () . get_last_visited () # assume prettify returns an object of `DisplayablePerson` type return prettify ( last_visited_by ) Access protected members from outside the class \u00b6 Variables with leading _ are considered protected members. Access to this variable directly from outside of the class is dangerous and potential to run time errors. Avoid class Rectangle ( object ): def __init__ ( self , width , height ): self . _width = width self . _height = height rectangle = Rectangle ( 5 , 6 ) calculate ( rectangle . _width , rectangle . _height ) This is also an example of over-engineering. If all you need is some data that stores values. you can simple create dataclasses. Use @dataclass class Rectangle : width : Number height : Number def area ( self ) -> Number : return self . width * self . height Assigning to built-in (reserved) keywards \u00b6 Python has a number of built-in functions that are always accessible in the interpreter. Unless you have a special reason, you should neither overwrite these functions nor assign a value to a variable that has the same name as a built-in function. Overwriting a built-in might have undesired side effects or can cause runtime errors. Python developers usually use built-ins \u2018as-is\u2019. If their behaviour is changed, it can be very tricky to trace back the actual error. Avoid list = [ 1 , 2 , 3 ] my_list = list () # Error: TypeError: 'list' object is not callable sum = len # reassigned sum to be len operator sum ( range ( 10 )) # instead of 45, you get 10 Python is not Java \u00b6 Both Python and Java is object oriented programming langauge. However, there is Java way of doing things and Pythonic way of doing things. Do not translate directly from Java to Python. Avoid class AreaCalculator : @staticmethod def calc_rectangle ( width : Number , height : Number ) -> Number : return width * height the the above example the method calc_rectangle can just be a simple function since it does not require any internal states of the AreaCalculator . Avoid class Rectangle : def set_width ( self , width : Number ): self . _width = width def set_height ( self , height : Number ): self . _width = width def get_width ( self ) -> Number : return self . _width def set_heigh ( self ) -> Number : return self . _height Mixing positional and keyword arguments in function. \u00b6 prior to Python3.8 mixing positional argument with keyword arguments can lead to confusing results. Avoid def calculate_compound_interest ( principle : Number , rate : Number , terms : Number , compounded_monthly : bool = False , to_string : bool = False ): ... interest = calculate_compound_interest ( 1_000_000 , 2.5 , 10 , True , False ) interest2 = calculate_compound_interest ( 1_000_000 , 2.5 , 10 , to_string = True , compounded_monthly = True ) the caller of the function have to be aware of the keyword argument's order because it can be ambigous on which one is first or second. Use def calculate_compound_interest ( principle : Number , rate : Number , terms : Number , / , * , # Changed to indicate positional arguments ends compounded_monthly : bool = False , to_string : bool = False ): ... # interest = calculate_compound_interest(1_000_000, 2.5, 10, True, False) # this will fail interest2 = calculate_compound_interest ( 1_000_000 , 2.5 , 10 , to_string = True , compounded_monthly = True ) Redundant Context \u00b6 Do not add unnecessary data to variable names, especially if the context is already clear Avoid class Person : def __init__ ( self , person_first_name , person_last_name , person_age ): self . person_first_name = person_first_name self . person_last_name = person_last_name self . person_age_name = person_age_name Use class Person : def __init__ ( self , first_name , last_name , age ): self . first_name = first_name self . last_name = last_name self . age_name = age_name Mixing Configs and Constants \u00b6 Do not mix stuff that can change vs the values that doesn't change in one file. If your configs is getting longer than one page, you should consider externalize the config into a file. Avoid # inside config.py MY_CONST = 123 MY_DICT = { \"my_key\" : \"myvalue\" , ... \"last_key\" : \"lastvalue\" } ... # more stuff some_variable = os . enviorn [ \"ENV_VAR_NAME\" ] Use # inside constant.py MY_CONST = 123 MY_DICT = { \"my_key\" : \"myvalue\" , ... \"last_key\" : \"lastvalue\" } ... # more stuff # inside config.py # If you only have a few options here some_variable = os . enviorn [ \"ENV_VAR_NAME\" ] Use # inside constant.py MY_CONST = 123 MY_DICT = { \"my_key\" : \"myvalue\" , ... \"last_key\" : \"lastvalue\" } ... # more stuff # inside config.py # if you have business logic or longer than 5 options class FeatureIsNoneError ( Exception ): pass class TorchModelSetting ( BaseModel ): version : str device_count : class InterpolationSetting ( BaseModel ): enable_gpu : Optional [ bool ] model_settings : TorchModelSetting interpolation_factor : Optional [ conint ( gt = 2 )] interpolation_method : Optional [ str ] interpolate_on_integral : Optional [ bool ] class Config : extra = \"forbid\" Using in to check containment in a (large) list \u00b6 lack of understanding of the internals usually results in unexpected performance impacts. Checking if an element is contained in a list using the in operator might be slow for large lists. If you must check this often, consider to change the list to a set or use bisect . Avoid list_of_large_items = [ ... ] for key in some_other_iterable : if key in list_of_large_items : do_something () Use list_of_large_items = [ ... ] large_items = set ( list_of_large_items ) for key in some_other_relatively_large_iterable : if key in large_items : do_something () if you only need a few checks, it is better to use bisect Use list_of_a_handful_items = [ ... ] list_of_large_items = [ ... ] def contains ( a , x ): 'Locate the leftmost value exactly equal to x' i = bisect_left ( a , x ) if i != len ( a ) and a [ i ] == x : return True return False for key in list_of_a_handful_items : if contains ( list_of_large_items , key ): do_something () Flattening Arrow Code \u00b6 Stacked and nested if (conditional) statements make it hard to follow the code logic. Instead of nesting conditions you can combine them with boolean operators Avoid user = \"Snorlax\" age = 30 job = \"data scientist\" if age > 30 : if user == \"Snorlax\" : if job == \"data scientist\" : do_science_work () else : do_work () Use def is_data_scienstist ( age : int , user : str , job : str ) -> bool : ... if is_data_scienstist (): do_science_work () else : do_work () if the number of conditions is simple you can use boolean operator to connect Use if age > 30 and user == \"Snorlax\" and job == \"data scientist\" : do_science_work () else : do_work () String manipulation and formatting \u00b6 String is very versatille type and often you need to convert the data into string. Not using f-string can lead to a lot of code that is also error prune. Using f-string it is usually faster than the format() or string subsitution method with % . Rounding numbers to lesser digit Avoid pi = 3.1415926 pi_3_sifig = str ( math . round ( pi , 2 )) Use pi = 3.1415926 pi_3_sifig = f ' { pi : .2f } ' Use interest_rate = 0.14159 interest_rate_percentage = f ' { interest_rate : .2% } ' # will output '14.16%' Create leading digits Avoid month = 1 month_printable = \"0\" + str ( month ) if month < 10 else str ( month ) Use month = 1 month_printable = f ' { month : 02d } ' adding separator to stringify your number for presentation Avoid revenue = 1000000000 revenue_printable = some_function_formatter ( revenue , separator = \",\" ) # this will return 1,000,000,000 Use revenue = 1000000000 revenue_printable = f ' { revenue : ,d } ' # you can replace `,` with whatever separate. # If you need to pass dynamic separtor, you can do `f\"{N: {sep}d}\"` where sep is a variable If you are using python 3.8 or newer You no longer needs to have duplicated logic for constructing text that looks like name=value Avoid cost = \"$1,000\" print ( f \"cost= { cost } \" ) # or print(\"cost={cost}\".format(cost=cost)) # or god forbidden # print(\"cost =\" + cost) print ( \"cost=\" + ( 10 - len ( cost ) * \" \" + cost ) # to print 'cost= $1,000' Use cost = \"$1,000\" print ( f \" { cost =} \" ) # this is a short hand # if you need to add spaces before the value print ( f \" { cost = : > 10 } \" ) # will print, > is right align 'cost= $1,000' Converting datetime or part of it to string Avoid today = datetime . datetime ( year = 2021 , month = 8 , day = 8 ) today_ISO8601_parts = today . strftime ( \"%Y-%m- %d \" ) . split ( \"-\" ) print ( f \"today in discouraged format: { today_ISO8601_parts [ 1 ] } / { today_ISO8601_parts [ 2 ] } / { today_ISO8601_parts [ 0 ] } \" ) Use today = datetime . datetime ( year = 2021 , month = 8 , day = 8 ) print ( f \"today in discouraged format: { today : %m/%d/%Y } , ISO8601 format: { today : %Y-%m-%d } \" ) Not using defaultdict or Counter instead of dict \u00b6 Python's dict is a very versatile container, but it shouldn't be used in certain cases. You are trying to count things You are manually adding default values Avoid text = \"some long text you are trying to count the frequency of words.\" word_count_dict = {} for w in text . split ( \" \" ): if w in word_count_dict : word_count_dict [ w ] += 1 else : word_count_dict [ w ] = 1 Use from collections import defaultdict word_count_dict = defaultdict ( int ) for w in text . split ( \" \" ): word_count_dict [ w ] += 1 Or even better to use Counter Use from collections import Counter word_counts = Counter ( text . split ( \" \" )) Using dict or mutable type for immutable data \u00b6 Often dict is used as general container of data like config values and etc. However, if the data is suppose to be immutable, then you really should use the correct immutable types to avoid accidental data overwrite. Avoid config = {} config [ \"service\" ] = \"https://www.intuit.com/prod/service_name\" config [ \"rate\" ] = 25 config [ \"country\" ] = \"US\" Use from dataclasses import dataclass @dataclass ( frozen = True ) class Config : service : str rate : int country : str Using string instead of enum \u00b6 String types are a poor choice when the list of possibility is fairly small and finite. Using string type also could leads to risk of misspelling, escaping exhaustive checks with linters or pattern matching and code duplications. Avoid def train ( classifier = \"tree\" ): if classifier == \"tree\" : pass elif classifier == \"forest\" : pass elif classifier == \"cnn\" : pass else : raise ValueError Use from enum import Enum class Classifier ( Enum ): TREE = 0 FOREST = 1 CNN = 2 def train ( classifer : Classifier = Classifier . LBFGS ): if classifer == Classifier . TREE : pass elif classifer == Classifier . FOREST : pass elif classifer == Classifier . CNN : pass else : raise ValueError with python3.10, you can do pattern matching.","title":"Anti Patterns"},{"location":"tutorials/packaging/anti_patterns/#anti-patterns","text":"Below is a list of anti-patterns that we should avoid in our code bases.","title":"Anti-Patterns"},{"location":"tutorials/packaging/anti_patterns/#maintanance","text":"A program is said to be maintainable if it is easy to understand and modify as per the requirement.","title":"Maintanance"},{"location":"tutorials/packaging/anti_patterns/#blanket-tryexcept-block","text":"Avoid try : some_function () except Exception as e : pass Avoid try : some_function () except : log () Don't catch exceptions unless there is something you can do the fix it. It is usually a sign of badly designed API that uses exceptions to control flow. Use try : some_function ( my_input ) except FixableException as e : # Catch specific exception fix_the_problem () log . info ( f \"fixing the problem encountered by { my_input =} \" ) Use try : some_function ( my_input ) except NotFixableException as e : log . error ( f \"I can't do anything with { my_input =} \" ) raise e # it is also good idea to wrap your own exception `raise MyNewException(e)`","title":"Blanket Try/Except Block"},{"location":"tutorials/packaging/anti_patterns/#business-logic-in-__init__py","text":"__init__.py just like any files, it can contains any legal python code. The primary use of __init__.py is for python to understand the package structure of your code. A good use of __init__.py is to use it to expose objects and types that others could use. Do not have custom logics in this file. Avoid # inside __init__.py import my_module def some_function ( lst : List [ int ]) -> str : return \"\" . join ( lst ) class MyObject : def __init__ ( self ): ... def call ( self ): ... Use # inside __init__.py from my_module import myfunc , MyClass from my_second_module import myfunc as my_second_func , MyClass as MySecondClass from some_other_module import create_singleton my_singleton_obj = create_singleton ()","title":"Business logic in __init__.py"},{"location":"tutorials/packaging/anti_patterns/#avoid-wildcard-imports-import-the-world","text":"During python's import mechanism any code in the module's import will be executed. Avoid from my_module import * Use from my_module import ( my_func , MyClass )","title":"Avoid wildcard imports (import the world)"},{"location":"tutorials/packaging/anti_patterns/#not-using-contextmanager-with-stateful-code","text":"Whenever you have code that is stateful with system resources like openning a file or create a network socket, you should use context manager to handle the context. Avoid f = open ( path ) for l in f : do_something ( l ) f . close () Use with open ( path ) as f : for l in f : do_something ( l ) Another important thing to note is that context manager is created for handling potential failures of system resources. Do not abuse it by using the context manager for things that doesn't require entering a context and exiting the context when done.","title":"Not using ContextManager with stateful code"},{"location":"tutorials/packaging/anti_patterns/#returning-multiple-variable-types","text":"If a function is suppose to return a given type (e.g. List, Tuple, Dict, MyObject) suddenly returns something else (e.g. None ) Avoid def get_meaning_of_life ( question : str ) -> str : if question != \"The Hitchhiker's Guide to the Galaxy\" : return None else : return \"42\" Avoid def parse_value ( try_value : str ) -> Union [ int , str ]: try : return int ( try_value ) except ValueError as e : pass # this is also anti-pattern, using exception as flow control if not try_value . isalpha (): return None return try_value Use def get_meaning_of_life ( question : str ) -> str : if question != \"The Hitchhiker's Guide to the Galaxy\" : raise UnknownQuestionException ( \"Marvin!\" ) else : return \"42\"","title":"Returning multiple variable types"},{"location":"tutorials/packaging/anti_patterns/#using-single-letter-or-abbeviations-in-your-variable-name","text":"variable names should follow the principle of least astonishment. One-character names should generally be avoided, because they contain little to no information about what they refer to. However, there are a couple of exceptions that make some sense in their given contexts. Avoid passwd = \"abc\" cust = fname = \"John\" comp = \"Intuit\" d = { \"key\" : \"val\" } l = [ 1 , 2 , 3 ] t = ( 1 , 2 ) Use password = \"abc\" customer = \"John\" company = \"Intuit\" lookup = { \"key\" , \"val\" } short_ints = [ 1 , 2 , 3 ] temp_slice = ( 1 , 2 ) for key , value in lookup : if key == \"data\" : print ( f \" { key =} , { value =} \"","title":"Using single letter or abbeviations in your variable name"},{"location":"tutorials/packaging/anti_patterns/#passing-implicity-data-shapes-arounds","text":"Generic containers is great for simple data types but should be avoided to hold nested values. Avoid def process ( data : Dict ) -> Dict : data_container = data [ \"my_key\" ][ 0 ][ - 1 ] pretty_data = prettify ( data_container ) data [ \"my_key\" ][ 0 ][ - 1 ][ \"Data\" ] = pretty_data return data You should use a dataclass or pydantic datamodel to model the object you want to operate on. Use @dataclass class LikedLocation : places : List [ POI ] likes : List [ Number ] def get_nth_places ( self , n : int ) -> POI : return self . places [ n ] def get_first_place ( self ) -> POI : return self . places [ 0 ] @dataclass class POI : visited_by : List [ Person ] def get_nth_visited ( self , n : int ) -> Person : return self . visited_by [ n ] def get_last_visited ( self ) -> Person : return self . get_nth_visited ( - 1 ) @dataclass class Person : first_name : str last_name : str date_of_birth : Datetime def format_last_visited ( locations : LikedLocation ) -> ` DisplayablePerson ` : last_visited_by = locations . get_first_place () . get_last_visited () # assume prettify returns an object of `DisplayablePerson` type return prettify ( last_visited_by )","title":"Passing implicity data shapes arounds"},{"location":"tutorials/packaging/anti_patterns/#access-protected-members-from-outside-the-class","text":"Variables with leading _ are considered protected members. Access to this variable directly from outside of the class is dangerous and potential to run time errors. Avoid class Rectangle ( object ): def __init__ ( self , width , height ): self . _width = width self . _height = height rectangle = Rectangle ( 5 , 6 ) calculate ( rectangle . _width , rectangle . _height ) This is also an example of over-engineering. If all you need is some data that stores values. you can simple create dataclasses. Use @dataclass class Rectangle : width : Number height : Number def area ( self ) -> Number : return self . width * self . height","title":"Access protected members from outside the class"},{"location":"tutorials/packaging/anti_patterns/#assigning-to-built-in-reserved-keywards","text":"Python has a number of built-in functions that are always accessible in the interpreter. Unless you have a special reason, you should neither overwrite these functions nor assign a value to a variable that has the same name as a built-in function. Overwriting a built-in might have undesired side effects or can cause runtime errors. Python developers usually use built-ins \u2018as-is\u2019. If their behaviour is changed, it can be very tricky to trace back the actual error. Avoid list = [ 1 , 2 , 3 ] my_list = list () # Error: TypeError: 'list' object is not callable sum = len # reassigned sum to be len operator sum ( range ( 10 )) # instead of 45, you get 10","title":"Assigning to built-in (reserved) keywards"},{"location":"tutorials/packaging/anti_patterns/#python-is-not-java","text":"Both Python and Java is object oriented programming langauge. However, there is Java way of doing things and Pythonic way of doing things. Do not translate directly from Java to Python. Avoid class AreaCalculator : @staticmethod def calc_rectangle ( width : Number , height : Number ) -> Number : return width * height the the above example the method calc_rectangle can just be a simple function since it does not require any internal states of the AreaCalculator . Avoid class Rectangle : def set_width ( self , width : Number ): self . _width = width def set_height ( self , height : Number ): self . _width = width def get_width ( self ) -> Number : return self . _width def set_heigh ( self ) -> Number : return self . _height","title":"Python is not Java"},{"location":"tutorials/packaging/anti_patterns/#mixing-positional-and-keyword-arguments-in-function","text":"prior to Python3.8 mixing positional argument with keyword arguments can lead to confusing results. Avoid def calculate_compound_interest ( principle : Number , rate : Number , terms : Number , compounded_monthly : bool = False , to_string : bool = False ): ... interest = calculate_compound_interest ( 1_000_000 , 2.5 , 10 , True , False ) interest2 = calculate_compound_interest ( 1_000_000 , 2.5 , 10 , to_string = True , compounded_monthly = True ) the caller of the function have to be aware of the keyword argument's order because it can be ambigous on which one is first or second. Use def calculate_compound_interest ( principle : Number , rate : Number , terms : Number , / , * , # Changed to indicate positional arguments ends compounded_monthly : bool = False , to_string : bool = False ): ... # interest = calculate_compound_interest(1_000_000, 2.5, 10, True, False) # this will fail interest2 = calculate_compound_interest ( 1_000_000 , 2.5 , 10 , to_string = True , compounded_monthly = True )","title":"Mixing positional and keyword arguments in function."},{"location":"tutorials/packaging/anti_patterns/#redundant-context","text":"Do not add unnecessary data to variable names, especially if the context is already clear Avoid class Person : def __init__ ( self , person_first_name , person_last_name , person_age ): self . person_first_name = person_first_name self . person_last_name = person_last_name self . person_age_name = person_age_name Use class Person : def __init__ ( self , first_name , last_name , age ): self . first_name = first_name self . last_name = last_name self . age_name = age_name","title":"Redundant Context"},{"location":"tutorials/packaging/anti_patterns/#mixing-configs-and-constants","text":"Do not mix stuff that can change vs the values that doesn't change in one file. If your configs is getting longer than one page, you should consider externalize the config into a file. Avoid # inside config.py MY_CONST = 123 MY_DICT = { \"my_key\" : \"myvalue\" , ... \"last_key\" : \"lastvalue\" } ... # more stuff some_variable = os . enviorn [ \"ENV_VAR_NAME\" ] Use # inside constant.py MY_CONST = 123 MY_DICT = { \"my_key\" : \"myvalue\" , ... \"last_key\" : \"lastvalue\" } ... # more stuff # inside config.py # If you only have a few options here some_variable = os . enviorn [ \"ENV_VAR_NAME\" ] Use # inside constant.py MY_CONST = 123 MY_DICT = { \"my_key\" : \"myvalue\" , ... \"last_key\" : \"lastvalue\" } ... # more stuff # inside config.py # if you have business logic or longer than 5 options class FeatureIsNoneError ( Exception ): pass class TorchModelSetting ( BaseModel ): version : str device_count : class InterpolationSetting ( BaseModel ): enable_gpu : Optional [ bool ] model_settings : TorchModelSetting interpolation_factor : Optional [ conint ( gt = 2 )] interpolation_method : Optional [ str ] interpolate_on_integral : Optional [ bool ] class Config : extra = \"forbid\"","title":"Mixing Configs and Constants"},{"location":"tutorials/packaging/anti_patterns/#using-in-to-check-containment-in-a-large-list","text":"lack of understanding of the internals usually results in unexpected performance impacts. Checking if an element is contained in a list using the in operator might be slow for large lists. If you must check this often, consider to change the list to a set or use bisect . Avoid list_of_large_items = [ ... ] for key in some_other_iterable : if key in list_of_large_items : do_something () Use list_of_large_items = [ ... ] large_items = set ( list_of_large_items ) for key in some_other_relatively_large_iterable : if key in large_items : do_something () if you only need a few checks, it is better to use bisect Use list_of_a_handful_items = [ ... ] list_of_large_items = [ ... ] def contains ( a , x ): 'Locate the leftmost value exactly equal to x' i = bisect_left ( a , x ) if i != len ( a ) and a [ i ] == x : return True return False for key in list_of_a_handful_items : if contains ( list_of_large_items , key ): do_something ()","title":"Using in to check containment in a (large) list"},{"location":"tutorials/packaging/anti_patterns/#flattening-arrow-code","text":"Stacked and nested if (conditional) statements make it hard to follow the code logic. Instead of nesting conditions you can combine them with boolean operators Avoid user = \"Snorlax\" age = 30 job = \"data scientist\" if age > 30 : if user == \"Snorlax\" : if job == \"data scientist\" : do_science_work () else : do_work () Use def is_data_scienstist ( age : int , user : str , job : str ) -> bool : ... if is_data_scienstist (): do_science_work () else : do_work () if the number of conditions is simple you can use boolean operator to connect Use if age > 30 and user == \"Snorlax\" and job == \"data scientist\" : do_science_work () else : do_work ()","title":"Flattening Arrow Code"},{"location":"tutorials/packaging/anti_patterns/#string-manipulation-and-formatting","text":"String is very versatille type and often you need to convert the data into string. Not using f-string can lead to a lot of code that is also error prune. Using f-string it is usually faster than the format() or string subsitution method with % . Rounding numbers to lesser digit Avoid pi = 3.1415926 pi_3_sifig = str ( math . round ( pi , 2 )) Use pi = 3.1415926 pi_3_sifig = f ' { pi : .2f } ' Use interest_rate = 0.14159 interest_rate_percentage = f ' { interest_rate : .2% } ' # will output '14.16%' Create leading digits Avoid month = 1 month_printable = \"0\" + str ( month ) if month < 10 else str ( month ) Use month = 1 month_printable = f ' { month : 02d } ' adding separator to stringify your number for presentation Avoid revenue = 1000000000 revenue_printable = some_function_formatter ( revenue , separator = \",\" ) # this will return 1,000,000,000 Use revenue = 1000000000 revenue_printable = f ' { revenue : ,d } ' # you can replace `,` with whatever separate. # If you need to pass dynamic separtor, you can do `f\"{N: {sep}d}\"` where sep is a variable If you are using python 3.8 or newer You no longer needs to have duplicated logic for constructing text that looks like name=value Avoid cost = \"$1,000\" print ( f \"cost= { cost } \" ) # or print(\"cost={cost}\".format(cost=cost)) # or god forbidden # print(\"cost =\" + cost) print ( \"cost=\" + ( 10 - len ( cost ) * \" \" + cost ) # to print 'cost= $1,000' Use cost = \"$1,000\" print ( f \" { cost =} \" ) # this is a short hand # if you need to add spaces before the value print ( f \" { cost = : > 10 } \" ) # will print, > is right align 'cost= $1,000' Converting datetime or part of it to string Avoid today = datetime . datetime ( year = 2021 , month = 8 , day = 8 ) today_ISO8601_parts = today . strftime ( \"%Y-%m- %d \" ) . split ( \"-\" ) print ( f \"today in discouraged format: { today_ISO8601_parts [ 1 ] } / { today_ISO8601_parts [ 2 ] } / { today_ISO8601_parts [ 0 ] } \" ) Use today = datetime . datetime ( year = 2021 , month = 8 , day = 8 ) print ( f \"today in discouraged format: { today : %m/%d/%Y } , ISO8601 format: { today : %Y-%m-%d } \" )","title":"String manipulation and formatting"},{"location":"tutorials/packaging/anti_patterns/#not-using-defaultdict-or-counter-instead-of-dict","text":"Python's dict is a very versatile container, but it shouldn't be used in certain cases. You are trying to count things You are manually adding default values Avoid text = \"some long text you are trying to count the frequency of words.\" word_count_dict = {} for w in text . split ( \" \" ): if w in word_count_dict : word_count_dict [ w ] += 1 else : word_count_dict [ w ] = 1 Use from collections import defaultdict word_count_dict = defaultdict ( int ) for w in text . split ( \" \" ): word_count_dict [ w ] += 1 Or even better to use Counter Use from collections import Counter word_counts = Counter ( text . split ( \" \" ))","title":"Not using defaultdict or Counter instead of dict"},{"location":"tutorials/packaging/anti_patterns/#using-dict-or-mutable-type-for-immutable-data","text":"Often dict is used as general container of data like config values and etc. However, if the data is suppose to be immutable, then you really should use the correct immutable types to avoid accidental data overwrite. Avoid config = {} config [ \"service\" ] = \"https://www.intuit.com/prod/service_name\" config [ \"rate\" ] = 25 config [ \"country\" ] = \"US\" Use from dataclasses import dataclass @dataclass ( frozen = True ) class Config : service : str rate : int country : str","title":"Using dict or mutable type for immutable data"},{"location":"tutorials/packaging/anti_patterns/#using-string-instead-of-enum","text":"String types are a poor choice when the list of possibility is fairly small and finite. Using string type also could leads to risk of misspelling, escaping exhaustive checks with linters or pattern matching and code duplications. Avoid def train ( classifier = \"tree\" ): if classifier == \"tree\" : pass elif classifier == \"forest\" : pass elif classifier == \"cnn\" : pass else : raise ValueError Use from enum import Enum class Classifier ( Enum ): TREE = 0 FOREST = 1 CNN = 2 def train ( classifer : Classifier = Classifier . LBFGS ): if classifer == Classifier . TREE : pass elif classifer == Classifier . FOREST : pass elif classifer == Classifier . CNN : pass else : raise ValueError with python3.10, you can do pattern matching.","title":"Using string instead of enum"},{"location":"tutorials/packaging/api_design/","text":"Summary Use a flat namespace for packages/modules. Use primitive-typed functions/methods. High cohesion & low coupling. Avoid Low cohension & high coupling. Avoid pandas objects as arguments and returns. Avoid exposing classes from packages/modules. API Design \u00b6 This guide uses the term \"API\" to refer to the interface of a package, module, class or function. It outlines specific recommendations for each type, but general rules apply to all. API design is arguably the most important part of writing clean reusable code. A well-designed API should be intuitive and straightforward for users, requiring minimal reference to documentation. This topic is particularly relevant to data science code because API design often becomes an afterthought compared to the actual functionality and data modeling. This leads to code that's often single-use, even when the core functionality is quite general. Additionally, duplicate functionality gets implemented in separate repositories by different developers who find it not worthwhile to adapt hardcoded/specialized logic to their projects. This problem can also exist within a single project/repository. Modules can be tailored so specifically to a dataset or model architecture that using them outside of that context becomes impossible. Goals of a Good API \u00b6 A well-designed API should exhibit the following properties: Legibility : Users should understand how to utilize an API immediately. The meaning of arguments and return values should be obvious, requiring minimal documentation. State Simplicity : Users should readily comprehend the state's lifetime within an API. There should be no hidden caching behavior, allowing users to deallocate state on demand. Interoperability : Users should immediately understand how to utilize library functions/classes with functions/classes from other libraries, even outside of the given library. Example Libraries \u00b6 Python Standard Library : This library embodies a consistent style and uses patterns that make it very user-friendly. NumPy : This function-oriented library exposes one primary class and numerous functions. For most numpy functions, the first argument is typically a np.ndarray , and the output is also a np.ndarray . This means that understanding the vast array of functions in the library only necessitates familiarity with one object type. Scikit-learn : This object-oriented library exposes many model classes. Most scikit-learn objects inherit from a small subset of base classes with well-defined methods. Nearly all scikit-learn objects have a fit(X, y) method and a predict(X) method. This consistency allows most scikit-learn objects to be interchangeable within user application code. The libraries mentioned above are all heavily used within data science code and they follow design patterns that we can learn from. We can derive a base set of guidelines from the patterns we see in these interfaces. Design Guidelines \u00b6 These guidelines are principles to strive for when designing an API, but not absolute rules. Exceptions exist: Use a flat namespace for packages/modules. Use primitive types in function/method interfaces. Avoid dictionary-oriented interfaces. Avoid pandas-oriented interfaces. Avoid variable length keyword arguments (**kwargs) Avoid exposing stateful objects. Use mostly functions. Minimize disk access and serialization. Use a flat namespace for packages/modules \u00b6 This makes it so that a developer coming into your code will always know what publicly accessible identifiers are available at the library level. This also avoids having to make multiple imports from the same library and polluting application code namespace with library objects/modules. Furthermore this discourages users from aliasing imports since the library only exposes a single namespace. Use import my_library result = my_library . submodule1 . function1 () obj = my_library . submodule1 . Class1 () result2 = my_library . submodule2 . function2 () obj2 = my_library . submodule2 . Class2 () Avoid from my_library.submodule1 import function as function1 , Class1 from my_library.submodule2 import function as function2 , Class2 result = function1 () obj = Class1 () result2 = function2 () obj2 = Class2 () An example of a library that does this extremely well is numpy . Use primitive types in function/method interfaces \u00b6 The goal is to create simple and intuitive function/method interfaces by primarily using primitive data types (e.g., str , int , float , bool ). This promotes code readability, maintainability, and testability. Why Primitive Types? - Clarity : Functions with clear input and output types are easier to understand and use. - Testability : Unit tests can be written more easily when dealing with simple data types. - Flexibility : Functions that rely on primitive types are more adaptable to different use cases. When to Consider Complex Data Structures \u00b6 While primitive types are generally preferred, there are situations where complex data structures like dictionaries or DataFrames might be necessary: Large Data Set s: For performance reasons, using NumPy arrays or Pandas DataFrames can be more efficient. Structured Data : Dictionaries can be useful for representing key-value pairs, especially when the structure is not fixed. Domain-Specific Data : In some cases, using custom data classes might be appropriate to model complex domain-specific concepts. However, even when using complex data structures, strive to minimize their complexity and expose a simple interface to the user. Best Practices \u00b6 Prioritize Primitive Types : Always consider using primitive types as the primary building blocks of your API. Use Type Hints : Employ Python's type hinting system to improve code readability and catch potential type errors early. Minimize Dictionary Usage : If dictionaries are necessary, consider using namedtuples or custom data classes for structured data. 4. Optimize Object Usage : When working with Object, focus on exposing a simple interface that hides the underlying complexity. Handle Errors Gracefully : Implement robust error handling mechanisms to prevent unexpected behavior and provide informative error messages. By following these guidelines, you can create APIs that are both powerful and easy to use. Dictionary-Oriented Interfaces \u00b6 A common pattern is to use dictionary arguments and return values. The problem with this kind of design is it often creates an implicit API which is not immediately obvious from the function signature. A function that can be understood from inspecting the name and arguments is generally better than one that requires reading the docstring. Dictionary Arguments \u00b6 Often times deeply nested structures are required as input arguments. These structures are nearly always unnecessary to perform the core logic/utility provided by the code. The structural requirements of the dictionary cannot be known by inspecting the function arguments. The user of the function must look at the documentation or the function body to understand how to use it. Avoid def join_first_and_last ( parts , sep = ' ' ): return parts [ \"first\" ] + sep + parts [ \"last\" ] # Example call data = { \"first\" : \"hello\" , \"last\" : \"world\" } result = join_first_and_last ( data ) Solution : Force the user to destructure the data prior to passing it in to the function. This is a good practice because it limits the scope of what each function \"knows\" about. If the function can be written in a way that the context in which is it called is unimportant, this makes the code more reusable and easier to test. Use def join_first_and_last ( first , last , sep = ' ' ): return first + sep + last # Example call data = { \"first\" : \"hello\" , \"last\" : \"world\" } result = join_first_and_last ( data [ \"first\" ], data [ \"last\" ]) The most common exceptions to using a dictionary as an argument is when the dictionary is meant to be iterated over within the function or used as a lookup table. Exception # Iteration usage def max_key_length ( dictionary : dict ): return max ( map ( len , dictionary . keys ())) # Lookup table usage def largest_zip_code_population ( zip_codes : list , zip_to_population : dict ): return max ( zip_to_population . get ( code , - 1 ) for code in zip_codes ) Dictionary Returns \u00b6 When dictionaries are used as return values, this causes a similar problem to using them as an arguments. The user of the function cannot determine the structure of the dictionary by inspecting the function signature. This also nearly always requires that the user to destructure the return value in order to use it. Avoid def precision_recall ( model , x_test , y_test ): precision = model . precision ( x_test , y_test ) recall = model . recall ( x_test , y_test ) return { \"precision\" : precision , \"recall\" : recall } # Example call result = precision_recall ( ... ) precision = result [ \"precision\" ] recall = result [ \"recall\" ] Use tuples as returns. This avoids forcing the user to destructure the return value. Notice that in the above example the dictionary keys needed to be duplicated in the calling code in order to access the data. Use def precision_recall ( model , x_test , y_test ): precision = model . precision ( x_test , y_test ) recall = model . recall ( x_test , y_test ) return precision , recall # Example call precision , recall = precision_recall ( ... ) The most common exception to using a dictionary as a return type is when the dictionary is meant to be iterated over by the calling code or used as a lookup table. Exception def get_column_types (): return { 'zip_code' : str , 'salaries_and_wages' : int , 'flag_old_or_blind' : bool , } Pandas-Oriented Interfaces \u00b6 The worst functions are those that consume DataFrames as arguments and have very specific requirements on what columns are present. This is one of the most common anti-patterns found in data science python code. This creates an implicit API that in the worst case (and most common case) requires that the developer read the entire function body to understand how to use it. Imagine if you needed to read the body of every function included in the python standard library in order to understand it. We take it for granted that the API is so simple and easy to understand. Pandas Arguments \u00b6 This is the most common type of function in data science codebases (often significantly more complex). Avoid def typecast_zip_codes ( df : pd . DataFrame ): df [ 'zip' ] = df [ 'zip' ] . astype ( int ) # Example call typecast_zip_codes ( df ) It is a function designed for a specific dataset that assumes that the calling code has a DataFrame with a specific column. It is unclear from the signature that the DataFrame is modified in-place. It is unclear from the signature if there is a return that should be used. Solution : Make the API oriented around a series/array and to leave the deconstruction of the DataFrame to the calling code. Use def typecast_zip_codes ( array : np . ndarray ): return array . astype ( int ) # Example call df [ 'zip' ] = typecast_zip_codes ( df [ 'zip' ] . values ) The calling code is burdened with the responsibilty of deconstructing/modifying the DataFrame but this is a good thing . This allows the calling code to fully understand the side-effects of the call without having to look at the implementation. This is easier to test and much more maintainable. If this kind of API is used throughout the codebase, then there will be very few places that have assumptions about DataFrame structure. Pandas Class Arguments \u00b6 A similar problem occurs in object-oriented library design but to a worse degree. A common anti-pattern in data science code is to unnecessarily save entire datasets as a class member (or within multiple classes). This is generally done so that the class can later reference that member variable from a method without including it in the method argument list. Avoid class MyPredictor : def __init__ ( self , df : pd . DataFrame , config : dict ): self . train_df = df self . iterations = config . get ( 'iterations' , 0 ) def train ( self ): ... def predict ( self ): ... In this code it is confusing what the lifetime of the df argument is and what it is even used for. An object should only track state that must be present during the entire lifetime of the object. If the df is only used in the train method then it should not be passed to the constructor. A second issue here is that the config dictionary argument creates an implicit API. The user of the class cannot tell what the contents of the config dictionary should be without looking at the implementation of the constructor. This is why frameworks like sklearn make all of their hyperparameters explicit in their constructors. It allows you to be able to tell at the call-site if you are configuring the estimator correctly. Imagine if the caller had made a typo in a dictionary key for the example above. The mistake would be silently ignored unless an explict check was implemented. An explicit check would introduce unnecessary complexity when a language feature (arguments) is already available to handle this. Solution : Colocate the DataFrame with its usage and make the class constructor take explicit arguments. Use class MyPredictor : def __init__ ( self , n_iterations : int ): self . n_iterations = n_iterations def train ( self , X : np . ndarray , y : np . ndarray ): ... def predict ( self , X : np . ndarray ): ... Avoid variable length keyword arguments (**kwargs) \u00b6 Variable length keyword arguments should almost never be used because they hide what is actually being done with them. Whenever they are used, the user will almost always have to read the documentation to understand how they are being used. Avoid def example ( arg , ** kwargs ): if 'foo' in kwargs : ... The only time when keyword arguments reduce ambiguity is if your library is wrapping a call to another library which is highly configurable. Rather than replicating all of the keyword arguments in the calling function signature, forward the configuration parameters with kwargs. Use import foo_library def example ( arg , ** foo_params ): result = foo_library . example_call ( ** foo_params ) ... Another potential use-case where keyword arguments are useful is if the intention is to iterate over the key-value pairs (for example to use as metadata). Because this actually changes the call syntax of the function, it is generally more explicit to pass this kind of object in as a dictionary instead. Avoid def example ( ** tags ): ... Use def example ( tags : dict ): ... Avoid exposing stateful objects \u00b6 Only expose classes when a library/module must manage state. Object-oriented data science code is very useful when complex state cannot be abstracted into procedures without exposing many different internal state variables to the end user. However, nearly all object-oriented code can be written in a procedure-oriented style. The benefit of procedure-oriented code is that it makes the lifetime and usage of in-memory state well-understood. Single Method Classes \u00b6 The most common object-oriented anti-pattern is when a class has a constructor and only a single public method. This can almost always be replaced with a function call with zero cost. Avoid class Greeter : def __init__ ( self , name ): self . name = name def greet ( self ): return f 'Hello, { self . name } !' Use def greet ( name ): return f 'Hello, { name } !' Argument Avoidance \u00b6 Another common anti-pattern is the use of classes to reduce the number of arguments on each method call. Rather than scoping arguments to the functions they are relevant to, all arguments for all methods are passed to the constructor and then 0-argument methods called. This design pattern often occurs when developers attempt to define a process with an object. Avoid from sklearn.tree import DecisionTreeRegressor class MyModel : def __init__ ( self , X_train , y_train , X_test , y_test , hyperparams ): self . X_train = X_train self . y_train = y_train self . X_test = X_test self . y_test = y_test self . hyperparams = hyperparams self . model = None def create_model ( self ): self . model = DecisionTreeRegressor ( ** self . hyperparams ) def train ( self ): if not self . model : self . create_model () self . model . fit ( self . X_train , self . y_train ) def score ( self ): return self . model . score ( self . X_test , self . y_test ) A common problem with this type of design is that the class methods have a required call order that is not explicit in the API. In the example above, this is remedied by checking if member variables have been set. This complicates the logic, adds almost no value, and distracts from the actual logical operations being performed. Use from sklearn.tree import DecisionTreeRegressor def create_model ( hyperparams ): return DecisionTreeRegressor ( ** hyperparams ) def train ( model , X_train , y_train ): model . fit ( X_train , y_train ) def score ( model , X_test , y_test ): return model . score ( X_test , y_test ) Minimize disk access and serialization. \u00b6 It is common for data science code to produce and consume many artifacts during both training and prediction. This causes many codebases to become saturated with disk access to these artifacts and hardcoded paths in every single module. In codebases where this is left unchecked, disk access can be nearly everywhere and cause huge performance issues. It is common to find a class which has a path as a constuctor argument to load some config or data. Functions/classes should almost never read from disk to execute the provided logic. The logic and the state management should always be separated. The following example may seem like an egregious use of serialization, but these patterns can all be commonly found in code. Avoid import numpy as np import yaml class Encoder : def __init__ ( self , config_filename : str , categories_filename : str ): config = yaml . load ( config_filename , Loader = yaml . FullLoader ) self . min_frequency = config . get ( 'min_frequency' , 10 ) self . default = config . get ( 'default' , - 1 ) self . categories = None if categories_filename is not None : self . categories = np . load ( categories_filename ) def train ( self , training_filename : str ): values = np . load ( training_filename ) unique , counts = np . unique ( values , return_counts = True ) categories = unique [ counts >= self . min_frequency ] self . categories = categories def predict ( self , test_filename ): values = np . load ( test_filename ) lookup_table = dict ( zip ( self . categories , range ( len ( self . categories )))) return np . array ([ lookup_table . get ( item , self . default ) for item in values ]) def save ( self , output_filename ): np . save ( output_filename , self . categories ) The constructor expects two arguments, one for configuration and one for a previously saved state. Unless the function body were read, it would be unclear that the second argument was used to load previously saved state. Next, each function takes a filename as input instead of an in-memory object which makes the object far more difficult to use. Solution : The exposed API should always assume that configurations and parameters will be in-memory (unless they absolutely cannot be). This vastly simplifies the usage of the object. Use import numpy as np class Encoder : def __init__ ( self , min_frequency = 10 , default =- 1 ): self . min_frequency = min_frequency self . default = default self . lookup_table = None def train ( self , values : np . ndarray ): unique , counts = np . unique ( values , return_counts = True ) categories = unique [ counts >= self . min_frequency ] self . lookup_table = dict ( zip ( categories , range ( len ( categories )))) def predict ( self , values ): return np . array ([ self . lookup_table . get ( item , self . default ) for item in values ]) This design requires that the Encoder object would be serialized by user. This is preferable to implementing custom serialization logic as part of the class since the user can use common shared serialization facilities (e.g. pickle) to manage disk access. Cohesion & Coupling \u00b6 High cohesion is when you have a class that does a well defined job. Low cohesion is when a class does a lot of jobs that does't have much in common. High coupling is when modules are tightly interconnected via many complex interfaces and information flows. Low coupling is when modules are loosely interconnected and isolated from the implementation details of each other. One module can be changed or replaced without impacting other modules. Coupling and cohension goes in pairs like the two faces of a coin. Cohension is like an onion -- it requires layers to keep things separate and distinct. It is not just about the size of your functions but also the relationships with each other. Coupling is like a knife -- it cuts through the layers and connects multiple facets. It is not just about the number of objects but also the degree of mutual interdependence. A good book for this is described in Chapter 10 of code like a pro book.","title":"API Design"},{"location":"tutorials/packaging/api_design/#api-design","text":"This guide uses the term \"API\" to refer to the interface of a package, module, class or function. It outlines specific recommendations for each type, but general rules apply to all. API design is arguably the most important part of writing clean reusable code. A well-designed API should be intuitive and straightforward for users, requiring minimal reference to documentation. This topic is particularly relevant to data science code because API design often becomes an afterthought compared to the actual functionality and data modeling. This leads to code that's often single-use, even when the core functionality is quite general. Additionally, duplicate functionality gets implemented in separate repositories by different developers who find it not worthwhile to adapt hardcoded/specialized logic to their projects. This problem can also exist within a single project/repository. Modules can be tailored so specifically to a dataset or model architecture that using them outside of that context becomes impossible.","title":"API Design"},{"location":"tutorials/packaging/api_design/#goals-of-a-good-api","text":"A well-designed API should exhibit the following properties: Legibility : Users should understand how to utilize an API immediately. The meaning of arguments and return values should be obvious, requiring minimal documentation. State Simplicity : Users should readily comprehend the state's lifetime within an API. There should be no hidden caching behavior, allowing users to deallocate state on demand. Interoperability : Users should immediately understand how to utilize library functions/classes with functions/classes from other libraries, even outside of the given library.","title":"Goals of a Good API"},{"location":"tutorials/packaging/api_design/#example-libraries","text":"Python Standard Library : This library embodies a consistent style and uses patterns that make it very user-friendly. NumPy : This function-oriented library exposes one primary class and numerous functions. For most numpy functions, the first argument is typically a np.ndarray , and the output is also a np.ndarray . This means that understanding the vast array of functions in the library only necessitates familiarity with one object type. Scikit-learn : This object-oriented library exposes many model classes. Most scikit-learn objects inherit from a small subset of base classes with well-defined methods. Nearly all scikit-learn objects have a fit(X, y) method and a predict(X) method. This consistency allows most scikit-learn objects to be interchangeable within user application code. The libraries mentioned above are all heavily used within data science code and they follow design patterns that we can learn from. We can derive a base set of guidelines from the patterns we see in these interfaces.","title":"Example Libraries"},{"location":"tutorials/packaging/api_design/#design-guidelines","text":"These guidelines are principles to strive for when designing an API, but not absolute rules. Exceptions exist: Use a flat namespace for packages/modules. Use primitive types in function/method interfaces. Avoid dictionary-oriented interfaces. Avoid pandas-oriented interfaces. Avoid variable length keyword arguments (**kwargs) Avoid exposing stateful objects. Use mostly functions. Minimize disk access and serialization.","title":"Design Guidelines"},{"location":"tutorials/packaging/api_design/#namespace","text":"This makes it so that a developer coming into your code will always know what publicly accessible identifiers are available at the library level. This also avoids having to make multiple imports from the same library and polluting application code namespace with library objects/modules. Furthermore this discourages users from aliasing imports since the library only exposes a single namespace. Use import my_library result = my_library . submodule1 . function1 () obj = my_library . submodule1 . Class1 () result2 = my_library . submodule2 . function2 () obj2 = my_library . submodule2 . Class2 () Avoid from my_library.submodule1 import function as function1 , Class1 from my_library.submodule2 import function as function2 , Class2 result = function1 () obj = Class1 () result2 = function2 () obj2 = Class2 () An example of a library that does this extremely well is numpy .","title":"Use a flat namespace for packages/modules"},{"location":"tutorials/packaging/api_design/#types","text":"The goal is to create simple and intuitive function/method interfaces by primarily using primitive data types (e.g., str , int , float , bool ). This promotes code readability, maintainability, and testability. Why Primitive Types? - Clarity : Functions with clear input and output types are easier to understand and use. - Testability : Unit tests can be written more easily when dealing with simple data types. - Flexibility : Functions that rely on primitive types are more adaptable to different use cases.","title":"Use primitive types in function/method interfaces"},{"location":"tutorials/packaging/api_design/#when-to-consider-complex-data-structures","text":"While primitive types are generally preferred, there are situations where complex data structures like dictionaries or DataFrames might be necessary: Large Data Set s: For performance reasons, using NumPy arrays or Pandas DataFrames can be more efficient. Structured Data : Dictionaries can be useful for representing key-value pairs, especially when the structure is not fixed. Domain-Specific Data : In some cases, using custom data classes might be appropriate to model complex domain-specific concepts. However, even when using complex data structures, strive to minimize their complexity and expose a simple interface to the user.","title":"When to Consider Complex Data Structures"},{"location":"tutorials/packaging/api_design/#best-practices","text":"Prioritize Primitive Types : Always consider using primitive types as the primary building blocks of your API. Use Type Hints : Employ Python's type hinting system to improve code readability and catch potential type errors early. Minimize Dictionary Usage : If dictionaries are necessary, consider using namedtuples or custom data classes for structured data. 4. Optimize Object Usage : When working with Object, focus on exposing a simple interface that hides the underlying complexity. Handle Errors Gracefully : Implement robust error handling mechanisms to prevent unexpected behavior and provide informative error messages. By following these guidelines, you can create APIs that are both powerful and easy to use.","title":"Best Practices"},{"location":"tutorials/packaging/api_design/#dictionary-types","text":"A common pattern is to use dictionary arguments and return values. The problem with this kind of design is it often creates an implicit API which is not immediately obvious from the function signature. A function that can be understood from inspecting the name and arguments is generally better than one that requires reading the docstring.","title":"Dictionary-Oriented Interfaces"},{"location":"tutorials/packaging/api_design/#dictionary-arguments","text":"Often times deeply nested structures are required as input arguments. These structures are nearly always unnecessary to perform the core logic/utility provided by the code. The structural requirements of the dictionary cannot be known by inspecting the function arguments. The user of the function must look at the documentation or the function body to understand how to use it. Avoid def join_first_and_last ( parts , sep = ' ' ): return parts [ \"first\" ] + sep + parts [ \"last\" ] # Example call data = { \"first\" : \"hello\" , \"last\" : \"world\" } result = join_first_and_last ( data ) Solution : Force the user to destructure the data prior to passing it in to the function. This is a good practice because it limits the scope of what each function \"knows\" about. If the function can be written in a way that the context in which is it called is unimportant, this makes the code more reusable and easier to test. Use def join_first_and_last ( first , last , sep = ' ' ): return first + sep + last # Example call data = { \"first\" : \"hello\" , \"last\" : \"world\" } result = join_first_and_last ( data [ \"first\" ], data [ \"last\" ]) The most common exceptions to using a dictionary as an argument is when the dictionary is meant to be iterated over within the function or used as a lookup table. Exception # Iteration usage def max_key_length ( dictionary : dict ): return max ( map ( len , dictionary . keys ())) # Lookup table usage def largest_zip_code_population ( zip_codes : list , zip_to_population : dict ): return max ( zip_to_population . get ( code , - 1 ) for code in zip_codes )","title":"Dictionary Arguments"},{"location":"tutorials/packaging/api_design/#dictionary-returns","text":"When dictionaries are used as return values, this causes a similar problem to using them as an arguments. The user of the function cannot determine the structure of the dictionary by inspecting the function signature. This also nearly always requires that the user to destructure the return value in order to use it. Avoid def precision_recall ( model , x_test , y_test ): precision = model . precision ( x_test , y_test ) recall = model . recall ( x_test , y_test ) return { \"precision\" : precision , \"recall\" : recall } # Example call result = precision_recall ( ... ) precision = result [ \"precision\" ] recall = result [ \"recall\" ] Use tuples as returns. This avoids forcing the user to destructure the return value. Notice that in the above example the dictionary keys needed to be duplicated in the calling code in order to access the data. Use def precision_recall ( model , x_test , y_test ): precision = model . precision ( x_test , y_test ) recall = model . recall ( x_test , y_test ) return precision , recall # Example call precision , recall = precision_recall ( ... ) The most common exception to using a dictionary as a return type is when the dictionary is meant to be iterated over by the calling code or used as a lookup table. Exception def get_column_types (): return { 'zip_code' : str , 'salaries_and_wages' : int , 'flag_old_or_blind' : bool , }","title":"Dictionary Returns"},{"location":"tutorials/packaging/api_design/#pandas-types","text":"The worst functions are those that consume DataFrames as arguments and have very specific requirements on what columns are present. This is one of the most common anti-patterns found in data science python code. This creates an implicit API that in the worst case (and most common case) requires that the developer read the entire function body to understand how to use it. Imagine if you needed to read the body of every function included in the python standard library in order to understand it. We take it for granted that the API is so simple and easy to understand.","title":"Pandas-Oriented Interfaces"},{"location":"tutorials/packaging/api_design/#pandas-arguments","text":"This is the most common type of function in data science codebases (often significantly more complex). Avoid def typecast_zip_codes ( df : pd . DataFrame ): df [ 'zip' ] = df [ 'zip' ] . astype ( int ) # Example call typecast_zip_codes ( df ) It is a function designed for a specific dataset that assumes that the calling code has a DataFrame with a specific column. It is unclear from the signature that the DataFrame is modified in-place. It is unclear from the signature if there is a return that should be used. Solution : Make the API oriented around a series/array and to leave the deconstruction of the DataFrame to the calling code. Use def typecast_zip_codes ( array : np . ndarray ): return array . astype ( int ) # Example call df [ 'zip' ] = typecast_zip_codes ( df [ 'zip' ] . values ) The calling code is burdened with the responsibilty of deconstructing/modifying the DataFrame but this is a good thing . This allows the calling code to fully understand the side-effects of the call without having to look at the implementation. This is easier to test and much more maintainable. If this kind of API is used throughout the codebase, then there will be very few places that have assumptions about DataFrame structure.","title":"Pandas Arguments"},{"location":"tutorials/packaging/api_design/#pandas-class-arguments","text":"A similar problem occurs in object-oriented library design but to a worse degree. A common anti-pattern in data science code is to unnecessarily save entire datasets as a class member (or within multiple classes). This is generally done so that the class can later reference that member variable from a method without including it in the method argument list. Avoid class MyPredictor : def __init__ ( self , df : pd . DataFrame , config : dict ): self . train_df = df self . iterations = config . get ( 'iterations' , 0 ) def train ( self ): ... def predict ( self ): ... In this code it is confusing what the lifetime of the df argument is and what it is even used for. An object should only track state that must be present during the entire lifetime of the object. If the df is only used in the train method then it should not be passed to the constructor. A second issue here is that the config dictionary argument creates an implicit API. The user of the class cannot tell what the contents of the config dictionary should be without looking at the implementation of the constructor. This is why frameworks like sklearn make all of their hyperparameters explicit in their constructors. It allows you to be able to tell at the call-site if you are configuring the estimator correctly. Imagine if the caller had made a typo in a dictionary key for the example above. The mistake would be silently ignored unless an explict check was implemented. An explicit check would introduce unnecessary complexity when a language feature (arguments) is already available to handle this. Solution : Colocate the DataFrame with its usage and make the class constructor take explicit arguments. Use class MyPredictor : def __init__ ( self , n_iterations : int ): self . n_iterations = n_iterations def train ( self , X : np . ndarray , y : np . ndarray ): ... def predict ( self , X : np . ndarray ): ...","title":"Pandas Class Arguments"},{"location":"tutorials/packaging/api_design/#kwargs","text":"Variable length keyword arguments should almost never be used because they hide what is actually being done with them. Whenever they are used, the user will almost always have to read the documentation to understand how they are being used. Avoid def example ( arg , ** kwargs ): if 'foo' in kwargs : ... The only time when keyword arguments reduce ambiguity is if your library is wrapping a call to another library which is highly configurable. Rather than replicating all of the keyword arguments in the calling function signature, forward the configuration parameters with kwargs. Use import foo_library def example ( arg , ** foo_params ): result = foo_library . example_call ( ** foo_params ) ... Another potential use-case where keyword arguments are useful is if the intention is to iterate over the key-value pairs (for example to use as metadata). Because this actually changes the call syntax of the function, it is generally more explicit to pass this kind of object in as a dictionary instead. Avoid def example ( ** tags ): ... Use def example ( tags : dict ): ...","title":"Avoid variable length keyword arguments (**kwargs)"},{"location":"tutorials/packaging/api_design/#classes","text":"Only expose classes when a library/module must manage state. Object-oriented data science code is very useful when complex state cannot be abstracted into procedures without exposing many different internal state variables to the end user. However, nearly all object-oriented code can be written in a procedure-oriented style. The benefit of procedure-oriented code is that it makes the lifetime and usage of in-memory state well-understood.","title":"Avoid exposing stateful objects"},{"location":"tutorials/packaging/api_design/#single-method-classes","text":"The most common object-oriented anti-pattern is when a class has a constructor and only a single public method. This can almost always be replaced with a function call with zero cost. Avoid class Greeter : def __init__ ( self , name ): self . name = name def greet ( self ): return f 'Hello, { self . name } !' Use def greet ( name ): return f 'Hello, { name } !'","title":"Single Method Classes"},{"location":"tutorials/packaging/api_design/#argument-avoidance","text":"Another common anti-pattern is the use of classes to reduce the number of arguments on each method call. Rather than scoping arguments to the functions they are relevant to, all arguments for all methods are passed to the constructor and then 0-argument methods called. This design pattern often occurs when developers attempt to define a process with an object. Avoid from sklearn.tree import DecisionTreeRegressor class MyModel : def __init__ ( self , X_train , y_train , X_test , y_test , hyperparams ): self . X_train = X_train self . y_train = y_train self . X_test = X_test self . y_test = y_test self . hyperparams = hyperparams self . model = None def create_model ( self ): self . model = DecisionTreeRegressor ( ** self . hyperparams ) def train ( self ): if not self . model : self . create_model () self . model . fit ( self . X_train , self . y_train ) def score ( self ): return self . model . score ( self . X_test , self . y_test ) A common problem with this type of design is that the class methods have a required call order that is not explicit in the API. In the example above, this is remedied by checking if member variables have been set. This complicates the logic, adds almost no value, and distracts from the actual logical operations being performed. Use from sklearn.tree import DecisionTreeRegressor def create_model ( hyperparams ): return DecisionTreeRegressor ( ** hyperparams ) def train ( model , X_train , y_train ): model . fit ( X_train , y_train ) def score ( model , X_test , y_test ): return model . score ( X_test , y_test )","title":"Argument Avoidance"},{"location":"tutorials/packaging/api_design/#serialization","text":"It is common for data science code to produce and consume many artifacts during both training and prediction. This causes many codebases to become saturated with disk access to these artifacts and hardcoded paths in every single module. In codebases where this is left unchecked, disk access can be nearly everywhere and cause huge performance issues. It is common to find a class which has a path as a constuctor argument to load some config or data. Functions/classes should almost never read from disk to execute the provided logic. The logic and the state management should always be separated. The following example may seem like an egregious use of serialization, but these patterns can all be commonly found in code. Avoid import numpy as np import yaml class Encoder : def __init__ ( self , config_filename : str , categories_filename : str ): config = yaml . load ( config_filename , Loader = yaml . FullLoader ) self . min_frequency = config . get ( 'min_frequency' , 10 ) self . default = config . get ( 'default' , - 1 ) self . categories = None if categories_filename is not None : self . categories = np . load ( categories_filename ) def train ( self , training_filename : str ): values = np . load ( training_filename ) unique , counts = np . unique ( values , return_counts = True ) categories = unique [ counts >= self . min_frequency ] self . categories = categories def predict ( self , test_filename ): values = np . load ( test_filename ) lookup_table = dict ( zip ( self . categories , range ( len ( self . categories )))) return np . array ([ lookup_table . get ( item , self . default ) for item in values ]) def save ( self , output_filename ): np . save ( output_filename , self . categories ) The constructor expects two arguments, one for configuration and one for a previously saved state. Unless the function body were read, it would be unclear that the second argument was used to load previously saved state. Next, each function takes a filename as input instead of an in-memory object which makes the object far more difficult to use. Solution : The exposed API should always assume that configurations and parameters will be in-memory (unless they absolutely cannot be). This vastly simplifies the usage of the object. Use import numpy as np class Encoder : def __init__ ( self , min_frequency = 10 , default =- 1 ): self . min_frequency = min_frequency self . default = default self . lookup_table = None def train ( self , values : np . ndarray ): unique , counts = np . unique ( values , return_counts = True ) categories = unique [ counts >= self . min_frequency ] self . lookup_table = dict ( zip ( categories , range ( len ( categories )))) def predict ( self , values ): return np . array ([ self . lookup_table . get ( item , self . default ) for item in values ]) This design requires that the Encoder object would be serialized by user. This is preferable to implementing custom serialization logic as part of the class since the user can use common shared serialization facilities (e.g. pickle) to manage disk access.","title":"Minimize disk access and serialization."},{"location":"tutorials/packaging/api_design/#cohesion-coupling","text":"High cohesion is when you have a class that does a well defined job. Low cohesion is when a class does a lot of jobs that does't have much in common. High coupling is when modules are tightly interconnected via many complex interfaces and information flows. Low coupling is when modules are loosely interconnected and isolated from the implementation details of each other. One module can be changed or replaced without impacting other modules. Coupling and cohension goes in pairs like the two faces of a coin. Cohension is like an onion -- it requires layers to keep things separate and distinct. It is not just about the size of your functions but also the relationships with each other. Coupling is like a knife -- it cuts through the layers and connects multiple facets. It is not just about the number of objects but also the degree of mutual interdependence. A good book for this is described in Chapter 10 of code like a pro book.","title":"Cohesion &amp; Coupling"},{"location":"tutorials/packaging/code_style/","text":"Summary Use ruff or black code formatting. Use either napoleon docstring type annotations or PEP-484 built-in type annotations. Use ruff for linting (static code analysis). Use mypy for type checking. Code Style \u00b6 A consistent code style makes code easier to read and understand. The main benefit of increased legibility and beautiful code is saving developer time. A consistent code style allows anyone to quickly familiarize themselves with a codebase even if it is being worked on by many people. A recommended read is the clean code python . There are many tools/guidelines which can be used to for code style in python so a common question is which one to use. The reality is that most of the tools/guidelines are fairly good and have generally useful default behavior. In practice, for most tools, warnings end up being ignored or specific linting options are turned off because they are too restrictive. If a developer of a repository disagrees with an option, then they will just remove it entirely so that it does not bother them anymore. When using a set of guidelines which does not have a tool associated with it, rules are often forgotten or implemented differently by different developers. Goals \u00b6 Due to the reasons above, the goal of a styling specification would be to have the following properties: Automation : The code style must have a tool. It must have automated formatting so that users do not need to worry about styling the code manually. Minimal Configuration : Minimize options so that users do not need to make decisions about style. More decisions leads to more configuration and more inconsistencies in code style. Customizable Rules : The tool should allow leads to set the type of rules going to be enforced and rules can be ignored. Example Tools \u00b6 The following are the most commonly used tools for python code quality and ensure consistency. Linters \u00b6 Linters are usually broken into two types: Logical and Stylistic linters. Logical Linter: Code Errors Code with potentially unintended results Dangrous code patterns Stylistic Liners: Code not conforming to defined conventions. Below are some of the popular tools ruff ; A modern(Rust) linter that combines the best features of flake8, pylint, and isort. It's highly customizable and offers fast performance. flake8 : the wrapper which verifies PEP-8 , pyflakes, and circular complexity \u201c. It has a low rate of false positives. Formatters \u00b6 Formatters will format the actual python file based on rules. ruff A drop-in replacement for black , but much faster. black An automated code formatter with no configuration options. A huge benefit of black is that unlike the previously mentioned tools this does not have formatting options. Black does not strictly follow PEP-8 guidelines but is generally compliant. It is also important to understand that black is only an opinionated formatter , it does not check all of the styles problem that other linters do. isort Format imports order. Sort imports alphabetically, and automatically separated into sections and by type. In a very controlled development environment, each may be useful tools by just enabling the default behavior and forbidding custom options. Recommendation \u00b6 For a comprehensive and efficient code style and linting solution, we highly recommend using ruff . It offers a powerful combination of features and performance benefits: Fast Performance : Leverages Rust for speed, making it ideal for large codebases. Comprehensive Linting : Checks for a wide range of style issues, potential bugs, and performance problems. Customization : Easily tailor the linting rules to your specific needs. Compatibility : Seamlessly integrates with popular tools like black and isort . For formatting, black remains a solid choice for its simplicity and strict adherence to a specific style. However, for those seeking a faster alternative, ruff can also be used as a drop-in replacement. To ensure type safety, mypy should be used to statically type-check your code. Style Guides \u00b6 Beyond automated checks and warnings for code, style guides provide good recommendations for how you should write code. Three style guides in particular provide almost universally useful advice: PEP-8 : The official python style guide provides the basis for almost all formatters and other style guides. This provides the broadest and most foundational rules for writing legible python code. Google Style Guide : This has a lot of useful recommendations, however many of their guidelines were chosen to maximize compatibility between python 2 and python 3. Since python 2 is officially deprecated, all of the compatibility guidelines should be ignored. For these reasons the main Hitchhiker's Guide to Python : This provides its own style guide which is universally regarded as good practice. Though none of these style guides can programmatically format code for you, they are essential reference for how to write clean, simple python code. Type Annotations \u00b6 Type annotations of some kind are recommended but can be implemented in different ways. Some form of type annotation is always recommended so that code usage is less ambiguous. The recommended strategies for annotating the types of your code: PEP-484 Style type annotations \u00b6 This places the type annotation directly in code. Pros: Code can be checked using automated code checkers (See: mypy & pyre-check ) Cons: Tends to make function declarations verbose Can be difficult to document data structures (Requires importing typing ) def subtract ( minuend : int , subtrahend : int ) -> int : \"\"\" Subtract the subtrahend from the minuend. Args: minuend: The basis to subtract from. subtrahend: The value to subtract. Result: difference: The difference between the numbers. \"\"\" return minuend - subtrahend Variable Naming \u00b6 In the style guides mentioned above, the naming conventions are primarily concerned with the case used (snake_case vs CamelCase, etc.) However, this section is in regards to the actual words which should be used in a name. Naming is one of the hardest problems in programming. Good naming can drastically increase the readability of your code. To name things well, variables should have the following properties: Descriptive Unambiguous Should not contain the type. This is what type annotations are for Should be short if possible. Long names make code more difficult to read Abbrivations for names is ok if it is well-known but should be reframed. ie. n , sz , cnt , idx , dt , ts , env , cfg , ctx etc. Dictionaries \u00b6 Dictionaries should have information about both the key and value in the name. Acceptable Forms: {singular-key}_{plural-values} (Preferred) Somewhat ambiguous, but succinct {key}_to_{value} Somewhat verbose {value}_by_{key} Somewhat verbose, less direct than {key}_to_{value} {key}_{value}_lookup Somewhat verbose, but describes how it should be used Each of the forms can be more appropriate in different scenarios where the meaning is more clear in context. Use word_lengths = { \"hello\" : 5 , \"world\" : 5 } index_to_word = { 1 : \"hello\" , 2 : \"world\" } word_by_index = { 1 : \"hello\" , 2 : \"world\" } index_word_lookup = { 1 : \"hello\" , 2 : \"world\" } Avoid words = { 1 : \"hello\" , 2 : \"world\" } # Ambiguous, No key information word_lookup = { 1 : \"hello\" , 2 : \"world\" } # No key information word_dict = { 1 : \"hello\" , 2 : \"world\" } # Type included in name In some cases there are well-understood mappings that are meant to be iterated over. In these cases it makes sense to just use the pluralized version of the word and name the variable after the contents. Anything that is meant to be iterated over should be pluralized. Exception headers = { \"Content-Type\" : \"application/json\" } cookies = { \"tz\" : \"America/Los_Angeles\" } hyperparameters = { \"min_samples_leaf\" : 50 } gunicorn_options = { \"workers\" : 8 } # Gunicorn uses `options` Avoid header_name_to_value = { \"Content-Type\" : \"application/json\" } # Too verbose cookie_name_to_value = { \"tz\" : \"America/Los_Angeles\" } # Too verbose hyperparameter_name_to_value = { \"min_samples_leaf\" : 50 } # Too verbose In other cases, there are libraries that have predefined names for their arguments that do not follow the conventions above. In this case it is acceptable to follow their conventions when interacting with their code. This makes the code less ambiguous because the library name and the application-code variable name match. Exception feed_dict = { \"name\" : tensor } # TensorFlow uses this name so it is acceptable Lists/Series/Arrays/Sets \u00b6 Collections (non-dictionary) should always be the plural version of whatever is contained within. In the case where the value type is ambiguous, try to name the collection so it is possible to determine what is contained within. Use `python zip_codes = [92127, 12345] names = {\"Johnny\", \"Lisa\", \"Mark\", \"Denny\"} column_names = [\"zip_code\", \"wages\"] # `names` suffix indicates string value Avoid zip_list = [ 92127 , 12345 ] # Do not include type list = [ 92127 , 12345 ] # Shadows built-in list, Ambiguous items = [ 92127 , 12345 ] # Ambiguous columns = [ \"zip_code\" , \"wages\" ] # Value type is ambiguous Integers/Floats \u00b6 When possible, number names should indicate the how the value should be used. Contrary to the rules regarding collections, there are a many well-understood values that indicate a number which should be unambiguous. Use limit = 10 threshold = 0.7 size = 10 # Potentially ambiguous index = 10 # Potentially ambiguous count = 10 # Potentially ambiguous n_items = 10 # `n` prefix always indicates a number, preferred `num_items` min_items = 10 # `min` prefix always indicates a number max_items = 10 # `max` prefix always indicates a number buy_price = 10 # Domain specific words always indicate a number Avoid items = 10 # Ambiguous, could indicate a collection n = 10 # Not Descriptive n_value = 10 # `value` suffix is not descriptive n_quantity = 10 # `quantity` suffix is not descriptive Strings \u00b6 When possible, string names should indicate how the value should be used. The naming conventions of strings are similar to the conventions of numbers. There are many well-understood names that could indicate a string type. Common Forms: {descriptor}_key May indicate lookup key {descriptor}_name May indicate lookup key Use environment_name = 'cdev' environment_key = 'cdev' environment = 'cdev' # Well-known string value, Potentially ambiguous env = 'cdev' # Well-known abbreviation, Potentially ambiguous Avoid value = 'cdev' # Ambiguous config = 'cdev' # Ambiguous could indicate an object Classes \u00b6 Class instances should always be named after the class itself. For the purposes of describing the forms which a variable should be named the following class name will be used DescriptionNoun . Acceptable Forms: {description}_{noun} (Preferred) CamelCase to snake_case conversion {noun} More succinct, potentially ambiguous Use class ExampleContext : pass example_context = ExampleContext () context = ExampleContext () Functions \u00b6 Function should always be named in lower case and separated with underscore ( _ ). The words that you use to name your function should clearly describe the function\u2019s intent (what the function does). All functions should clearly indicates the input and output variable types. When returning from function avoid generic container types like List or Dict , use additional hints if you must return such types, ie. List[int] or Dict[str, int] . Acceptable Forms: def {verb}_{intent}({nouns}:T) -> U: (Preferred) def {verb}({noun}:T}) -> U: More succinct, potentially ambiguous Use def sum ( iterable : Iterable ) -> Number : def remove_underscore ( s : str ) -> str : Avoid def foo ( a ): def process ( parameters : Dict ) -> Dict : def calculate ( results : Dict [ Any , Dict ]) -> Dict : Ruff Setup \u00b6 It is recommend to setup ruff in your pyproject.toml file. [tool.ruff] line-length = 88 target-version = \"py312\" [tool.ruff.lint] select = [ \"A\" , # Assignment expressions \"ARG\" , # Argument-related issues \"B\" , # Builtin usage \"C\" , # Complexity \"COM\" , # Comments \"DTZ\" , # Datetime and time zone issues \"E\" , # Pythonic style guide violations \"EM\" , # Empty blocks \"F\" , # Formatting \"FBT\" , # False positives and benign issues \"I\" , # Imports \"ICN\" , # Inconsistent naming \"ISC\" , # Inconsistent spacing \"N\" , # Naming conventions \"PLC\" , # Potential logical errors \"PLE\" , # Potential library errors \"PLR\" , # Potential runtime errors \"PLW\" , # Potential performance warnings \"Q\" , # Quality of life suggestions \"RUF\" , # Ruff-specific checks \"TID\" , # Type checker issues \"UP\" , # Unused code \"W\" , # Warnings \"YTT\" , # Yield type hints ] ignore = [ \"FBT003\" , # Ignore a specific false positive or benign issue (boolean-positional-value-in-call) ] [tool.mypy] files = [ \"your_modules\" , \"tests\" ] no_implicit_optional = true check_untyped_defs = true","title":"Code Style"},{"location":"tutorials/packaging/code_style/#code-style","text":"A consistent code style makes code easier to read and understand. The main benefit of increased legibility and beautiful code is saving developer time. A consistent code style allows anyone to quickly familiarize themselves with a codebase even if it is being worked on by many people. A recommended read is the clean code python . There are many tools/guidelines which can be used to for code style in python so a common question is which one to use. The reality is that most of the tools/guidelines are fairly good and have generally useful default behavior. In practice, for most tools, warnings end up being ignored or specific linting options are turned off because they are too restrictive. If a developer of a repository disagrees with an option, then they will just remove it entirely so that it does not bother them anymore. When using a set of guidelines which does not have a tool associated with it, rules are often forgotten or implemented differently by different developers.","title":"Code Style"},{"location":"tutorials/packaging/code_style/#goals","text":"Due to the reasons above, the goal of a styling specification would be to have the following properties: Automation : The code style must have a tool. It must have automated formatting so that users do not need to worry about styling the code manually. Minimal Configuration : Minimize options so that users do not need to make decisions about style. More decisions leads to more configuration and more inconsistencies in code style. Customizable Rules : The tool should allow leads to set the type of rules going to be enforced and rules can be ignored.","title":"Goals"},{"location":"tutorials/packaging/code_style/#example-tools","text":"The following are the most commonly used tools for python code quality and ensure consistency.","title":"Example Tools"},{"location":"tutorials/packaging/code_style/#linters","text":"Linters are usually broken into two types: Logical and Stylistic linters. Logical Linter: Code Errors Code with potentially unintended results Dangrous code patterns Stylistic Liners: Code not conforming to defined conventions. Below are some of the popular tools ruff ; A modern(Rust) linter that combines the best features of flake8, pylint, and isort. It's highly customizable and offers fast performance. flake8 : the wrapper which verifies PEP-8 , pyflakes, and circular complexity \u201c. It has a low rate of false positives.","title":"Linters"},{"location":"tutorials/packaging/code_style/#formatters","text":"Formatters will format the actual python file based on rules. ruff A drop-in replacement for black , but much faster. black An automated code formatter with no configuration options. A huge benefit of black is that unlike the previously mentioned tools this does not have formatting options. Black does not strictly follow PEP-8 guidelines but is generally compliant. It is also important to understand that black is only an opinionated formatter , it does not check all of the styles problem that other linters do. isort Format imports order. Sort imports alphabetically, and automatically separated into sections and by type. In a very controlled development environment, each may be useful tools by just enabling the default behavior and forbidding custom options.","title":"Formatters"},{"location":"tutorials/packaging/code_style/#recommendation","text":"For a comprehensive and efficient code style and linting solution, we highly recommend using ruff . It offers a powerful combination of features and performance benefits: Fast Performance : Leverages Rust for speed, making it ideal for large codebases. Comprehensive Linting : Checks for a wide range of style issues, potential bugs, and performance problems. Customization : Easily tailor the linting rules to your specific needs. Compatibility : Seamlessly integrates with popular tools like black and isort . For formatting, black remains a solid choice for its simplicity and strict adherence to a specific style. However, for those seeking a faster alternative, ruff can also be used as a drop-in replacement. To ensure type safety, mypy should be used to statically type-check your code.","title":"Recommendation"},{"location":"tutorials/packaging/code_style/#style-guides","text":"Beyond automated checks and warnings for code, style guides provide good recommendations for how you should write code. Three style guides in particular provide almost universally useful advice: PEP-8 : The official python style guide provides the basis for almost all formatters and other style guides. This provides the broadest and most foundational rules for writing legible python code. Google Style Guide : This has a lot of useful recommendations, however many of their guidelines were chosen to maximize compatibility between python 2 and python 3. Since python 2 is officially deprecated, all of the compatibility guidelines should be ignored. For these reasons the main Hitchhiker's Guide to Python : This provides its own style guide which is universally regarded as good practice. Though none of these style guides can programmatically format code for you, they are essential reference for how to write clean, simple python code.","title":"Style Guides"},{"location":"tutorials/packaging/code_style/#type-annotations","text":"Type annotations of some kind are recommended but can be implemented in different ways. Some form of type annotation is always recommended so that code usage is less ambiguous. The recommended strategies for annotating the types of your code:","title":"Type Annotations"},{"location":"tutorials/packaging/code_style/#pep-484-style-type-annotations","text":"This places the type annotation directly in code. Pros: Code can be checked using automated code checkers (See: mypy & pyre-check ) Cons: Tends to make function declarations verbose Can be difficult to document data structures (Requires importing typing ) def subtract ( minuend : int , subtrahend : int ) -> int : \"\"\" Subtract the subtrahend from the minuend. Args: minuend: The basis to subtract from. subtrahend: The value to subtract. Result: difference: The difference between the numbers. \"\"\" return minuend - subtrahend","title":"PEP-484 Style type annotations"},{"location":"tutorials/packaging/code_style/#variable-naming","text":"In the style guides mentioned above, the naming conventions are primarily concerned with the case used (snake_case vs CamelCase, etc.) However, this section is in regards to the actual words which should be used in a name. Naming is one of the hardest problems in programming. Good naming can drastically increase the readability of your code. To name things well, variables should have the following properties: Descriptive Unambiguous Should not contain the type. This is what type annotations are for Should be short if possible. Long names make code more difficult to read Abbrivations for names is ok if it is well-known but should be reframed. ie. n , sz , cnt , idx , dt , ts , env , cfg , ctx etc.","title":"Variable Naming"},{"location":"tutorials/packaging/code_style/#dictionaries","text":"Dictionaries should have information about both the key and value in the name. Acceptable Forms: {singular-key}_{plural-values} (Preferred) Somewhat ambiguous, but succinct {key}_to_{value} Somewhat verbose {value}_by_{key} Somewhat verbose, less direct than {key}_to_{value} {key}_{value}_lookup Somewhat verbose, but describes how it should be used Each of the forms can be more appropriate in different scenarios where the meaning is more clear in context. Use word_lengths = { \"hello\" : 5 , \"world\" : 5 } index_to_word = { 1 : \"hello\" , 2 : \"world\" } word_by_index = { 1 : \"hello\" , 2 : \"world\" } index_word_lookup = { 1 : \"hello\" , 2 : \"world\" } Avoid words = { 1 : \"hello\" , 2 : \"world\" } # Ambiguous, No key information word_lookup = { 1 : \"hello\" , 2 : \"world\" } # No key information word_dict = { 1 : \"hello\" , 2 : \"world\" } # Type included in name In some cases there are well-understood mappings that are meant to be iterated over. In these cases it makes sense to just use the pluralized version of the word and name the variable after the contents. Anything that is meant to be iterated over should be pluralized. Exception headers = { \"Content-Type\" : \"application/json\" } cookies = { \"tz\" : \"America/Los_Angeles\" } hyperparameters = { \"min_samples_leaf\" : 50 } gunicorn_options = { \"workers\" : 8 } # Gunicorn uses `options` Avoid header_name_to_value = { \"Content-Type\" : \"application/json\" } # Too verbose cookie_name_to_value = { \"tz\" : \"America/Los_Angeles\" } # Too verbose hyperparameter_name_to_value = { \"min_samples_leaf\" : 50 } # Too verbose In other cases, there are libraries that have predefined names for their arguments that do not follow the conventions above. In this case it is acceptable to follow their conventions when interacting with their code. This makes the code less ambiguous because the library name and the application-code variable name match. Exception feed_dict = { \"name\" : tensor } # TensorFlow uses this name so it is acceptable","title":"Dictionaries"},{"location":"tutorials/packaging/code_style/#listsseriesarrayssets","text":"Collections (non-dictionary) should always be the plural version of whatever is contained within. In the case where the value type is ambiguous, try to name the collection so it is possible to determine what is contained within. Use `python zip_codes = [92127, 12345] names = {\"Johnny\", \"Lisa\", \"Mark\", \"Denny\"} column_names = [\"zip_code\", \"wages\"] # `names` suffix indicates string value Avoid zip_list = [ 92127 , 12345 ] # Do not include type list = [ 92127 , 12345 ] # Shadows built-in list, Ambiguous items = [ 92127 , 12345 ] # Ambiguous columns = [ \"zip_code\" , \"wages\" ] # Value type is ambiguous","title":"Lists/Series/Arrays/Sets"},{"location":"tutorials/packaging/code_style/#integersfloats","text":"When possible, number names should indicate the how the value should be used. Contrary to the rules regarding collections, there are a many well-understood values that indicate a number which should be unambiguous. Use limit = 10 threshold = 0.7 size = 10 # Potentially ambiguous index = 10 # Potentially ambiguous count = 10 # Potentially ambiguous n_items = 10 # `n` prefix always indicates a number, preferred `num_items` min_items = 10 # `min` prefix always indicates a number max_items = 10 # `max` prefix always indicates a number buy_price = 10 # Domain specific words always indicate a number Avoid items = 10 # Ambiguous, could indicate a collection n = 10 # Not Descriptive n_value = 10 # `value` suffix is not descriptive n_quantity = 10 # `quantity` suffix is not descriptive","title":"Integers/Floats"},{"location":"tutorials/packaging/code_style/#strings","text":"When possible, string names should indicate how the value should be used. The naming conventions of strings are similar to the conventions of numbers. There are many well-understood names that could indicate a string type. Common Forms: {descriptor}_key May indicate lookup key {descriptor}_name May indicate lookup key Use environment_name = 'cdev' environment_key = 'cdev' environment = 'cdev' # Well-known string value, Potentially ambiguous env = 'cdev' # Well-known abbreviation, Potentially ambiguous Avoid value = 'cdev' # Ambiguous config = 'cdev' # Ambiguous could indicate an object","title":"Strings"},{"location":"tutorials/packaging/code_style/#classes","text":"Class instances should always be named after the class itself. For the purposes of describing the forms which a variable should be named the following class name will be used DescriptionNoun . Acceptable Forms: {description}_{noun} (Preferred) CamelCase to snake_case conversion {noun} More succinct, potentially ambiguous Use class ExampleContext : pass example_context = ExampleContext () context = ExampleContext ()","title":"Classes"},{"location":"tutorials/packaging/code_style/#functions","text":"Function should always be named in lower case and separated with underscore ( _ ). The words that you use to name your function should clearly describe the function\u2019s intent (what the function does). All functions should clearly indicates the input and output variable types. When returning from function avoid generic container types like List or Dict , use additional hints if you must return such types, ie. List[int] or Dict[str, int] . Acceptable Forms: def {verb}_{intent}({nouns}:T) -> U: (Preferred) def {verb}({noun}:T}) -> U: More succinct, potentially ambiguous Use def sum ( iterable : Iterable ) -> Number : def remove_underscore ( s : str ) -> str : Avoid def foo ( a ): def process ( parameters : Dict ) -> Dict : def calculate ( results : Dict [ Any , Dict ]) -> Dict :","title":"Functions"},{"location":"tutorials/packaging/code_style/#ruff-setup","text":"It is recommend to setup ruff in your pyproject.toml file. [tool.ruff] line-length = 88 target-version = \"py312\" [tool.ruff.lint] select = [ \"A\" , # Assignment expressions \"ARG\" , # Argument-related issues \"B\" , # Builtin usage \"C\" , # Complexity \"COM\" , # Comments \"DTZ\" , # Datetime and time zone issues \"E\" , # Pythonic style guide violations \"EM\" , # Empty blocks \"F\" , # Formatting \"FBT\" , # False positives and benign issues \"I\" , # Imports \"ICN\" , # Inconsistent naming \"ISC\" , # Inconsistent spacing \"N\" , # Naming conventions \"PLC\" , # Potential logical errors \"PLE\" , # Potential library errors \"PLR\" , # Potential runtime errors \"PLW\" , # Potential performance warnings \"Q\" , # Quality of life suggestions \"RUF\" , # Ruff-specific checks \"TID\" , # Type checker issues \"UP\" , # Unused code \"W\" , # Warnings \"YTT\" , # Yield type hints ] ignore = [ \"FBT003\" , # Ignore a specific false positive or benign issue (boolean-positional-value-in-call) ] [tool.mypy] files = [ \"your_modules\" , \"tests\" ] no_implicit_optional = true check_untyped_defs = true","title":"Ruff Setup"},{"location":"tutorials/packaging/logging/","text":"Summary Use the Loguru loguru library if possible. Use the standard python logging library otherwise. Expose single logger accessible at package level. Expose logging configuration method. Avoid multiple loggers. Avoid using print . Avoid LOG ANY SENSITIVE INFORMATION. Logging \u00b6 With good program architecture debugging is a breeze, because bugs will be where they should be. -- David May use a single logger. use lazy logging option if supported instead of checking if logger is enabled for a certain level. consistent naming. The recommended name for your logger is logger . Use the Correct Levels When Logging. Include a Timestamp for each log entry. Adopt the ISO-8601 Format for Timestamps. DO NOT create new methods for handling logs. Use the correct levels when logging \u00b6 DEBUG: You should use this level for debugging purposes in development. INFO: You should use this level when something interesting\u2014but expected\u2014happens (e.g., a user sends a new document of certain type to our application). WARNING: You should use this level when something unexpected or unusual happens. It\u2019s not an error, but you should pay attention to it. ERROR: This level is for things that go wrong but are usually recoverable (e.g., internal exceptions you can handle or APIs returning error results). CRITICAL: You should use this level in a doomsday scenario. The application is unusable. At this level, someone should be woken up at 2 a.m. DON'T REINVNET THE WHEEL \u00b6 Print statement is so easy but it comes with a price. You rarely need to subclass the logging module. Most of the time, you can achieve what you need through Structured logging or creating a new Record for the logger or create a new Handler to handle the record. When to log \u00b6 A rule of thumb when it comes to wehn to log is to think of logs as a story. If you are trying to tell a story, you should have a beginning, middle and end section. Begining of an operation. (e.g. Preparing connection to a service) Middle of an operation. (e.g. update relevant progress on download/upload files, etc.) End of an operation. (e.g. conclude an operation is either succeeded or failed.) What to log \u00b6 Logs are stories, what to log usually boils down to one of the two themes (Auditing or Diagnostic purpose). If I read this log, I know what is going on internally with the system? or If I read this log, I know what I need to do to next. A word of advice is knowning what you are logging and not log anything senstive. Assuming what you log is a public record all the time. Loguru common configuration parameters \u00b6 sink\uff1a You can pass in a file object \uff08file-like object\uff09, Or a str String or pathlib.Path object , Or a way \uff08coroutine function\uff09, or logging Modular Handler\uff08logging.Handler\uff09. level (int or str, optional) \uff1a The lowest severity level that recorded messages should be sent to the receiver . format (str or callable, optional) \uff1a Format module , Before sending it to the receiver , Use templates to format recorded messages . filter (callable, str or dict, optional) \uff1a Used to determine whether each recorded message should be sent to the receiver . colorize (bool, optional) : \u2013 Whether the color tags contained in the formatted message should be converted to Ansi Code , Or otherwise . If None, According to whether the sink is TTY Make a choice automatically . serialize (bool, optional) \uff1a Before sending it to the receiver , Whether the recorded message and its record should first be converted to JSON character string . backtrace (bool, optional) \uff1a Whether the formatted exception trace should be extended up , Beyond capture point , To display the full stack trace of the build error . diagnose (bool, optional) \uff1a Whether exception tracking should display variable values to simplify debugging . In production , This should be set to \u201cFalse\u201d, To avoid leaking sensitive data . enqueue (bool, optional) \uff1a Whether the message to be recorded should pass through the multiprocess secure queue before reaching the receiver . When logging to a file through multiple processes , This is very useful . This also has the advantage of making log calls non blocking . catch (bool, optional) \uff1a Whether the errors that occur when the receiver processes log messages should be automatically captured . If True The exception message is displayed on the sys.stderr. however , The exception does not propagate to the caller , To prevent the application from crashing . Loguru formatting keys \u00b6 formatting template properties as follows \uff1a Key Description elapsed The time difference from the beginning of the program exception Formatting exception ( If there is ), Otherwise ' None ' extra User bound property Dictionary ( See bind()) file The file that makes the logging call function Functions for logging calls level Used to record the severity of the message line Line number in source code message Recorded messages ( It's not formatted yet ) module Used to record the severity of the message name Logging calls name process The name of the process making the logging call thread The name of the thread making the logging call time The perceived local time when a log call is made Recommendated Loguru formatting string \u00b6 from loguru import logger , _defaults as loguru_defaults def log_format ( record ): if 'tid' in record [ \"extra\" ]: return ( \"[{time:YYYY-MM-DD HH:mm:ss.SSS}] | \" \"[ {extra[tid]} ] | \" \"[ {level: <8} ] | \" \" {process} - {thread} | \" \" {name} : {function: <15} : {line} | \" \"- {message} | \" \" {extra} \" ) else : return loguru_defaults . LOGURU_FORMAT + ' \\n ' logger . add ( sys . stdout , format = log_format ) config = { \"handlers\" : [ { \"sink\" : sys . stderr , \"format\" : log_format , \"backtrace\" : False , \"diagnose\" : True , \"encoding\" : \"utf-8\" , 'level' : 'DEBUG' , }, ], \"extra\" : { \"version\" : \"GITHASH\" }, } logger . configure ( ** config ) with logger . contextualize ( tid = current_tid ): logger . info ( ... )","title":"Logging"},{"location":"tutorials/packaging/logging/#logging","text":"With good program architecture debugging is a breeze, because bugs will be where they should be. -- David May use a single logger. use lazy logging option if supported instead of checking if logger is enabled for a certain level. consistent naming. The recommended name for your logger is logger . Use the Correct Levels When Logging. Include a Timestamp for each log entry. Adopt the ISO-8601 Format for Timestamps. DO NOT create new methods for handling logs.","title":"Logging"},{"location":"tutorials/packaging/logging/#use-the-correct-levels-when-logging","text":"DEBUG: You should use this level for debugging purposes in development. INFO: You should use this level when something interesting\u2014but expected\u2014happens (e.g., a user sends a new document of certain type to our application). WARNING: You should use this level when something unexpected or unusual happens. It\u2019s not an error, but you should pay attention to it. ERROR: This level is for things that go wrong but are usually recoverable (e.g., internal exceptions you can handle or APIs returning error results). CRITICAL: You should use this level in a doomsday scenario. The application is unusable. At this level, someone should be woken up at 2 a.m.","title":"Use the correct levels when logging"},{"location":"tutorials/packaging/logging/#dont-reinvnet-the-wheel","text":"Print statement is so easy but it comes with a price. You rarely need to subclass the logging module. Most of the time, you can achieve what you need through Structured logging or creating a new Record for the logger or create a new Handler to handle the record.","title":"DON'T REINVNET THE WHEEL"},{"location":"tutorials/packaging/logging/#when-to-log","text":"A rule of thumb when it comes to wehn to log is to think of logs as a story. If you are trying to tell a story, you should have a beginning, middle and end section. Begining of an operation. (e.g. Preparing connection to a service) Middle of an operation. (e.g. update relevant progress on download/upload files, etc.) End of an operation. (e.g. conclude an operation is either succeeded or failed.)","title":"When to log"},{"location":"tutorials/packaging/logging/#what-to-log","text":"Logs are stories, what to log usually boils down to one of the two themes (Auditing or Diagnostic purpose). If I read this log, I know what is going on internally with the system? or If I read this log, I know what I need to do to next. A word of advice is knowning what you are logging and not log anything senstive. Assuming what you log is a public record all the time.","title":"What to log"},{"location":"tutorials/packaging/logging/#loguru-common-configuration-parameters","text":"sink\uff1a You can pass in a file object \uff08file-like object\uff09, Or a str String or pathlib.Path object , Or a way \uff08coroutine function\uff09, or logging Modular Handler\uff08logging.Handler\uff09. level (int or str, optional) \uff1a The lowest severity level that recorded messages should be sent to the receiver . format (str or callable, optional) \uff1a Format module , Before sending it to the receiver , Use templates to format recorded messages . filter (callable, str or dict, optional) \uff1a Used to determine whether each recorded message should be sent to the receiver . colorize (bool, optional) : \u2013 Whether the color tags contained in the formatted message should be converted to Ansi Code , Or otherwise . If None, According to whether the sink is TTY Make a choice automatically . serialize (bool, optional) \uff1a Before sending it to the receiver , Whether the recorded message and its record should first be converted to JSON character string . backtrace (bool, optional) \uff1a Whether the formatted exception trace should be extended up , Beyond capture point , To display the full stack trace of the build error . diagnose (bool, optional) \uff1a Whether exception tracking should display variable values to simplify debugging . In production , This should be set to \u201cFalse\u201d, To avoid leaking sensitive data . enqueue (bool, optional) \uff1a Whether the message to be recorded should pass through the multiprocess secure queue before reaching the receiver . When logging to a file through multiple processes , This is very useful . This also has the advantage of making log calls non blocking . catch (bool, optional) \uff1a Whether the errors that occur when the receiver processes log messages should be automatically captured . If True The exception message is displayed on the sys.stderr. however , The exception does not propagate to the caller , To prevent the application from crashing .","title":"Loguru common configuration parameters"},{"location":"tutorials/packaging/logging/#loguru-formatting-keys","text":"formatting template properties as follows \uff1a Key Description elapsed The time difference from the beginning of the program exception Formatting exception ( If there is ), Otherwise ' None ' extra User bound property Dictionary ( See bind()) file The file that makes the logging call function Functions for logging calls level Used to record the severity of the message line Line number in source code message Recorded messages ( It's not formatted yet ) module Used to record the severity of the message name Logging calls name process The name of the process making the logging call thread The name of the thread making the logging call time The perceived local time when a log call is made","title":"Loguru formatting keys"},{"location":"tutorials/packaging/logging/#recommendated-loguru-formatting-string","text":"from loguru import logger , _defaults as loguru_defaults def log_format ( record ): if 'tid' in record [ \"extra\" ]: return ( \"[{time:YYYY-MM-DD HH:mm:ss.SSS}] | \" \"[ {extra[tid]} ] | \" \"[ {level: <8} ] | \" \" {process} - {thread} | \" \" {name} : {function: <15} : {line} | \" \"- {message} | \" \" {extra} \" ) else : return loguru_defaults . LOGURU_FORMAT + ' \\n ' logger . add ( sys . stdout , format = log_format ) config = { \"handlers\" : [ { \"sink\" : sys . stderr , \"format\" : log_format , \"backtrace\" : False , \"diagnose\" : True , \"encoding\" : \"utf-8\" , 'level' : 'DEBUG' , }, ], \"extra\" : { \"version\" : \"GITHASH\" }, } logger . configure ( ** config ) with logger . contextualize ( tid = current_tid ): logger . info ( ... )","title":"Recommendated Loguru formatting string"},{"location":"tutorials/packaging/makefile/","text":"Summary Assume make is executed inside the virtual environment. Wrap all virtual environment class inside make . Avoid using calls that leaks the environment to system/global. Tips \u00b6 The basics \u00b6 Using Makefile , everything is based on the dependencies and timestamps. If a dependnecy's timestamp is more recent than the target, then the rule is executed. Take a look of this basic example. .venv/bin/python : python3 -m venv .venv .venv/.install.stamp : . venv / bin / python pyproject . toml .venv/bin/python -m poetry install touch .venv/.install.stamp test : . venv /. install . stamp .venv/bin/python -m pytest tests/ If the pyproject.toml file is changed when running make test , it will test the rule .venv/.install.stamp and detects there is a new timestamp, which cause this rule to be executed. The dependency DAGs are traversed recursively. Use variables \u00b6 VENV := .venv INSTALL_STAMP := $( VENV ) /.install.stamp PYTHON := $( VENV ) /bin/python $(PYTHON) : python3 -m venv $( VENV ) $(INSTALL_STAMP) : $( PYTHON ) pyproject . toml $( PYTHON ) -m poetry install touch $( INSTALL_STAMP ) test : $( INSTALL_STAMP ) $( PYTHON ) -m pytest ./tests/ Environment variables with defaults \u00b6 Instead of hardcoding the name of your virtualenv folder, you can read it from the current shell environment and use a default value: VENV := $( shell echo $$ { VIRTUAL_ENV-.venv } ) In Shell scripts, ${VAR_NAME-val} will first try to get $VAR_NAME from the environment, if it can it will use the value that is set by the enviornment, otherwise, it will take the default val . You can pass this variable from the commandline as well LOG_FORMAT = json make test # or use export to have the variable in env: export LOG_FORMAT = json && make test Check if a command is avaliable \u00b6 It is nice to check if certain command available before blindly execute them in run-time to discover it fails. PY3 := $( shell command -v python3 2 > /dev/null ) $(PYTHON) : @if [ -z $( PY3 ) ] ; then echo \"python3 could not be found. See https://docs.python.org/3/\" ; exit 2 ; fi python3 -m venv $( VENV ) List avaiable targets \u00b6 when running make , by default it tries to execute all target. This might not be intended. We can customize this by doing .DEFAULT_GOAL := help help : @echo \"Please use 'make <target>' where <target> is one of\" @echo \"\" @echo \" install install packages and prepare environment\" @echo \" format reformat code\" @echo \" lint run the code linters\" @echo \" test run all the tests\" @echo \" clean remove *.pyc files and __pycache__ directory\" @echo \"\" @echo \"Check the Makefile to know exactly what each target is doing.\" with .DEFAULT_GOAL it will be executing help instead of the all . with a little shell script magic you can don't have to maintain the help message manually. .DEFAULT_GOAL := help help : ## Display this help @awk 'BEGIN {FS = \":.*##\"; printf \"\\nUsage:\\n make \\033[36m\\033[0m\\n\\nTargets:\\n\"} /^[a-zA-Z_-]+:.*?##/ { printf \"\\033[36m%-10s\\033[0m %s\\n\", $$1, $$2 }' $( MAKEFILE_LIST ) deps : ## Check dependencies ... clean : ## Cleanup the project folders ... build : clean deps ## Build the project ... PHONY \u00b6 Make by default assumes the target of a rule is a file. If you have rules (tasks) that does not produce files on disk. (eg. make clean or make test ), then you can mark them as .PHONY . When a target is maked as PHONY , the targets are assumes to be never up-to-date and will always run when invoked. .PHONY : clean clean : rm -rf $( VENV ) .PHONY : test test : ... Example \u00b6 NAME := ${ PROJECT_NAME -myproject } INSTALL_STAMP := .install.stamp POETRY := $( shell command -v poetry 2 > /dev/null ) .DEFAULT_GOAL := help .PHONY : help help : @awk 'BEGIN {FS = \":.*##\"; printf \"\\nUsage:\\n make \\033[36m\\033[0m\\n\\nTargets:\\n\"} /^[a-zA-Z_-]+:.*?##/ { printf \"\\033[36m%-10s\\033[0m %s\\n\", $$1, $$2 }' $( MAKEFILE_LIST ) install : $( INSTALL_STAMP ) $(INSTALL_STAMP) : pyproject . toml poetry . lock @if [ -z $( POETRY ) ] ; then echo \"Poetry could not be found. See https://python-poetry.org/docs/\" ; exit 2 ; fi $( POETRY ) install touch $( INSTALL_STAMP ) .PHONY : clean clean : find . -type d -name \"__pycache__\" | xargs rm -rf {} ; rm -rf $( INSTALL_STAMP ) .coverage .mypy_cache .PHONY : lint lint : $( INSTALL_STAMP ) $( POETRY ) run isort --profile = black --lines-after-imports = 2 --check-only ./tests/ $( NAME ) $( POETRY ) run black --check ./tests/ $( NAME ) --diff $( POETRY ) run flake8 --ignore = W503,E501 ./tests/ $( NAME ) $( POETRY ) run mypy ./tests/ $( NAME ) --ignore-missing-imports $( POETRY ) run bandit -r $( NAME ) -s B608 .PHONY : format format : $( INSTALL_STAMP ) $( POETRY ) run isort --profile = black --lines-after-imports = 2 ./tests/ $( NAME ) $( POETRY ) run black ./tests/ $( NAME ) .PHONY : test test : $( INSTALL_STAMP ) $( POETRY ) run pytest ./tests/ --cov-report term-missing --cov-fail-under 100 --cov $( NAME )","title":"Makefile"},{"location":"tutorials/packaging/makefile/#tips","text":"","title":"Tips"},{"location":"tutorials/packaging/makefile/#the-basics","text":"Using Makefile , everything is based on the dependencies and timestamps. If a dependnecy's timestamp is more recent than the target, then the rule is executed. Take a look of this basic example. .venv/bin/python : python3 -m venv .venv .venv/.install.stamp : . venv / bin / python pyproject . toml .venv/bin/python -m poetry install touch .venv/.install.stamp test : . venv /. install . stamp .venv/bin/python -m pytest tests/ If the pyproject.toml file is changed when running make test , it will test the rule .venv/.install.stamp and detects there is a new timestamp, which cause this rule to be executed. The dependency DAGs are traversed recursively.","title":"The basics"},{"location":"tutorials/packaging/makefile/#use-variables","text":"VENV := .venv INSTALL_STAMP := $( VENV ) /.install.stamp PYTHON := $( VENV ) /bin/python $(PYTHON) : python3 -m venv $( VENV ) $(INSTALL_STAMP) : $( PYTHON ) pyproject . toml $( PYTHON ) -m poetry install touch $( INSTALL_STAMP ) test : $( INSTALL_STAMP ) $( PYTHON ) -m pytest ./tests/","title":"Use variables"},{"location":"tutorials/packaging/makefile/#environment-variables-with-defaults","text":"Instead of hardcoding the name of your virtualenv folder, you can read it from the current shell environment and use a default value: VENV := $( shell echo $$ { VIRTUAL_ENV-.venv } ) In Shell scripts, ${VAR_NAME-val} will first try to get $VAR_NAME from the environment, if it can it will use the value that is set by the enviornment, otherwise, it will take the default val . You can pass this variable from the commandline as well LOG_FORMAT = json make test # or use export to have the variable in env: export LOG_FORMAT = json && make test","title":"Environment variables with defaults"},{"location":"tutorials/packaging/makefile/#check-if-a-command-is-avaliable","text":"It is nice to check if certain command available before blindly execute them in run-time to discover it fails. PY3 := $( shell command -v python3 2 > /dev/null ) $(PYTHON) : @if [ -z $( PY3 ) ] ; then echo \"python3 could not be found. See https://docs.python.org/3/\" ; exit 2 ; fi python3 -m venv $( VENV )","title":"Check if a command is avaliable"},{"location":"tutorials/packaging/makefile/#list-avaiable-targets","text":"when running make , by default it tries to execute all target. This might not be intended. We can customize this by doing .DEFAULT_GOAL := help help : @echo \"Please use 'make <target>' where <target> is one of\" @echo \"\" @echo \" install install packages and prepare environment\" @echo \" format reformat code\" @echo \" lint run the code linters\" @echo \" test run all the tests\" @echo \" clean remove *.pyc files and __pycache__ directory\" @echo \"\" @echo \"Check the Makefile to know exactly what each target is doing.\" with .DEFAULT_GOAL it will be executing help instead of the all . with a little shell script magic you can don't have to maintain the help message manually. .DEFAULT_GOAL := help help : ## Display this help @awk 'BEGIN {FS = \":.*##\"; printf \"\\nUsage:\\n make \\033[36m\\033[0m\\n\\nTargets:\\n\"} /^[a-zA-Z_-]+:.*?##/ { printf \"\\033[36m%-10s\\033[0m %s\\n\", $$1, $$2 }' $( MAKEFILE_LIST ) deps : ## Check dependencies ... clean : ## Cleanup the project folders ... build : clean deps ## Build the project ...","title":"List avaiable targets"},{"location":"tutorials/packaging/makefile/#phony","text":"Make by default assumes the target of a rule is a file. If you have rules (tasks) that does not produce files on disk. (eg. make clean or make test ), then you can mark them as .PHONY . When a target is maked as PHONY , the targets are assumes to be never up-to-date and will always run when invoked. .PHONY : clean clean : rm -rf $( VENV ) .PHONY : test test : ...","title":"PHONY"},{"location":"tutorials/packaging/makefile/#example","text":"NAME := ${ PROJECT_NAME -myproject } INSTALL_STAMP := .install.stamp POETRY := $( shell command -v poetry 2 > /dev/null ) .DEFAULT_GOAL := help .PHONY : help help : @awk 'BEGIN {FS = \":.*##\"; printf \"\\nUsage:\\n make \\033[36m\\033[0m\\n\\nTargets:\\n\"} /^[a-zA-Z_-]+:.*?##/ { printf \"\\033[36m%-10s\\033[0m %s\\n\", $$1, $$2 }' $( MAKEFILE_LIST ) install : $( INSTALL_STAMP ) $(INSTALL_STAMP) : pyproject . toml poetry . lock @if [ -z $( POETRY ) ] ; then echo \"Poetry could not be found. See https://python-poetry.org/docs/\" ; exit 2 ; fi $( POETRY ) install touch $( INSTALL_STAMP ) .PHONY : clean clean : find . -type d -name \"__pycache__\" | xargs rm -rf {} ; rm -rf $( INSTALL_STAMP ) .coverage .mypy_cache .PHONY : lint lint : $( INSTALL_STAMP ) $( POETRY ) run isort --profile = black --lines-after-imports = 2 --check-only ./tests/ $( NAME ) $( POETRY ) run black --check ./tests/ $( NAME ) --diff $( POETRY ) run flake8 --ignore = W503,E501 ./tests/ $( NAME ) $( POETRY ) run mypy ./tests/ $( NAME ) --ignore-missing-imports $( POETRY ) run bandit -r $( NAME ) -s B608 .PHONY : format format : $( INSTALL_STAMP ) $( POETRY ) run isort --profile = black --lines-after-imports = 2 ./tests/ $( NAME ) $( POETRY ) run black ./tests/ $( NAME ) .PHONY : test test : $( INSTALL_STAMP ) $( POETRY ) run pytest ./tests/ --cov-report term-missing --cov-fail-under 100 --cov $( NAME )","title":"Example"},{"location":"tutorials/packaging/structure/","text":"Summary Use pyproject.toml for project and dependency management Use poetry for packaging and releasing Use tox or nox for environment automation in testing Use pytest for testing framework Avoid deeply nested namespaces (e.g. Java-style) Structure \u00b6 for simple projects. . \u251c\u2500\u2500 app \u251c\u2500\u2500 tests \u2502 \u2514\u2500\u2500 unit_tests \u2502 \u2514\u2500\u2500 integration_tests \u251c\u2500\u2500 .flake8 \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 .pre-commit-config.yaml \u251c\u2500\u2500 .python-version \u251c\u2500\u2500 mypy.ini \u251c\u2500\u2500 pytest.ini \u251c\u2500\u2500 pyproject.toml \u251c\u2500\u2500 README.md \u2514\u2500\u2500 Makefile If your project is rather large, you may opt for the monorepo pattern . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 libs \u2502 \u251c\u2500\u2500 common-lib \u2502 \u251c\u2500\u2500 lib-one \u2502 \u2514\u2500\u2500 lib-one \u251c\u2500\u2500 poetry.lock \u251c\u2500\u2500 projects \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 project-one \u2502 \u2514\u2500\u2500 project-two \u2514\u2500\u2500 pyproject.toml under /projects Project code (Python modules) go here. Each project has its own dependencies. Each project is considered to have its own releasable artifact. under /libs Each lib has its own dependencies. Each lib can optionally depends on the common-lib if there one. project under /projects will use path import of the lib under /libs .","title":"Structure"},{"location":"tutorials/packaging/structure/#structure","text":"for simple projects. . \u251c\u2500\u2500 app \u251c\u2500\u2500 tests \u2502 \u2514\u2500\u2500 unit_tests \u2502 \u2514\u2500\u2500 integration_tests \u251c\u2500\u2500 .flake8 \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 .pre-commit-config.yaml \u251c\u2500\u2500 .python-version \u251c\u2500\u2500 mypy.ini \u251c\u2500\u2500 pytest.ini \u251c\u2500\u2500 pyproject.toml \u251c\u2500\u2500 README.md \u2514\u2500\u2500 Makefile If your project is rather large, you may opt for the monorepo pattern . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 libs \u2502 \u251c\u2500\u2500 common-lib \u2502 \u251c\u2500\u2500 lib-one \u2502 \u2514\u2500\u2500 lib-one \u251c\u2500\u2500 poetry.lock \u251c\u2500\u2500 projects \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 project-one \u2502 \u2514\u2500\u2500 project-two \u2514\u2500\u2500 pyproject.toml under /projects Project code (Python modules) go here. Each project has its own dependencies. Each project is considered to have its own releasable artifact. under /libs Each lib has its own dependencies. Each lib can optionally depends on the common-lib if there one. project under /projects will use path import of the lib under /libs .","title":"Structure"},{"location":"tutorials/packaging/testing/","text":"Summary Use tox or nox for automation. Use pytest for testing. Use pytest-mock for mocking. Use pytest markers to limit test set. Use mutmut for mutation testing. Use # pragma: no cover for lines doesn't need coverage. ie. config lines. Testing Best Practices \u00b6 This guide outlines recommended practices for writing effective tests in Python. Use tox or nox for automation: These tools help manage different environments and dependencies for your tests, simplifying the testing process. Use pytest for testing: Pytest is a popular and powerful testing framework that provides features like parametrization, fixtures, and various plugins for extended functionality. Use pytest-mock for mocking: Mocking allows you to isolate the code under test from its dependencies, making tests more reliable. Use Fixtures in conftest.py to setup your test and building the basic building blocks. Define reusable test setups in conftest.py using fixtures. Leverage different scopes (session, module, class, and function) for fixtures based on their purpose. Use parameterized tests to test different input use cases. Ensure a good code coverage. Preferred to have over 80% coverage. Mutation test can help you figure out if your unit test is good enough. Pytest have vast amount of plug-ins to help you make your tests better. Test Discovery \u00b6 Pytest will automatically discover tests matching patterns like test_*.py or *_test.py . Troubleshooting Tip: If tests are missing, use pytest --collect-only to verify that pytest detects all tests and fixtures correctly. Example of Using Pytest Plugins \u00b6 Pytest plugins can enhance your testing experience. # pyproject.toml [ pytest ] addopts = -- cov = your_package -- cov - report = term - missing Writing Effective Tests \u00b6 Fixtures for resuable setups \u00b6 Fixtures are how test setups (other helpers) are shared between tests. Can build on top of each other to model complext functionality. Customize functionality by overriding other fixtures . Can be parametrized. Define reusable setups in conftest.py using fixtures. Use different fixture scopes ( session , module , class , function ) based on purpose: Session: Set up once per session (e.g., database setup). Module: Set up once per module (e.g., module-level constants). Class: Set up once per test class (e.g., reusable states). Function: Set up for individual tests. Yield Fixture aka. Context Fixture \u00b6 This is a fixture that is created using the yield statement. import pytest @pytest . fixture def db_connection (): connection = setup_database_connection () yield connection teardown_database_connection ( connection ) The code above the yield is executed as setup for the fixture, while the code after the yield is executed as clean-up. The value yielded is the fixture value received by the user. Just like all Context-Managers, every call pushes the context on the stack, it follows the LIFO order. Fixture resolution \u00b6 Pytest uses an in-memory DAG (Directed Acyclic Graph) to figure out what is the order of the fixture a test needs. Each fixture that is required for execution is run once ; The value of the fixture is stored and use to compute the other dependent fixture. Execution \u00b6 When fixture code is executed, it follows the following rules. Session scoped fixtures are executed if they have not already been executed in this test run. Otherwise, the results of previous execution are used. Module scoped fixtures are executed if they have not already been executed as part of this test module in this test run. Otherwise, the results of the previous execution are used. Class scoped fixture are executed if they have not already been executed as part of this class in the test run. Otherwise, the results of previous execution are used. Function scoped fixtures are executed. Once all the fixtures are evaluated, the test function is called with the values for the fixtures filled in. Don't modify fixture values in other fixtures \u00b6 Pytest test cases are usually executed in parallel but fixtures are usually executed only once. When multiple fixtures may depend on the same upstream fixture. If any one of these modifies the upstream fixture\u2019s value, all others will also see the modified value; this will lead to unexpected behavior. If you must override certain values of the parent fixtures, you should make a deepcopy of the data. Avoid @pytest . fixture def engineer (): return { \"name\" : \"Alex\" , \"team\" : \"Intuit-AI\" , } @pytest . fixture def ds ( engineer ): engineer [ \"name\" ] = \"Joy\" return engineer def test_antipattern ( engineer , ds ): assert engineer == { \"name\" : \"Alex\" , \"team\" : \"Intuit-AI\" } assert ds == { \"name\" : \"Joy\" , \"team\" : \"Intuit-AI\" } In the above case, since ds modified the value, all of them will have the name Joy instead. Use @pytest . fixture def engineer (): return { \"name\" : \"Alex\" , \"team\" : \"Intuit-AI\" , } @pytest . fixture def ds ( engineer ): joy = deepcopy ( engineer ) # Create a deepcopy of the object joy [ \"name\" ] = \"Joy\" return joy def test_pattern ( engineer , ds ): assert engineer == { \"name\" : \"Alex\" , \"team\" : \"Intuit-AI\" } assert ds == { \"name\" : \"Joy\" , \"team\" : \"Intuit-AI\" } Test collection \u00b6 During test collection, every test module, test class, test function in the test-discovery is picked up. In parallel, every fixture also picked up by inspecting the conftest.py files as well as test modules. If you ever encountered a program during your test and zero test cases ran, it is usually some problem with this phase. Check your fixture and paramterized code to see if there is any mistakes there. Patterns \u00b6 The following patterns should be considered when developing tests. Fixtures \u00b6 You should places all your fixtures in various scopes in the correct locations. session scoped fixtures should be located in conftest.py at the package or root level. class scoped and module scoped fixture should be residing the the conftest.py at the same level as your module is defined. function scoped fixture should be in the same file as the actual test cases. Example of Parameterized Tests \u00b6 Parameterized tests allow you to run the same test with different inputs. import pytest @pytest . mark . parametrize ( \"input, expected\" , [ ( 1 , 2 ), ( 2 , 3 ), ( 3 , 4 ), ]) def test_increment ( input , expected ): assert input + 1 == expected Example of Using Mocking \u00b6 Using Mocks Effectively \u00b6 Mocking with pytest-mock: Mock dependencies using pytest-mock to ensure test isolation. This prevents external dependencies (e.g., APIs or databases) from impacting test outcomes. Use spy functions with pytest-mock when you need to monitor call behavior without replacing the function entirely. Eliminate flaky test due to \"mock leak\" when a test does not reset a patch. Reduce boilerplate code Simple example def test_example_function ( mocker ): mock_function = mocker . spy ( module , 'function_name' ) result = module . example_function () mock_function . assert_called_once () A more complex example # inside of test_mocker.py def test_fn (): return 42 class TestMockerFixtureSuite ( object ): \"\"\" Only use a class when you want to organize tests into a suite \"\"\" @pytest . mark . xfail ( strict = True , msg = \"We want this test to fail.\" ) def test_mocker ( self , mocker ): mocker . patch ( \"test_mocker.test_fn\" , return_value = 84 ) assert another_test_fn () == 84 assert False def test_mocker_follow_up ( self ): assert test_fn () == 42 @pytest . fixture def mock_fn ( self , mocker ): return mocker . patch ( \"test_mocker.test_fn\" , return_value = 84 ) def test_mocker_with_fixture ( self , mock_fn ): # notice this function depends on `mock_fn` to override fixture value assert another_test_fn () == 84 Prefer Response Frameworks over Manual Mocking \u00b6 Instead of manually creating HTTP response objects, use tools like responses (for requests library) or respx (for HTTPX) to mock HTTP requests. This improves accuracy in simulating real responses and helps avoid errors in test setup. Never manually create Response objects for tests; instead use the responses library (if using request library) to define what the expected raw API response is. If using any other http library, you can use HTTPretty instead or respx if needs HTTPX support. Assertions & Validations \u00b6 Parameterized Tests for Multiple Inputs \u00b6 Parameterization allows us to asserting the same behavior with various inputs and expected outputs. Make separate tests for distinct behaviors. Copy-pasting code in multiple tests increase boilerplate - use parametrize to reduce this. Never loop over test cases inside a test Parameterize heterogenous behaviors can lead to complex branching codes and bugs. import pytest @pytest . mark . parametrize ( \"input, expected\" , [ ( 1 , 2 ), ( 2 , 3 ), ( 3 , 4 ), ]) def test_increment ( input , expected ): assert input + 1 == expected Clear Error Handling in Test \u00b6 Separate test cases for valid and invalid cases. This avoids complex branching within tests and makes errors easier to trace. # util.py def divide ( a , b ): return a / b Avoid @pytest . mark . parametrize ( \"a, b, expected, is_error\" , [ ( 1 , 1 , 1 , False ), ( 42 , 1 , 42 , False ), ( 84 , 2 , 42 , False ), ( 42 , \"b\" , TypeError , True ), ( \"a\" , 42 , TypeError , True ), ( 42 , 0 , ZeroDivisionError , True ), ]) def test_divide_antipattern ( a , b , expected , is_error ): if is_error : with pytest . raises ( expected ): divide ( a , b ) else : assert divide ( a , b ) == expected Use @pytest . mark . parametrize ( \"a, b, expected\" , [ ( 1 , 1 , 1 ), ( 42 , 1 , 42 ), ( 84 , 2 , 42 ), ]) def test_divide_ok ( a , b , expected ): assert divide ( a , b ) == expected @pytest . mark . parametrize ( \"a, b, expected\" , [ ( 42 , \"b\" , TypeError ), ( \"a\" , 42 , TypeError ), ( 42 , 0 , ZeroDivisionError ), ]) def test_divide_error ( a , b , expected ): with pytest . raises ( expected ): divide ( a , b ) Prefer tmpdir over static locations \u00b6 Sometimes in testing you need a directory or files that you can work with to test out your code. Don't create files in static or predefined locations on your filesystem. You should use the tmpdir fixture and create the fiels on-the-fly to test. # util.py def process_file ( fp : IO ) -> List [ int ]: \"\"\"Toy function that returns an array of line lengths.\"\"\" return [ len ( l . strip ()) for l in fp . readlines ()] Avoid @pytest . mark . parametrize ( \"filename, expected\" , [ ( \"first.txt\" , [ 3 , 3 , 3 ]), ( \"second.txt\" , [ 5 , 5 ]), ]) def test_antipattern ( filename , expected ): with open ( \"resources/\" + filename ) as fp : assert process_file ( fp ) == expected Use @pytest . mark . parametrize ( \"contents, expected\" , [ ( \"foo \\n bar \\n baz\" , [ 3 , 3 , 3 ]), ( \"hello \\n world\" , [ 5 , 5 ]), ]) def test_pattern ( tmpdir , contents , expected ): # tmpdir is build-in to global pytest fixture tmp_file = tmpdir . join ( \"testfile.txt\" ) tmp_file . write ( contents ) with tmp_file . open () as fp : assert process_file ( fp ) == expected Coverage and Mutation Testing \u00b6 Code Coverage Goals \u00b6 Aim for over 80% code coverage, using pytest-cov to generate reports. Exclude configuration code and error handlers using # pragma: no cover. Mutation Testing \u00b6 Mutation testing with mutmut checks test effectiveness by introducing small changes (mutations) in the code. This ensures that tests detect when code behavior changes, offering more insight than code coverage alone. Plugins \u00b6 pytest-xdist : Use pytest-xdist for parallel test execution to speed up CI/CD pipelines. This can reduce overall test time, especially for larger test suites. pytest-randomly : run tests randomly (useful to catch weird bugs that runs sequentially). pytest-sugar : shows failures and errors instantly and shows a progress bar. pytest-icdiff : better diff when asserting error happens. (also pytest-clarity) pytest-html : get a html base report of the test. pytest-instafail : shows failures and errors instantly instead of waiting until the end of test session. pytest-timeout : terminate tests after a certain timeout. pytest-parallel : for parallel and concurrent testing. pytest-picked : Run the tests related to the unstaged files or the current branch (according to Git). pytest-benchmark : fixture for benchmarking code. pytest-cov : Code coverage. (MUST HAVE!) pytest-lazy-fixture : allows you to get values of fixture from paratmertized method. pytest-freezegun : Freeze time! pytest-leaks : Find resource leaks. pytest-deadfixtures : find out fixtures that are not used or duplicated. pytest-responses : fixture for requests library mocking.","title":"Testing"},{"location":"tutorials/packaging/testing/#testing-best-practices","text":"This guide outlines recommended practices for writing effective tests in Python. Use tox or nox for automation: These tools help manage different environments and dependencies for your tests, simplifying the testing process. Use pytest for testing: Pytest is a popular and powerful testing framework that provides features like parametrization, fixtures, and various plugins for extended functionality. Use pytest-mock for mocking: Mocking allows you to isolate the code under test from its dependencies, making tests more reliable. Use Fixtures in conftest.py to setup your test and building the basic building blocks. Define reusable test setups in conftest.py using fixtures. Leverage different scopes (session, module, class, and function) for fixtures based on their purpose. Use parameterized tests to test different input use cases. Ensure a good code coverage. Preferred to have over 80% coverage. Mutation test can help you figure out if your unit test is good enough. Pytest have vast amount of plug-ins to help you make your tests better.","title":"Testing Best Practices"},{"location":"tutorials/packaging/testing/#test-discovery","text":"Pytest will automatically discover tests matching patterns like test_*.py or *_test.py . Troubleshooting Tip: If tests are missing, use pytest --collect-only to verify that pytest detects all tests and fixtures correctly.","title":"Test Discovery"},{"location":"tutorials/packaging/testing/#example-of-using-pytest-plugins","text":"Pytest plugins can enhance your testing experience. # pyproject.toml [ pytest ] addopts = -- cov = your_package -- cov - report = term - missing","title":"Example of Using Pytest Plugins"},{"location":"tutorials/packaging/testing/#writing-effective-tests","text":"","title":"Writing Effective Tests"},{"location":"tutorials/packaging/testing/#fixtures-for-resuable-setups","text":"Fixtures are how test setups (other helpers) are shared between tests. Can build on top of each other to model complext functionality. Customize functionality by overriding other fixtures . Can be parametrized. Define reusable setups in conftest.py using fixtures. Use different fixture scopes ( session , module , class , function ) based on purpose: Session: Set up once per session (e.g., database setup). Module: Set up once per module (e.g., module-level constants). Class: Set up once per test class (e.g., reusable states). Function: Set up for individual tests.","title":"Fixtures for resuable setups"},{"location":"tutorials/packaging/testing/#yield-fixture-aka-context-fixture","text":"This is a fixture that is created using the yield statement. import pytest @pytest . fixture def db_connection (): connection = setup_database_connection () yield connection teardown_database_connection ( connection ) The code above the yield is executed as setup for the fixture, while the code after the yield is executed as clean-up. The value yielded is the fixture value received by the user. Just like all Context-Managers, every call pushes the context on the stack, it follows the LIFO order.","title":"Yield Fixture aka. Context Fixture"},{"location":"tutorials/packaging/testing/#fixture-resolution","text":"Pytest uses an in-memory DAG (Directed Acyclic Graph) to figure out what is the order of the fixture a test needs. Each fixture that is required for execution is run once ; The value of the fixture is stored and use to compute the other dependent fixture.","title":"Fixture resolution"},{"location":"tutorials/packaging/testing/#execution","text":"When fixture code is executed, it follows the following rules. Session scoped fixtures are executed if they have not already been executed in this test run. Otherwise, the results of previous execution are used. Module scoped fixtures are executed if they have not already been executed as part of this test module in this test run. Otherwise, the results of the previous execution are used. Class scoped fixture are executed if they have not already been executed as part of this class in the test run. Otherwise, the results of previous execution are used. Function scoped fixtures are executed. Once all the fixtures are evaluated, the test function is called with the values for the fixtures filled in.","title":"Execution"},{"location":"tutorials/packaging/testing/#dont-modify-fixture-values-in-other-fixtures","text":"Pytest test cases are usually executed in parallel but fixtures are usually executed only once. When multiple fixtures may depend on the same upstream fixture. If any one of these modifies the upstream fixture\u2019s value, all others will also see the modified value; this will lead to unexpected behavior. If you must override certain values of the parent fixtures, you should make a deepcopy of the data. Avoid @pytest . fixture def engineer (): return { \"name\" : \"Alex\" , \"team\" : \"Intuit-AI\" , } @pytest . fixture def ds ( engineer ): engineer [ \"name\" ] = \"Joy\" return engineer def test_antipattern ( engineer , ds ): assert engineer == { \"name\" : \"Alex\" , \"team\" : \"Intuit-AI\" } assert ds == { \"name\" : \"Joy\" , \"team\" : \"Intuit-AI\" } In the above case, since ds modified the value, all of them will have the name Joy instead. Use @pytest . fixture def engineer (): return { \"name\" : \"Alex\" , \"team\" : \"Intuit-AI\" , } @pytest . fixture def ds ( engineer ): joy = deepcopy ( engineer ) # Create a deepcopy of the object joy [ \"name\" ] = \"Joy\" return joy def test_pattern ( engineer , ds ): assert engineer == { \"name\" : \"Alex\" , \"team\" : \"Intuit-AI\" } assert ds == { \"name\" : \"Joy\" , \"team\" : \"Intuit-AI\" }","title":"Don't modify fixture values in other fixtures"},{"location":"tutorials/packaging/testing/#test-collection","text":"During test collection, every test module, test class, test function in the test-discovery is picked up. In parallel, every fixture also picked up by inspecting the conftest.py files as well as test modules. If you ever encountered a program during your test and zero test cases ran, it is usually some problem with this phase. Check your fixture and paramterized code to see if there is any mistakes there.","title":"Test collection"},{"location":"tutorials/packaging/testing/#patterns","text":"The following patterns should be considered when developing tests.","title":"Patterns"},{"location":"tutorials/packaging/testing/#fixtures","text":"You should places all your fixtures in various scopes in the correct locations. session scoped fixtures should be located in conftest.py at the package or root level. class scoped and module scoped fixture should be residing the the conftest.py at the same level as your module is defined. function scoped fixture should be in the same file as the actual test cases.","title":"Fixtures"},{"location":"tutorials/packaging/testing/#example-of-parameterized-tests","text":"Parameterized tests allow you to run the same test with different inputs. import pytest @pytest . mark . parametrize ( \"input, expected\" , [ ( 1 , 2 ), ( 2 , 3 ), ( 3 , 4 ), ]) def test_increment ( input , expected ): assert input + 1 == expected","title":"Example of Parameterized Tests"},{"location":"tutorials/packaging/testing/#example-of-using-mocking","text":"","title":"Example of Using Mocking"},{"location":"tutorials/packaging/testing/#using-mocks-effectively","text":"Mocking with pytest-mock: Mock dependencies using pytest-mock to ensure test isolation. This prevents external dependencies (e.g., APIs or databases) from impacting test outcomes. Use spy functions with pytest-mock when you need to monitor call behavior without replacing the function entirely. Eliminate flaky test due to \"mock leak\" when a test does not reset a patch. Reduce boilerplate code Simple example def test_example_function ( mocker ): mock_function = mocker . spy ( module , 'function_name' ) result = module . example_function () mock_function . assert_called_once () A more complex example # inside of test_mocker.py def test_fn (): return 42 class TestMockerFixtureSuite ( object ): \"\"\" Only use a class when you want to organize tests into a suite \"\"\" @pytest . mark . xfail ( strict = True , msg = \"We want this test to fail.\" ) def test_mocker ( self , mocker ): mocker . patch ( \"test_mocker.test_fn\" , return_value = 84 ) assert another_test_fn () == 84 assert False def test_mocker_follow_up ( self ): assert test_fn () == 42 @pytest . fixture def mock_fn ( self , mocker ): return mocker . patch ( \"test_mocker.test_fn\" , return_value = 84 ) def test_mocker_with_fixture ( self , mock_fn ): # notice this function depends on `mock_fn` to override fixture value assert another_test_fn () == 84","title":"Using Mocks Effectively"},{"location":"tutorials/packaging/testing/#prefer-response-frameworks-over-manual-mocking","text":"Instead of manually creating HTTP response objects, use tools like responses (for requests library) or respx (for HTTPX) to mock HTTP requests. This improves accuracy in simulating real responses and helps avoid errors in test setup. Never manually create Response objects for tests; instead use the responses library (if using request library) to define what the expected raw API response is. If using any other http library, you can use HTTPretty instead or respx if needs HTTPX support.","title":"Prefer Response Frameworks over Manual Mocking"},{"location":"tutorials/packaging/testing/#assertions-validations","text":"","title":"Assertions &amp; Validations"},{"location":"tutorials/packaging/testing/#parameterized-tests-for-multiple-inputs","text":"Parameterization allows us to asserting the same behavior with various inputs and expected outputs. Make separate tests for distinct behaviors. Copy-pasting code in multiple tests increase boilerplate - use parametrize to reduce this. Never loop over test cases inside a test Parameterize heterogenous behaviors can lead to complex branching codes and bugs. import pytest @pytest . mark . parametrize ( \"input, expected\" , [ ( 1 , 2 ), ( 2 , 3 ), ( 3 , 4 ), ]) def test_increment ( input , expected ): assert input + 1 == expected","title":"Parameterized Tests for Multiple Inputs"},{"location":"tutorials/packaging/testing/#clear-error-handling-in-test","text":"Separate test cases for valid and invalid cases. This avoids complex branching within tests and makes errors easier to trace. # util.py def divide ( a , b ): return a / b Avoid @pytest . mark . parametrize ( \"a, b, expected, is_error\" , [ ( 1 , 1 , 1 , False ), ( 42 , 1 , 42 , False ), ( 84 , 2 , 42 , False ), ( 42 , \"b\" , TypeError , True ), ( \"a\" , 42 , TypeError , True ), ( 42 , 0 , ZeroDivisionError , True ), ]) def test_divide_antipattern ( a , b , expected , is_error ): if is_error : with pytest . raises ( expected ): divide ( a , b ) else : assert divide ( a , b ) == expected Use @pytest . mark . parametrize ( \"a, b, expected\" , [ ( 1 , 1 , 1 ), ( 42 , 1 , 42 ), ( 84 , 2 , 42 ), ]) def test_divide_ok ( a , b , expected ): assert divide ( a , b ) == expected @pytest . mark . parametrize ( \"a, b, expected\" , [ ( 42 , \"b\" , TypeError ), ( \"a\" , 42 , TypeError ), ( 42 , 0 , ZeroDivisionError ), ]) def test_divide_error ( a , b , expected ): with pytest . raises ( expected ): divide ( a , b )","title":"Clear Error Handling in Test"},{"location":"tutorials/packaging/testing/#prefer-tmpdir-over-static-locations","text":"Sometimes in testing you need a directory or files that you can work with to test out your code. Don't create files in static or predefined locations on your filesystem. You should use the tmpdir fixture and create the fiels on-the-fly to test. # util.py def process_file ( fp : IO ) -> List [ int ]: \"\"\"Toy function that returns an array of line lengths.\"\"\" return [ len ( l . strip ()) for l in fp . readlines ()] Avoid @pytest . mark . parametrize ( \"filename, expected\" , [ ( \"first.txt\" , [ 3 , 3 , 3 ]), ( \"second.txt\" , [ 5 , 5 ]), ]) def test_antipattern ( filename , expected ): with open ( \"resources/\" + filename ) as fp : assert process_file ( fp ) == expected Use @pytest . mark . parametrize ( \"contents, expected\" , [ ( \"foo \\n bar \\n baz\" , [ 3 , 3 , 3 ]), ( \"hello \\n world\" , [ 5 , 5 ]), ]) def test_pattern ( tmpdir , contents , expected ): # tmpdir is build-in to global pytest fixture tmp_file = tmpdir . join ( \"testfile.txt\" ) tmp_file . write ( contents ) with tmp_file . open () as fp : assert process_file ( fp ) == expected","title":"Prefer tmpdir over static locations"},{"location":"tutorials/packaging/testing/#coverage-and-mutation-testing","text":"","title":"Coverage and Mutation Testing"},{"location":"tutorials/packaging/testing/#code-coverage-goals","text":"Aim for over 80% code coverage, using pytest-cov to generate reports. Exclude configuration code and error handlers using # pragma: no cover.","title":"Code Coverage Goals"},{"location":"tutorials/packaging/testing/#mutation-testing","text":"Mutation testing with mutmut checks test effectiveness by introducing small changes (mutations) in the code. This ensures that tests detect when code behavior changes, offering more insight than code coverage alone.","title":"Mutation Testing"},{"location":"tutorials/packaging/testing/#plugins","text":"pytest-xdist : Use pytest-xdist for parallel test execution to speed up CI/CD pipelines. This can reduce overall test time, especially for larger test suites. pytest-randomly : run tests randomly (useful to catch weird bugs that runs sequentially). pytest-sugar : shows failures and errors instantly and shows a progress bar. pytest-icdiff : better diff when asserting error happens. (also pytest-clarity) pytest-html : get a html base report of the test. pytest-instafail : shows failures and errors instantly instead of waiting until the end of test session. pytest-timeout : terminate tests after a certain timeout. pytest-parallel : for parallel and concurrent testing. pytest-picked : Run the tests related to the unstaged files or the current branch (according to Git). pytest-benchmark : fixture for benchmarking code. pytest-cov : Code coverage. (MUST HAVE!) pytest-lazy-fixture : allows you to get values of fixture from paratmertized method. pytest-freezegun : Freeze time! pytest-leaks : Find resource leaks. pytest-deadfixtures : find out fixtures that are not used or duplicated. pytest-responses : fixture for requests library mocking.","title":"Plugins"},{"location":"tutorials/performance/lookup_tables/","text":"Summary (Almost) Always use dictionaries Use a unique index Avoid code heavily oriented around pandas (especially indexing) Lookup Tables \u00b6 lookup tables or cache is what computer scientist calls a space-time trade off. What does that mean is you are using additional memory (RAM) to get faster access to data. Why dictionary is faster than list or tuple? \u00b6 When lookup for items, dictionaries have constant time complexity, O(1) while lists have linear time complexity, O(n). However, dictionaries have much bigger space complexity compare to lists eventhough both have O(n) space complexity in terms of big-O. I really want to use list \u00b6 before you do that, you should be aware of the cost of the methods. Method List Dict INSERT (Head or Tail) O(1) O(1)/O(N) INSERT (Random) O(N) O(1)/O(N) DELETE (Head or Tail) O(1) O(1)/O(N) DELETE (Random) O(N) O(1)/O(N) GET O(1) O(1)/O(N) Search (Sorted) O(log N) O(N)/O(N) Search (Random) O(N) O(N)/O(N) COPY O(N) O(N)/O(N) the amortized worst case scenario for dict is O(N). So becareful just because you have a dict, doesn't mean your get O(1) runtime behavior. when you do # This is `Search` O(N) if x in lst : print ( x ) # To get O(log N) in search for lst you can do. # assume lst is sorted index = bisect_left ( lst , x ) if index != len ( l ) and x == lst [ index ]: print ( x ) My key is not hashable \u00b6 If you just want check for existence of object you can do one of the two things here list_of_names = [ ... ] # this is a really large list # if you only want to check for existince a few times names_existence_lookup = set ( list_of_names ) for candidate in list_of_candidates : if candidate in names_existence_lookup : print ( f \" { candidate } on the short-list\" ) # can also create a dict with fixed value with the sentient that is special in python names_existence_lookup = { name : None for name in list_of_names } for candidate in list_of_candidates : if candidate in names_existence_lookup : print ( f \" { candidate } on the short-list\" ) In python list , set , and dict are not hashable objects. The reason these can't be hashed is because the content can change therefore, it is important to convert these into an immutable datatype before use for hashable objects.","title":"Lookup Tables"},{"location":"tutorials/performance/lookup_tables/#lookup-tables","text":"lookup tables or cache is what computer scientist calls a space-time trade off. What does that mean is you are using additional memory (RAM) to get faster access to data.","title":"Lookup Tables"},{"location":"tutorials/performance/lookup_tables/#why-dictionary-is-faster-than-list-or-tuple","text":"When lookup for items, dictionaries have constant time complexity, O(1) while lists have linear time complexity, O(n). However, dictionaries have much bigger space complexity compare to lists eventhough both have O(n) space complexity in terms of big-O.","title":"Why dictionary is faster than list or tuple?"},{"location":"tutorials/performance/lookup_tables/#i-really-want-to-use-list","text":"before you do that, you should be aware of the cost of the methods. Method List Dict INSERT (Head or Tail) O(1) O(1)/O(N) INSERT (Random) O(N) O(1)/O(N) DELETE (Head or Tail) O(1) O(1)/O(N) DELETE (Random) O(N) O(1)/O(N) GET O(1) O(1)/O(N) Search (Sorted) O(log N) O(N)/O(N) Search (Random) O(N) O(N)/O(N) COPY O(N) O(N)/O(N) the amortized worst case scenario for dict is O(N). So becareful just because you have a dict, doesn't mean your get O(1) runtime behavior. when you do # This is `Search` O(N) if x in lst : print ( x ) # To get O(log N) in search for lst you can do. # assume lst is sorted index = bisect_left ( lst , x ) if index != len ( l ) and x == lst [ index ]: print ( x )","title":"I really want to use list"},{"location":"tutorials/performance/lookup_tables/#my-key-is-not-hashable","text":"If you just want check for existence of object you can do one of the two things here list_of_names = [ ... ] # this is a really large list # if you only want to check for existince a few times names_existence_lookup = set ( list_of_names ) for candidate in list_of_candidates : if candidate in names_existence_lookup : print ( f \" { candidate } on the short-list\" ) # can also create a dict with fixed value with the sentient that is special in python names_existence_lookup = { name : None for name in list_of_names } for candidate in list_of_candidates : if candidate in names_existence_lookup : print ( f \" { candidate } on the short-list\" ) In python list , set , and dict are not hashable objects. The reason these can't be hashed is because the content can change therefore, it is important to convert these into an immutable datatype before use for hashable objects.","title":"My key is not hashable"},{"location":"tutorials/performance/multiprocessing/","text":"Summary Partition data into independent batches/chunks. Use shared memory for large data. Avoid naive element-wise processing. Avoid Using multithreading for CPU-bound tasks. Avoid Using multiprocessing for IO-bound tasks. Multi-Processing \u00b6 Due to the infamous python GIL, when you need more CPU power to crunch some data, multi-processing is the way to go. Python has a built-in multiprocessing library that make this feature avaiable out of the box. Multiprocessing library \u00b6 Let's take a look of the basic example of using multiprocessing in python. import multiprocessing def cpu_bound_processing ( data ): pass process = multiprocessing . Process ( target = cpu_bound_processing , args = ( x , y , z )) process . start () process . join () # any process finishes but not been joined becomes zombie process. with this example you can now utitlize more CPU cores to do CPU bound tasks. Shared Memory \u00b6 when the input size is large, it is in-efficient to make copies of the object to be passed to each sub-process. In this case, we should create one shared memory block of the original large object. Python have some capabilities build-in for this. You can use shared_memory module to do that. As mentioned above, when doing concurrent programming it is usually best to avoid using shared state as far as possible. This is particularly true when using multiple processes. However, if you really do need to use some shared data then multiprocessing provides a couple of ways of doing so. Numpy multiprocessing \u00b6 Often we uses NumPy arrays to represent image or video data. This is a great candidate for multiprocessing. Everything can be done with the build-in shared_memory module above, but the SharedArray library it is a wrapper around shm_open , mmap and friends function in C with python binding to makes this more user friendly when working with numpy. SharedArray has a few key functions: SharedArray.create(name, shape, dtype=float) creates a shared memory array SharedArray.attach(name) attaches a previously created shared memory array to a variable SharedArray.delete(name) deletes a shared memory array, however, existing attachments remain valid Example \u00b6 With SharedArray. import SharedArray def multiprocess_load_images ( num_workers = 12 ): files = os . listdir ( \"train_images\" ) # this assumes each file with 1400x1200 resolution with 3 (RBG) channels number_of_files = len ( files ) data = SharedArray . create ( 'data' , ( number_of_files , 1400 , 2100 , 3 )) worker_amount = int ( number_of_files / num_workers ) residual = number_of_files - ( num_works * worker_amount ) def load_images ( i , n ): # Chucking the potential inputs. this could be optmized to_load = files [ i : i + n ] for j , file in enumerate ( to_load ): data [ i + j ] = cv2 . imread ( \"train_images/\" + file ) processes = [] for worker_num in range ( num_workers ): run_worker_amount = worker_amount if worker_num == num_workers - 1 and residual != 0 : run_worker_amount += residual process = multiprocessing . Process ( target = load_images , args = ( worker_amount * worker_num , run_worker_amount )) processes . append ( process ) process . start () for process in processes : process . join () return data An example with SharedMemory directly from multiprocessing.shared_memory import SharedMemory from multiprocessing.managers import SharedMemoryManager from concurrent.futures import ProcessPoolExecutor , as_completed from multiprocessing import current_process , cpu_count , Process import numpy as np def work_with_shared_memory ( shm_name , shape , dtype ): print ( f 'With SharedMemory: { current_process () =} ' ) # Locate the shared memory by its name shm = SharedMemory ( shm_name ) # Create the np.recarray from the buffer of the shared memory np_array = np . recarray ( shape = shape , dtype = dtype , buf = shm . buf ) return np . nansum ( np_array . val ) # Some way to create that numpy array. np_array = ... shape , dtype = np_array . shape , np_array . dtype with SharedMemoryManager () as smm : # Create a shared memory of size np_arry.nbytes shm = smm . SharedMemory ( np_array . nbytes ) # Create a np.recarray using the buffer of shm shm_np_array = np . recarray ( shape = shape , dtype = dtype , buf = shm . buf ) # Copy the data into the shared memory np . copyto ( shm_np_array , np_array ) # Spawn some processes to do some work with ProcessPoolExecutor ( cpu_count ()) as exe : fs = [ exe . submit ( work_with_shared_memory , shm . name , shape , dtype ) for _ in range ( cpu_count ())] for _ in as_completed ( fs ): pass Huge arrays in numpy \u00b6 Python is notoriously known as a memory hogger and when you need to work with large amount of data it could result in out of memory error. One trick we can use in this case is memory mapped file in numpy. from concurrent.futures import ProcessPoolExecutor , as_completed from multiprocessing import Process import numpy as np worker , nrows , ncols = 10 , 1_000_000 , 100 def split_size_iter ( total_size : int , num_chunks : int ) -> Iterator [ Tuple [ int , int ]]: ... def print_matrix ( filename , worker_index_start , worker_index_end ): matrix = np . memmap ( filename , dtype = np . float32 , mode = 'r+' , shape = ( worker , nrows , ncols )) print matrix [ worker_index_start : worker_index_end ] def main (): matrix = np . memmap ( 'test.dat' , dtype = np . float32 , mode = 'w+' , shape = ( worker , nrows , ncols )) # some code to fill this matrix with ProcessPoolExecutor ( cworker ) as exe : fs = [ exe . submit ( print_matrix , 'test.dat' , start , end ) for start , end in split_size_iter ( worker , 4 )] for _ in as_completed ( fs ): pass Ray \u00b6 Ray is a powerful open source platform that makes it easy to write distributed Python programs and seamlessly scale them from your laptop to a cluster. Ray comes with support for the mulitprocessing.Pool API out of the box when importing ray.util.multiprocessing . Example \u00b6 import math import random import time def sample ( num_samples ): num_inside = 0 for _ in range ( num_samples ): x , y = random . uniform ( - 1 , 1 ), random . uniform ( - 1 , 1 ) if math . hypot ( x , y ) <= 1 : num_inside += 1 return num_inside def approximate_pi_distributed ( num_samples ): from ray.util.multiprocessing.pool import Pool # NOTE: Only the import statement is changed. pool = Pool () start = time . time () num_inside = 0 sample_batch_size = 100000 for result in pool . map ( sample , [ sample_batch_size for _ in range ( num_samples // sample_batch_size )]): num_inside += result print ( \"pi ~= {} \" . format (( 4 * num_inside ) / num_samples )) print ( \"Finished in: {:.2f} s\" . format ( time . time () - start )) Dask \u00b6 Dask is a Python parallel computing library geared towards scaling analytics and scientific computing workloads. Dask is composed of two parts: Dynamic task scheduling for optimized computation and Big Data collections such as like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments, which run on top of dynamic task schedulers. Example \u00b6 # Setup the cluster and client for Dask from dask.distributed import Client , LocalCluster n_workers = 3 cluster = LocalCluster ( n_workers = n_workers , threads_per_worker = 1 , memory_limit = '4GB' ) client = Client ( cluster ) client . wait_for_workers ( n_workers ) # Load dataset from dask_ml.datasets import make_classification as make_classification_dask X_dask , y_dask = make_classification_dask ( n_samples = 10000 , n_features = 10 , n_informative = 5 , n_redundant = 5 , random_state = 1 , chunks = 10 ) # Build a model import dask_lightgbm.core as dlgbm dmodel = dlgbm . LGBMClassifier ( n_estimators = 400 ) dmodel . fit ( X_dask , y_dask ) # make a single prediction row = [[ 2.56999479 , - 0.13019997 , 3.16075093 , - 4.35936352 , - 1.61271951 , - 1.39352057 , - 2.48924933 , - 1.93094078 , 3.26130366 , 2.05692145 ]] yhat = dmodel . predict ( da . from_array ( np . array ( row ))) print ( 'Prediction: %d ' % yhat [ 0 ]) y_pred = dmodel . predict ( X_dask , client = client ) acc_score = ( y_dask == y_pred ) . sum () / len ( y_dask ) acc_score = acc_score . compute () print ( acc_score )","title":"Multi-Processing"},{"location":"tutorials/performance/multiprocessing/#multi-processing","text":"Due to the infamous python GIL, when you need more CPU power to crunch some data, multi-processing is the way to go. Python has a built-in multiprocessing library that make this feature avaiable out of the box.","title":"Multi-Processing"},{"location":"tutorials/performance/multiprocessing/#multiprocessing-library","text":"Let's take a look of the basic example of using multiprocessing in python. import multiprocessing def cpu_bound_processing ( data ): pass process = multiprocessing . Process ( target = cpu_bound_processing , args = ( x , y , z )) process . start () process . join () # any process finishes but not been joined becomes zombie process. with this example you can now utitlize more CPU cores to do CPU bound tasks.","title":"Multiprocessing library"},{"location":"tutorials/performance/multiprocessing/#shared-memory","text":"when the input size is large, it is in-efficient to make copies of the object to be passed to each sub-process. In this case, we should create one shared memory block of the original large object. Python have some capabilities build-in for this. You can use shared_memory module to do that. As mentioned above, when doing concurrent programming it is usually best to avoid using shared state as far as possible. This is particularly true when using multiple processes. However, if you really do need to use some shared data then multiprocessing provides a couple of ways of doing so.","title":"Shared Memory"},{"location":"tutorials/performance/multiprocessing/#numpy-multiprocessing","text":"Often we uses NumPy arrays to represent image or video data. This is a great candidate for multiprocessing. Everything can be done with the build-in shared_memory module above, but the SharedArray library it is a wrapper around shm_open , mmap and friends function in C with python binding to makes this more user friendly when working with numpy. SharedArray has a few key functions: SharedArray.create(name, shape, dtype=float) creates a shared memory array SharedArray.attach(name) attaches a previously created shared memory array to a variable SharedArray.delete(name) deletes a shared memory array, however, existing attachments remain valid","title":"Numpy multiprocessing"},{"location":"tutorials/performance/multiprocessing/#example","text":"With SharedArray. import SharedArray def multiprocess_load_images ( num_workers = 12 ): files = os . listdir ( \"train_images\" ) # this assumes each file with 1400x1200 resolution with 3 (RBG) channels number_of_files = len ( files ) data = SharedArray . create ( 'data' , ( number_of_files , 1400 , 2100 , 3 )) worker_amount = int ( number_of_files / num_workers ) residual = number_of_files - ( num_works * worker_amount ) def load_images ( i , n ): # Chucking the potential inputs. this could be optmized to_load = files [ i : i + n ] for j , file in enumerate ( to_load ): data [ i + j ] = cv2 . imread ( \"train_images/\" + file ) processes = [] for worker_num in range ( num_workers ): run_worker_amount = worker_amount if worker_num == num_workers - 1 and residual != 0 : run_worker_amount += residual process = multiprocessing . Process ( target = load_images , args = ( worker_amount * worker_num , run_worker_amount )) processes . append ( process ) process . start () for process in processes : process . join () return data An example with SharedMemory directly from multiprocessing.shared_memory import SharedMemory from multiprocessing.managers import SharedMemoryManager from concurrent.futures import ProcessPoolExecutor , as_completed from multiprocessing import current_process , cpu_count , Process import numpy as np def work_with_shared_memory ( shm_name , shape , dtype ): print ( f 'With SharedMemory: { current_process () =} ' ) # Locate the shared memory by its name shm = SharedMemory ( shm_name ) # Create the np.recarray from the buffer of the shared memory np_array = np . recarray ( shape = shape , dtype = dtype , buf = shm . buf ) return np . nansum ( np_array . val ) # Some way to create that numpy array. np_array = ... shape , dtype = np_array . shape , np_array . dtype with SharedMemoryManager () as smm : # Create a shared memory of size np_arry.nbytes shm = smm . SharedMemory ( np_array . nbytes ) # Create a np.recarray using the buffer of shm shm_np_array = np . recarray ( shape = shape , dtype = dtype , buf = shm . buf ) # Copy the data into the shared memory np . copyto ( shm_np_array , np_array ) # Spawn some processes to do some work with ProcessPoolExecutor ( cpu_count ()) as exe : fs = [ exe . submit ( work_with_shared_memory , shm . name , shape , dtype ) for _ in range ( cpu_count ())] for _ in as_completed ( fs ): pass","title":"Example"},{"location":"tutorials/performance/multiprocessing/#huge-arrays-in-numpy","text":"Python is notoriously known as a memory hogger and when you need to work with large amount of data it could result in out of memory error. One trick we can use in this case is memory mapped file in numpy. from concurrent.futures import ProcessPoolExecutor , as_completed from multiprocessing import Process import numpy as np worker , nrows , ncols = 10 , 1_000_000 , 100 def split_size_iter ( total_size : int , num_chunks : int ) -> Iterator [ Tuple [ int , int ]]: ... def print_matrix ( filename , worker_index_start , worker_index_end ): matrix = np . memmap ( filename , dtype = np . float32 , mode = 'r+' , shape = ( worker , nrows , ncols )) print matrix [ worker_index_start : worker_index_end ] def main (): matrix = np . memmap ( 'test.dat' , dtype = np . float32 , mode = 'w+' , shape = ( worker , nrows , ncols )) # some code to fill this matrix with ProcessPoolExecutor ( cworker ) as exe : fs = [ exe . submit ( print_matrix , 'test.dat' , start , end ) for start , end in split_size_iter ( worker , 4 )] for _ in as_completed ( fs ): pass","title":"Huge arrays in numpy"},{"location":"tutorials/performance/multiprocessing/#ray","text":"Ray is a powerful open source platform that makes it easy to write distributed Python programs and seamlessly scale them from your laptop to a cluster. Ray comes with support for the mulitprocessing.Pool API out of the box when importing ray.util.multiprocessing .","title":"Ray"},{"location":"tutorials/performance/multiprocessing/#example_1","text":"import math import random import time def sample ( num_samples ): num_inside = 0 for _ in range ( num_samples ): x , y = random . uniform ( - 1 , 1 ), random . uniform ( - 1 , 1 ) if math . hypot ( x , y ) <= 1 : num_inside += 1 return num_inside def approximate_pi_distributed ( num_samples ): from ray.util.multiprocessing.pool import Pool # NOTE: Only the import statement is changed. pool = Pool () start = time . time () num_inside = 0 sample_batch_size = 100000 for result in pool . map ( sample , [ sample_batch_size for _ in range ( num_samples // sample_batch_size )]): num_inside += result print ( \"pi ~= {} \" . format (( 4 * num_inside ) / num_samples )) print ( \"Finished in: {:.2f} s\" . format ( time . time () - start ))","title":"Example"},{"location":"tutorials/performance/multiprocessing/#dask","text":"Dask is a Python parallel computing library geared towards scaling analytics and scientific computing workloads. Dask is composed of two parts: Dynamic task scheduling for optimized computation and Big Data collections such as like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments, which run on top of dynamic task schedulers.","title":"Dask"},{"location":"tutorials/performance/multiprocessing/#example_2","text":"# Setup the cluster and client for Dask from dask.distributed import Client , LocalCluster n_workers = 3 cluster = LocalCluster ( n_workers = n_workers , threads_per_worker = 1 , memory_limit = '4GB' ) client = Client ( cluster ) client . wait_for_workers ( n_workers ) # Load dataset from dask_ml.datasets import make_classification as make_classification_dask X_dask , y_dask = make_classification_dask ( n_samples = 10000 , n_features = 10 , n_informative = 5 , n_redundant = 5 , random_state = 1 , chunks = 10 ) # Build a model import dask_lightgbm.core as dlgbm dmodel = dlgbm . LGBMClassifier ( n_estimators = 400 ) dmodel . fit ( X_dask , y_dask ) # make a single prediction row = [[ 2.56999479 , - 0.13019997 , 3.16075093 , - 4.35936352 , - 1.61271951 , - 1.39352057 , - 2.48924933 , - 1.93094078 , 3.26130366 , 2.05692145 ]] yhat = dmodel . predict ( da . from_array ( np . array ( row ))) print ( 'Prediction: %d ' % yhat [ 0 ]) y_pred = dmodel . predict ( X_dask , client = client ) acc_score = ( y_dask == y_pred ) . sum () / len ( y_dask ) acc_score = acc_score . compute () print ( acc_score )","title":"Example"},{"location":"tutorials/performance/profiling_tools/","text":"Summary Use the standard python cProfile profiler Use PyCharm (Professional Edition) .pstat profile viewer Ensure entire program can be easily tested for performance Profiling Tools & Strategy \u00b6 Profiling code is a hugely important part of data science code since machine learning and AI algorithms are typically compute heavy processes. Performance tuning is relevant during both training and prediction time. During training time, performant code allows developers to have shorter development/training cycles. The difference between a function call that takes 10 seconds vs 1 second has a huge impact on the development experience. An example that many people have likely run into is when loading huge word vector models into memory. A load of GoogleNews W2V using Gensim can take multiple minutes before even being able to do a computation. During prediction time, performance typically dictates how much a model will cost to host. Performance is often ignored in favor of scaling out horizontally and just paying the cost of hosting. Besides saving on hosting costs, the biggest motivating factor for performance tuning code would be the ability to have a better/faster development experience (just like during training). Tools \u00b6 There are a variety of profiling tools for python which fall under two different categories: Scope-Based Profilers : These profilers generally use the built-in sys.setprofile or sys.settrace python library calls to track the duration of each stack frame. Every time a function enters a scope, a start time is recorded, and every time the function exits, the end time is recorded. This allows the profiler to record a hierarchical view of where time is spent. These types of profilers may also introduce a lot of overhead since every single scope entrance/exit is tracked. For programs which have many function calls (such as mapping a function over a giant collection) the overhead of a scope-based profiler may give inflated measurements. Another limitation is that many python functions do not generate a stack frame (for example built-in functions). This means that scope-based trackers may not give a granular enough idea of where time is being spent. Scope-based profilers cannot give line-level profiling information. Sampling Profilers : These profilers generally use the built-in sys._getframe python library call to get the current state of the application. In contrast to a scoped-based profiler, sampling profilers only periodically query the running python application to determine where time is being spent. Rather than tracking the duration of a function scope, a sampling profiler tracks the number of times it sees a specific stack frame. This means that a sampling profiler will not track a very accurate measure of the overall method time. A huge benefit of a sampling profiler is that it has the ability to give line-level profile information. It also has almost none of the overhead issues that a scope-based profiler has. Some sampling profilers are even designed to be run on long-running applications for production monitoring. The most widely used tools are the following: cProfile (Scope-Based): The python standard library profiler. pprofile (Scope-Based & Sampling): A pure python profiler with optional sampling. yappi (Scope-Based & Sampling): A profiler (written in C) with multi-threading support. scalene (Sampling): A profiler (written in C++) with built-in memory usage profiling. In general, it is recommended to start with cProfile and move on to other tools only if there is a need. Almost all use-cases are well-suited to the most basic scoped-based profiler and there are only infrequent times when more specialized profilers should be used. For example yappi may be well suited to testing a multi-threaded web server with extremely high traffic. Though sampling profilers can give extremely granular information, it is unlikely that the issues will not show up clearly in a scope-based profile. Strategy \u00b6 Warning To follow along with the tutorial you must have PyCharm Professional Edition . This tutorial makes extensive use of the .pstat profile viewer which is not available in the Community Edition In order to describe the process of profiling code, the following section will use cProfile and PyCharm to go through the steps of optimizing an unoptimized function. The example function will compute a categorical embedding. The function implements mapping an occupation code to an embedding vector. In the example the occupation code will be a number within the range (0, n_occupations) and the embedding will have the shape (n_occupations, n_components) . import pandas as pd import numpy as np # ------------------------------------------------------------------------------ # Example Datasets # ------------------------------------------------------------------------------ n_occupations = 100000 # The number of possible occupation codes n_components = 100 # The number of components in the occupation embedding n_samples = 10000 # The number of users def user_dataframe (): example_values = np . random . randint ( 0 , n_occupations * 2 , n_samples ) return pd . DataFrame ({ \"occupation_code\" : example_values , \"age\" : example_values , \"zip_code\" : example_values , }) def occupation_code_embedding (): index = np . arange ( n_occupations ) np . random . shuffle ( index ) values = np . random . random (( n_occupations , n_components )) return pd . DataFrame ( values , index = index ) # ------------------------------------------------------------------------------ # Example Function Implementation # ------------------------------------------------------------------------------ def occupation_embedding_lookup ( user_df , embedding_df ): result = pd . DataFrame ( columns = embedding_df . columns ) for index , row in user_df . iterrows (): value = row [ \"occupation_code\" ] # Check if the current occupation code is in the embedding index if embedding_df . index . isin ([ value ]) . any (): # Fetch the embedding for the given occupation code vector = embedding_df . loc [ value ] else : # Select 0th (default) vector if occupation code is not found vector = embedding_df . iloc [ 0 ] result . loc [ index ] = vector # Concatenate all occupation embeddings for all users return result # ------------------------------------------------------------------------------ # Main # ------------------------------------------------------------------------------ def main (): # Build an example user data and embedding user_df = user_dataframe () embedding_df = occupation_code_embedding () # Compute the embedding from the user feature result = occupation_embedding_lookup ( user_df , embedding_df ) if __name__ == '__main__' : main () In the above example, the occupation_embedding_lookup will be the focus of the profile. The user_dataframe and occupation_code_embedding functions will show up in future profiles but should be ignored since they are only used to generate example datasets. Question To demonstrate the importance of profiling, briefly look at the occupation_embedding_lookup function and come up with an guess for which lines will have the worst performance problems. What do you think will be the most time-consuming part of the function? How long do you think it will take the function to run? In pycharm, open the project, and file and then right click the within the editor panel. From the dropdown menu, select the Profile '<your_file>' option. This will invoke cProfile on the file. When the profile is complete, the PyCharm .pstat viewer will open the resulting profile. The results of any profiler that is capable of generating a .pstat file can be viewed in PyCharm (even if the profile was not generated within PyCharm). On this .pstat viewer panel, the Statistics tab is selected by default. Each row corresponds to a stack frame and its corresponding run time. This view can be useful, but it is hard to see exactly where bottleneck originated from. Notice that the most time consuming call is numpy.concatenate which is never directly called in the code example above. This usually indicates that this call is embedded in a library call. The get a more informative view of the problem areas in the call-stack, select the Call Graph tab (outlined in red above). This view presents a much more direct representation of what takes the most time. When the call graph is first presented, it may be too small to read the individual nodes. The zoom can be used by pressing the buttons on the top left of the panel (highlighted in red above). Each node in the PyCharm .pstat viewer represents a different stack frame. PyCharm color codes these stack frames from most time-consuming (red) to least time-consuming (green). Each stack frame tracks 3 things: Total time spent within the frame and all sub-frames ( Total ) Total time spent within the frame but not within a sub-frame ( Own ) Total number of times the frame was entered ( x<number> in the top right) This is the information that should be used to guide where to optimize the code. The first areas of focus should be the red boxes. The bottom 3 stack frames are composed of: main.py - The overall execution of the file. The Total will always equal the execution time of the program. In the example profile, the program took 65.8 seconds to execute. main - This is the def main() function defined to drive the program. occupation_embedding_lookup - This is the function that performs the embedding. In this view it becomes clear that the embedding computation takes up nearly all of the execution time (99%). The remaining execution time is spent in the initialization of data. None of this is very helpful since it is known that very little computation is occuring outside of the embedding function. The next most time consuming portion of the code is the __setitem__ call. Just from looking at the name, it is not clear where this is occuring. PyCharm can directly show the source code of the problem by right clicking on the node and selecting the \"Navigate To Source\" option (highlighted in red above). Upon inspecting the source code it becomes clear that this call is the DataFrame.__setitem__ method. Further inspection of the profile call graph chart reveals that this method calls into DataFrame.append which then calls into numpy.concatenate (The most time consuming individual function). There is only one place in the code where a DataFrame set item occurs in the occupation_embedding_lookup function: def occupation_embedding_lookup ( user_df , embedding_df ): result = pd . DataFrame ( columns = embedding_df . columns ) for index , row in user_df . iterrows (): value = row [ \"occupation_code\" ] if embedding_df . index . isin ([ value ]) . any (): vector = embedding_df . loc [ value ] else : vector = embedding_df . iloc [ 0 ] result . loc [ index ] = vector return result From the profile it becomes clear that every call to result.loc[index] = vector causes an array concatenation to be performed. This means that a completely new array is allocated every time this line is executed. To alleviate the issue, modify the function to collect the results in a structure designed for concatenation. A python list is a good candidate for this since it is tuned to handle appending. def occupation_embedding_lookup ( user_df , embedding_df ): result = list () for index , row in user_df . iterrows (): value = row [ \"occupation_code\" ] if embedding_df . index . isin ([ value ]) . any (): vector = embedding_df . loc [ value ] else : vector = embedding_df . iloc [ 0 ] result . append ( vector ) return pd . DataFrame ( result , index = user_df . index , columns = embedding_df . columns ) Run the profile on the updated function to view the results. The first thing to note is that the overall runtime has been drastically reduced. The original profile of 65.8 seconds has been reduced to 11.4 seconds. The new profile now shows that the next most time consuming part of the function is the check to see if the value is in the index. def occupation_embedding_lookup ( user_df , embedding_df ): result = list () for index , row in user_df . iterrows (): value = row [ \"occupation_code\" ] if embedding_df . index . isin ([ value ]) . any (): vector = embedding_df . loc [ value ] else : vector = embedding_df . iloc [ 0 ] result . append ( vector ) return pd . DataFrame ( result , index = user_df . index , columns = embedding_df . columns ) The line embedding_df.index.isin([value]).any() ends up checking to see if each of the index values match the current occupation code. Rather than an O(1) hash lookup, this performs an 0(n) search on the embedding index for every single row in the user_df . This can be alleviated by simplifying the logic and using the in operator on the pd.Index object. def occupation_embedding_lookup ( user_df , embedding_df ): result = list () for index , row in user_df . iterrows (): value = row [ \"occupation_code\" ] if value in embedding_df . index : vector = embedding_df . loc [ value ] else : vector = embedding_df . iloc [ 0 ] result . append ( vector ) return pd . DataFrame ( result , index = user_df . index , columns = embedding_df . columns ) Run the profile on the updated function to view the results. The final profile now shows that the overall execution time is 4.1 seconds. This is a 16x speedup from the original profile! Even though there may still be performance issues with this function, the code is already much more optimized than it was with just a few minor tweaks. Challenge Continue to profile the code and see if you can get a 1000x speedup!","title":"Profiling Tools & Strategy"},{"location":"tutorials/performance/profiling_tools/#profiling-tools-strategy","text":"Profiling code is a hugely important part of data science code since machine learning and AI algorithms are typically compute heavy processes. Performance tuning is relevant during both training and prediction time. During training time, performant code allows developers to have shorter development/training cycles. The difference between a function call that takes 10 seconds vs 1 second has a huge impact on the development experience. An example that many people have likely run into is when loading huge word vector models into memory. A load of GoogleNews W2V using Gensim can take multiple minutes before even being able to do a computation. During prediction time, performance typically dictates how much a model will cost to host. Performance is often ignored in favor of scaling out horizontally and just paying the cost of hosting. Besides saving on hosting costs, the biggest motivating factor for performance tuning code would be the ability to have a better/faster development experience (just like during training).","title":"Profiling Tools &amp; Strategy"},{"location":"tutorials/performance/profiling_tools/#tools","text":"There are a variety of profiling tools for python which fall under two different categories: Scope-Based Profilers : These profilers generally use the built-in sys.setprofile or sys.settrace python library calls to track the duration of each stack frame. Every time a function enters a scope, a start time is recorded, and every time the function exits, the end time is recorded. This allows the profiler to record a hierarchical view of where time is spent. These types of profilers may also introduce a lot of overhead since every single scope entrance/exit is tracked. For programs which have many function calls (such as mapping a function over a giant collection) the overhead of a scope-based profiler may give inflated measurements. Another limitation is that many python functions do not generate a stack frame (for example built-in functions). This means that scope-based trackers may not give a granular enough idea of where time is being spent. Scope-based profilers cannot give line-level profiling information. Sampling Profilers : These profilers generally use the built-in sys._getframe python library call to get the current state of the application. In contrast to a scoped-based profiler, sampling profilers only periodically query the running python application to determine where time is being spent. Rather than tracking the duration of a function scope, a sampling profiler tracks the number of times it sees a specific stack frame. This means that a sampling profiler will not track a very accurate measure of the overall method time. A huge benefit of a sampling profiler is that it has the ability to give line-level profile information. It also has almost none of the overhead issues that a scope-based profiler has. Some sampling profilers are even designed to be run on long-running applications for production monitoring. The most widely used tools are the following: cProfile (Scope-Based): The python standard library profiler. pprofile (Scope-Based & Sampling): A pure python profiler with optional sampling. yappi (Scope-Based & Sampling): A profiler (written in C) with multi-threading support. scalene (Sampling): A profiler (written in C++) with built-in memory usage profiling. In general, it is recommended to start with cProfile and move on to other tools only if there is a need. Almost all use-cases are well-suited to the most basic scoped-based profiler and there are only infrequent times when more specialized profilers should be used. For example yappi may be well suited to testing a multi-threaded web server with extremely high traffic. Though sampling profilers can give extremely granular information, it is unlikely that the issues will not show up clearly in a scope-based profile.","title":"Tools"},{"location":"tutorials/performance/profiling_tools/#strategy","text":"Warning To follow along with the tutorial you must have PyCharm Professional Edition . This tutorial makes extensive use of the .pstat profile viewer which is not available in the Community Edition In order to describe the process of profiling code, the following section will use cProfile and PyCharm to go through the steps of optimizing an unoptimized function. The example function will compute a categorical embedding. The function implements mapping an occupation code to an embedding vector. In the example the occupation code will be a number within the range (0, n_occupations) and the embedding will have the shape (n_occupations, n_components) . import pandas as pd import numpy as np # ------------------------------------------------------------------------------ # Example Datasets # ------------------------------------------------------------------------------ n_occupations = 100000 # The number of possible occupation codes n_components = 100 # The number of components in the occupation embedding n_samples = 10000 # The number of users def user_dataframe (): example_values = np . random . randint ( 0 , n_occupations * 2 , n_samples ) return pd . DataFrame ({ \"occupation_code\" : example_values , \"age\" : example_values , \"zip_code\" : example_values , }) def occupation_code_embedding (): index = np . arange ( n_occupations ) np . random . shuffle ( index ) values = np . random . random (( n_occupations , n_components )) return pd . DataFrame ( values , index = index ) # ------------------------------------------------------------------------------ # Example Function Implementation # ------------------------------------------------------------------------------ def occupation_embedding_lookup ( user_df , embedding_df ): result = pd . DataFrame ( columns = embedding_df . columns ) for index , row in user_df . iterrows (): value = row [ \"occupation_code\" ] # Check if the current occupation code is in the embedding index if embedding_df . index . isin ([ value ]) . any (): # Fetch the embedding for the given occupation code vector = embedding_df . loc [ value ] else : # Select 0th (default) vector if occupation code is not found vector = embedding_df . iloc [ 0 ] result . loc [ index ] = vector # Concatenate all occupation embeddings for all users return result # ------------------------------------------------------------------------------ # Main # ------------------------------------------------------------------------------ def main (): # Build an example user data and embedding user_df = user_dataframe () embedding_df = occupation_code_embedding () # Compute the embedding from the user feature result = occupation_embedding_lookup ( user_df , embedding_df ) if __name__ == '__main__' : main () In the above example, the occupation_embedding_lookup will be the focus of the profile. The user_dataframe and occupation_code_embedding functions will show up in future profiles but should be ignored since they are only used to generate example datasets. Question To demonstrate the importance of profiling, briefly look at the occupation_embedding_lookup function and come up with an guess for which lines will have the worst performance problems. What do you think will be the most time-consuming part of the function? How long do you think it will take the function to run? In pycharm, open the project, and file and then right click the within the editor panel. From the dropdown menu, select the Profile '<your_file>' option. This will invoke cProfile on the file. When the profile is complete, the PyCharm .pstat viewer will open the resulting profile. The results of any profiler that is capable of generating a .pstat file can be viewed in PyCharm (even if the profile was not generated within PyCharm). On this .pstat viewer panel, the Statistics tab is selected by default. Each row corresponds to a stack frame and its corresponding run time. This view can be useful, but it is hard to see exactly where bottleneck originated from. Notice that the most time consuming call is numpy.concatenate which is never directly called in the code example above. This usually indicates that this call is embedded in a library call. The get a more informative view of the problem areas in the call-stack, select the Call Graph tab (outlined in red above). This view presents a much more direct representation of what takes the most time. When the call graph is first presented, it may be too small to read the individual nodes. The zoom can be used by pressing the buttons on the top left of the panel (highlighted in red above). Each node in the PyCharm .pstat viewer represents a different stack frame. PyCharm color codes these stack frames from most time-consuming (red) to least time-consuming (green). Each stack frame tracks 3 things: Total time spent within the frame and all sub-frames ( Total ) Total time spent within the frame but not within a sub-frame ( Own ) Total number of times the frame was entered ( x<number> in the top right) This is the information that should be used to guide where to optimize the code. The first areas of focus should be the red boxes. The bottom 3 stack frames are composed of: main.py - The overall execution of the file. The Total will always equal the execution time of the program. In the example profile, the program took 65.8 seconds to execute. main - This is the def main() function defined to drive the program. occupation_embedding_lookup - This is the function that performs the embedding. In this view it becomes clear that the embedding computation takes up nearly all of the execution time (99%). The remaining execution time is spent in the initialization of data. None of this is very helpful since it is known that very little computation is occuring outside of the embedding function. The next most time consuming portion of the code is the __setitem__ call. Just from looking at the name, it is not clear where this is occuring. PyCharm can directly show the source code of the problem by right clicking on the node and selecting the \"Navigate To Source\" option (highlighted in red above). Upon inspecting the source code it becomes clear that this call is the DataFrame.__setitem__ method. Further inspection of the profile call graph chart reveals that this method calls into DataFrame.append which then calls into numpy.concatenate (The most time consuming individual function). There is only one place in the code where a DataFrame set item occurs in the occupation_embedding_lookup function: def occupation_embedding_lookup ( user_df , embedding_df ): result = pd . DataFrame ( columns = embedding_df . columns ) for index , row in user_df . iterrows (): value = row [ \"occupation_code\" ] if embedding_df . index . isin ([ value ]) . any (): vector = embedding_df . loc [ value ] else : vector = embedding_df . iloc [ 0 ] result . loc [ index ] = vector return result From the profile it becomes clear that every call to result.loc[index] = vector causes an array concatenation to be performed. This means that a completely new array is allocated every time this line is executed. To alleviate the issue, modify the function to collect the results in a structure designed for concatenation. A python list is a good candidate for this since it is tuned to handle appending. def occupation_embedding_lookup ( user_df , embedding_df ): result = list () for index , row in user_df . iterrows (): value = row [ \"occupation_code\" ] if embedding_df . index . isin ([ value ]) . any (): vector = embedding_df . loc [ value ] else : vector = embedding_df . iloc [ 0 ] result . append ( vector ) return pd . DataFrame ( result , index = user_df . index , columns = embedding_df . columns ) Run the profile on the updated function to view the results. The first thing to note is that the overall runtime has been drastically reduced. The original profile of 65.8 seconds has been reduced to 11.4 seconds. The new profile now shows that the next most time consuming part of the function is the check to see if the value is in the index. def occupation_embedding_lookup ( user_df , embedding_df ): result = list () for index , row in user_df . iterrows (): value = row [ \"occupation_code\" ] if embedding_df . index . isin ([ value ]) . any (): vector = embedding_df . loc [ value ] else : vector = embedding_df . iloc [ 0 ] result . append ( vector ) return pd . DataFrame ( result , index = user_df . index , columns = embedding_df . columns ) The line embedding_df.index.isin([value]).any() ends up checking to see if each of the index values match the current occupation code. Rather than an O(1) hash lookup, this performs an 0(n) search on the embedding index for every single row in the user_df . This can be alleviated by simplifying the logic and using the in operator on the pd.Index object. def occupation_embedding_lookup ( user_df , embedding_df ): result = list () for index , row in user_df . iterrows (): value = row [ \"occupation_code\" ] if value in embedding_df . index : vector = embedding_df . loc [ value ] else : vector = embedding_df . iloc [ 0 ] result . append ( vector ) return pd . DataFrame ( result , index = user_df . index , columns = embedding_df . columns ) Run the profile on the updated function to view the results. The final profile now shows that the overall execution time is 4.1 seconds. This is a 16x speedup from the original profile! Even though there may still be performance issues with this function, the code is already much more optimized than it was with just a few minor tweaks. Challenge Continue to profile the code and see if you can get a 1000x speedup!","title":"Strategy"}]}